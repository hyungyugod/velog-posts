# ğŸ“Œ 1. ë¹…ë¶„ê¸° 2025 ì‹¤ê¸° ì˜ˆì‹œ ë¬¸ì œ
## 1-1. 1ìœ í˜•
- ë‹µì€ ë§ì¶”ê¸´í–ˆëŠ”ë° ë” ì•Œì•„ë‘ë©´ ì¢‹ì„ ê²ƒë“¤ì€ ì•„ë˜ì™€ ê°™ë‹¤.
- í‰ê· ì„ ë³€ìˆ˜ mì— m = df['ê³ ê°ë§Œì¡±ë„'].mean() ì´ëŸ° ì‹ìœ¼ë¡œ ë‹´ì•„ê°€ëŠ” ê²ƒì´ ê¹”ë”í•˜ë‹¤. ê·¸ë¦¬ê³  í™•ì¸ë„ í•˜ê¸°
- df.groupby('ë¶€ì„œ')['ì—°ë´‰'].mean().nlargest(2)ë¥¼ ì‚¬ìš©í•˜ë©´ ê°€ì¥ í° 2ê°œì˜ ê°’ë§Œ ë½‘ì„ ìˆ˜ ìˆë‹¤.
```py
# ìˆ˜í–‰ 1)
df['ê³ ê°ë§Œì¡±ë„'] = df['ê³ ê°ë§Œì¡±ë„'].fillna(df['ê³ ê°ë§Œì¡±ë„'].mean())

# ìˆ˜í–‰ 2)
df = df.dropna(subset=['ê·¼ì†ì—°ìˆ˜'])

# ìˆ˜í–‰ 3)
print(df['ê³ ê°ë§Œì¡±ë„'].quantile(.75)) # 8

# ìˆ˜í–‰ 4)
print(df.groupby('ë¶€ì„œ')['ì—°ë´‰'].mean().sort_values(ascending=False)) # 74690
```

## 1-2. 2ìœ í˜•
- n_estimatorsëŠ” ë³µìˆ˜í˜•ìœ¼ë¡œ së¥¼ ë’¤ì— ê¼­ ë¶™ì—¬ì•¼ í•œë‹¤.
- metricsì— root_mean_squared_error í•¨ìˆ˜ê°€ ìˆë‹¤. ê·¸ë¦¬ê³  metricsì— aê°€ ì•„ë‹ˆë¼ eì¸ì ì„ ìœ ì˜í•˜ì
- ë§ˆì§€ë§‰ì— ì •ë‹µ í–‰ì˜ ê°œìˆ˜ ê¼­ í™•ì¸í•˜ê¸°
- ì¸ì½”ë”©í• ë•Œ ì¢…ë¥˜ê°€ ê°™ì€ì§€ ê¼­ í™•ì¸í•˜ê³  concatìœ¼ë¡œ í•©ì¹˜ê³  ì¸ì½”ë”©í•˜ê³  ë‚˜ëˆŒì§€ ê·¸ëƒ¥ ë‚˜ëˆŒì§€ ê²°ì •í•˜ê¸°
- edaì—ì„œ ì•„ë˜ë¥¼ ê¼­ ë¨¼ì € í™•ì¸í•˜ê³  í•´ì•¼í•œë‹¤. ì‹¤ì œë¡œ ì´ë¬¸ì œì—ì„œ í…ŒìŠ¤íŠ¸ì˜ ë²”ì£¼ê°€ í•˜ë‚˜ ì ì—ˆë‹¤.
```py
a = set(train['ì£¼êµ¬ë§¤ìƒí’ˆ'])
b = set(test['ì£¼êµ¬ë§¤ìƒí’ˆ'])
print('trainì—ë§Œ ìˆëŠ” ì¹¼ëŸ¼:', a-b)
print('testì—ë§Œ ìˆëŠ” ì¹¼ëŸ¼:', b-a)
```
- labelì¸ì½”ë”©ì€ ë¨¼ì € ì‚¬ì „ì„ ë§Œë“œëŠ” fitê³¼ì •ì´ ìˆê¸°ë•Œë¬¸ì— trainì´ testë¥¼ ë‹¤ í¬í•¨í•œë‹¤ë©´ ê·¸ëƒ¥ ë°ì´í„°ë¥¼ í•©ì¹  í•„ìš”ì—†ì´ ì§„í–‰í•´ë„ ëœë‹¤.
- ì›í•«ì¸ì½”ë”©ì€ í•œë²ˆ ë°ì´í„°ë¥¼ í•©ì³¤ë‹¤ê°€ ë¶„ë¦¬í•˜ëŠ” ë¡œì§ì´ í•„ìš”í•œë° ì´ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.
- ê·¸ë¦¬ê³  ì›¬ë§Œí•˜ë©´ ì›í•«ì¸ì½”ë”©ë„ ì§„í–‰í•´ë³´ê¸°
```py
df = concat([train, test])
df = pd.get_dummies(df)
train = df.iloc[:len(train)]
test = df.iloc[len(train):]
```
- ì•„ë˜ëŠ” ì „ì²´ ì½”ë“œì´ë‹¤.
```py
# 1) ì „ì²˜ë¦¬: ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ì¸ì½”ë”©
train['í™˜ë¶ˆê¸ˆì•¡'] = train['í™˜ë¶ˆê¸ˆì•¡'].fillna(0)
test['í™˜ë¶ˆê¸ˆì•¡'] = test['í™˜ë¶ˆê¸ˆì•¡'].fillna(0)

obj_cols = ['ì£¼êµ¬ë§¤ìƒí’ˆ','ì£¼êµ¬ë§¤ì§€ì ']

# baseline
# print(train.shape, test.shape)
# train = train.drop(obj_cols, axis=1)
# test = test.drop(obj_cols, axis=1)
# print(train.shape, test.shape)

# labelencoder
from sklearn.preprocessing import LabelEncoder
for col in obj_cols:
	le = LabelEncoder()
	train[col] = le.fit_transform(train[col])
	test[col] = le.transform(test[col])

# 2) ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
from sklearn.model_selection import train_test_split
x_tr, x_val, y_tr, y_val = train_test_split(train.drop('ì´êµ¬ë§¤ì•¡', axis=1), train['ì´êµ¬ë§¤ì•¡'], train_size=0.8, random_state=42)
# print(x_tr.shape, x_val.shape, y_tr.shape, y_val.shape)

# 3) ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
# lgbm
from lightgbm import LGBMRegressor
model = LGBMRegressor(verbose=-1, random_state=42, max_depth=6, n_estimators=300, learning_rate=0.02)

# randomforest
# from sklearn.ensemble import RandomForestRegressor
# model = RandomForestRegressor(random_state=42, max_depth=6, n_estimators=300)


from sklearn.metrics import root_mean_squared_error 
model.fit(x_tr, y_tr)
pred1 = model.predict(x_val)
print(root_mean_squared_error(y_val, pred1)) 
# lgbm max_depth=6, n_estimator=300, learning_rate=0.02  788.4995207798759
# rf random_state=42, max_depth=6, n_estimators=300 800.4953542753697

# 4) ê²°ê³¼ ì˜ˆì¸¡ ë° ì œì¶œ
pred = model.predict(test)
result = pd.DataFrame({
	'pred':pred
})

result.to_csv('result.csv', index=False)

# rs = pd.read_csv('result.csv')
# print(rs)
```

## 1-3. 3ìœ í˜•
- ìš°ì„  ë¡œê·¸ ë¦¬ì§€ìŠ¤í‹´ì´ë¼ê³  ëª…ì‹œí–ˆìœ¼ë‹ˆ ì¹¼ëŸ¼ ì „ì²´ì— ë¡œê·¸ ì—°ì‚°ì„ í•´ì¤˜ì•¼í•œë‹¤. ì´ëŠ” np.logë¡œ ë²¡í„°ë¡œê·¸ì—°ì‚°ì„ í•˜ë©´ëœë‹¤.
- df['Resistin'] = np.log(df['Resistin'])
- ê·¸ë¦¬ê³  F í†µê³„ëŸ‰ì˜ ê²€ì • í†µê³„ëŸ‰ì€ ë¶„ì‚°ë¹„ì´ë‹¤.
- ì´ë•Œ ë¶„ì‚°ìì²´ê°€ ì´ë¯¸ ììœ ë„ë¡œ ë‚˜ëˆ´ìœ¼ë¯€ë¡œ (var2 / df2) / (var1 / df1) ì´ë ‡ê²Œ ê¹Œì§€ ì•Ší•´ë„ ëœë‹¤. var2 / var1 ìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤.
- ë§Œì•½ ì œê³±í•©ë§Œ ìˆìœ¼ë©´ ììœ ë„ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ íƒ€ë‹¹í•˜ë‹¤.
- íŠ¹ì • ì—´ë§Œ ì„ íƒí•˜ëŠ”ê±´ ê·¸ëƒ¥ ì¡°ê±´ìœ¼ë¡œ ê±°ë¥¸ í›„ì— ì›í•˜ëŠ” ì—´ì„ ì„ íƒí•˜ë©´ ëœë‹¤.
- cond1 = df['Classification'] == 1
- var1 = df[cond1]['Resistin'].var()
- n1 = sum(cond1)ìœ¼ë¡œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì „ì²´ í–‰ê°œìˆ˜ë¥¼ ë°”ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤.
```py
#1
import numpy as np
df['Resistin'] = np.log(df['Resistin'])

cond1 = df['Classification'] == 1
cond2 = df['Classification'] == 2

var1 = df[cond1]['Resistin'].var()
var2 = df[cond2]['Resistin'].var()

result = var2 / var1
print(round(result,3))


#2
# ((ììœ ë„1 * ë¶„ì‚°1) + (ììœ ë„2 * ë¶„ì‚°2)) / ììœ ë„1+ììœ ë„2
n1 = sum(cond1)
n2 = sum(cond2)

result = (((n1-1) * var1) + ((n2-1) * var2)) / ((n1-1) + (n2-1))
print(round(result,3))


#3
from scipy import stats
result = stats.ttest_ind(df[cond1]['Resistin'], df[cond2]['Resistin'], equal_var = True)
print(round(result.pvalue,3))
```