# 📌 1. pandas 활용 정적 크롤링
### 📌 1-1. numpy / pandas에서 axis 의미
- axis=0 → 행(row) 방향을 따라 계산 (즉, 세로축 ↓)
- → 각 열(column)별 연산
- axis=1 → 열(column) 방향을 따라 계산 (즉, 가로축 →)
- → 각 행(row)별 연산

### 📌 1-2. 웹 크롤링 준비
- 기존에 크롤링 하던대로 get요청을 보내고 html을 파싱해둔다.
```py
# 영화 랭킹 정보 url을 GET 방식으로 요청
url = 'https://www.moviechart.co.kr/rank/realtime/index/image'
movie_ranking = requests.get(url)

# HTML parser 이용해 soup 객체 생성
soup = BeautifulSoup(movie_ranking.content, 'html.parser')
```
- 원하는 태그를 찾아서 태그 정보들을 가져온다.
```py
# strip이 개행 문자랑 스페이스 모두 날려준다.
# type(ticketings_tags[0]) = bs4.element.Tag -> 여기서 find 다시 사용가능 ->get_text도 사용가능
# ticketings.append(float(i.find("span").get_text().replace("%",""))) 이렇게 안하고 split(":")해도 될듯
movie_title_tags = soup.find_all("div", class_="movie-title")
ticketings_tags = soup.find_all("li", class_="ticketing")
date_tags = soup.find_all("li", class_="movie-launch")
```
- 가져온 태그에서 필요한 정보들만 추출하여 리스트로 담아준다.
- strip이 개행 문자랑 스페이스 모두 날려준다.
- type(ticketings_tags[0]) = bs4.element.Tag -> 여기서 find 다시 사용가능 ->get_text도 사용가능
- ticketings.append(float(i.find("span").get_text().replace("%",""))) 이렇게 안하고 split(":")해도 될듯하다.
- i.text.split(":")[1].strip() 이런 식의 방법이 가장 좋은 것 같다.
```py
movie_titles = []
for i in movie_title_tags:
  movie_titles.append(i.text.strip())

ticketings = []
for i in ticketings_tags:
  ticketings.append(float(i.find("span").get_text().replace("%","")))

launch_dates = []
for i in date_tags:
  launch_dates.append(i.text.split(":")[1].strip())
```
- find 함수 말고 select 함수가 좀 더 편한 거 같긴하다. 
- select는 문자열로 css 선택자를 주면 해당하는 태그를 전부 찾아준다.
- select_one은 그 중에 하나만 가져올 수 있도록 해준다.
- '#'이 id 공백이 모든 후손 > 직계 후손 .은 클래스임을 한번 다시 기억해보자
```py
# yes24
# select로는 css 선택자로 찾을 수 있다.
url = 'http://www.yes24.com/24/category/bestseller'
bestseller = requests.get(url)
soup = BeautifulSoup(bestseller.content, 'html.parser')
book = soup.select_one("#yesBestList a.gd_name")
```


### 📌 1-3. 크롤링해서 표 만들기
- 객체를 데이터 프레임으로 변환할때 아래처럼 칼럼명과 열의 데이터 즉 Series로 만들 것을 딕셔너리로 묶어서 DateFrmame 함수에 집어넣으면 된다.
- 이때 모든 데이터의 길이가 동일해야한다.
- range(1,len(movie_titles)+1)를 통해 1~전체 개수만큼의 리스트를 생성해서 Series로 만들어주면 순위를 잘 나타낼 수 있다.
```py
pd.DataFrame({"Rank": range(1,len(movie_titles)+1),
              "영화제목":movie_titles,
              "예매율":ticketings,
              "개봉일":launch_dates})
```

### 📌 1-4. 지저분한 데이터 전처리하기
- 정규표현식으로 활용하여 일일히 전처리를 해야한다.
- re는 정규표현식 라이브러리이고 sub는 정규표현식을 두번째 인자로 교환해준다. 세번째는 바꿀 대상을 넣는다. 이는 원본데이터를 변환하는것이 아니므로 재할당해주어야 한다.
- tag객체의 get_text(" ", strip=True)에는 strip여부를 설정할 수 있다.
- 첫번재 인자는 구분자이다.
```py
import re
athuor = soup.select("#yesBestList span.info_auth")

author_list = []

for i in athuor:
    text = i.get_text(" ", strip=True)         
    text = text.replace("정보 더 보기/감추기", "") 
    author_list.append(text)
  
clean_list = []

for t in author_list:
    t = re.sub(r"(저|역|글|원저|공저|감수)", "", t)
    t = re.sub(r"\s+", " ", t).strip()  # 공백 정리
    clean_list.append(t)

clean_list
```

# 📌 2. 동적 크롤링 준비
### 📌 2-1. 아나콘다
- 동적 크롤링을 할때 책임문제 때문에 코랩에서는 지원을 안하므로 연습하기 위해서는 아나콘다를 활용해서 연습을 해야한다.

### 📌 2-2. selenium
- webdriver는 실제로 브라우저를 켜고 조작할 수 있게 해주는 모듈이다.
- 예를 들어 webdriver.Chrome()을 쓰면 크롬 브라우저를 띄워서 자동으로 클릭, 입력, 크롤링 같은 작업을 할 수 있다.
- from selenium.webdriver.common.by import By : 브라우저 안에서 HTML 요소를 찾을 때 기준(Selector) 을 정의하는 도구이다.
```py
!pip install selenium

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By

import time
```
- webdriver.Chrome()을 실행하면 진짜 브라우저가 켜지는데 이는 Selenium이 원격 조종하기 위해 띄운 크롬 프로세스이다. 
- 이 브라우저는 chromedriver라는 중간 프로그램을 통해 Selenium 명령을 받는다.
- 네 코드 → Selenium(WebDriver API) → chromedriver.exe → 크롬 브라우저 엔진
- 보통 마우스로 클릭하기보단 코드로 .click(), .send_keys() 같은 걸 보내서 제어할 수 있게 된다.
- 아래는 가상 브라우저를 띄우고 버거킹에 접속하는 코드이다.
```py
url = "https://www.burgerking.co.kr/home"
driver = webdriver.Chrome()
driver.get(url)
```
- 이때 아래 명령을 사용하면 현재 가상 브라우저 화면에 보이는 html을 실시간으로 긁어오게 된다.
```py
driver.page_source
```