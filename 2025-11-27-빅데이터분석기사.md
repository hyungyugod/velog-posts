# 📌 1. 9회 기출
## 1-1. 1유형
- 그룹별로 결측치를 다르게 채울때 그룹별로 값을 계산해서 각각 행에 붙일 수 있는 열벡터를 만들어주는게 transform이다. (like window 함수)
- transform으로 생성한 열벡터를 기준으로 fillna를 하면 비어있는 결측치에는 해당 열벡터의 값이 들어간다.
- nlargest를 통해 해볼 수도 있다.
```py
import pandas as pd
df = pd.read_csv("https://raw.githubusercontent.com/lovedlim/inf/refs/heads/main/p4/9_1/hr.csv")
df['만족도'] = df['만족도'].fillna(df['만족도'].mean())
df['근속연수'] = df['근속연수'].fillna(df.groupby(['부서', '성과등급'])['근속연수'].transform('mean').astype(int))


df['연봉 / 근속연수'] = df['연봉'] / df['근속연수']
df['연봉 / 만족도'] = df['연봉'] / df['만족도']

print(df.sort_values('연봉 / 근속연수', ascending=False)['근속연수'].head(3)) # 1

print(df.sort_values('연봉 / 만족도', ascending=False)['교육참가횟수'].head(2)) # 6

# 7
```
- groupby에 대상 칼럼을 2개쓰면 이중 인덱스 구조가 된다. 여기서 unstack을 쓰면 가장 안쪽 즉 2번째 인덱스의 값들이 컬럼으로 올라간다. 
- 아래처럼 명확히 지정해줄 수도 있다.
```py
grouped.unstack()      # 성별이 컬럼으로 감
grouped.unstack(0)     # 지역코드가 컬럼으로 감
grouped.unstack('성별') # 명시적으로 성별을 컬럼으로
```
- 이 방법으로 성별별 총매출액 합계를 구한 후 성별의 차이 컬럼을 만들 수 있다. 이는 그룹된 인덱스 값의 차이를 계산하기 위해 인덱스를 컬럼으로 올리고 그 컬럼들간의 연산으로 새로운 컬럼을 만들때 유리하다. 
```py
# 1) 총대출액 컬럼 생성
df['총대출액'] = df['신용대출'] + df['담보대출']

# 2) 지역코드와 성별별로 총대출액 합계 계산
grouped = df.groupby(['지역코드', '성별'])['총대출액'].sum().unstack()
grouped
# 3) 성별 간 총대출액 차이가 가장 큰 지역코드 찾기 (절대값 사용)
grouped['차이'] = abs(grouped[1] - grouped[2])
result = grouped['차이'].idxmax()
print(result)
```
- 이를 한번에 할 수 있는 방법이 pivot_table이다. 지역코드와 성별로 집계용 crosstab을 만드는 것이다.
- 만약 pd.crosstab(df['지역코드'], df['성별'])이렇게 하면 빈도수를 계산해서 채우게 된다. 이는 chi2_contingency에서 독립성 검정을 위해 사용한다.
```py
# 1) 총대출액 컬럼 생성
df['총대출액'] = df['신용대출'] + df['담보대출']

# 2) 지역코드와 성별별로 총대출액 합계 계산
grouped = df.pivot_table(index='지역코드', columns='성별', values='총대출액', aggfunc='sum')

# 3) 성별 간 총대출액 차이가 가장 큰 지역코드 찾기 (절대값 사용)
grouped['차이'] = abs(grouped[1] - grouped[2])
result = grouped['차이'].idxmax()
print(result)
```
- 만약 칼럼에 누가봐도 한 종류로 묶일 칼럼들이 흩어져있을때는 이를 melt하여 하나의 컬럼으로 정리해줄 필요가 있다. 여기서는 범죄유형이 칼럼으로 마구 흩어져있어서 분석할때 정리가 힘들었다. 이를 값으로 내리는 것은 오직 melt를 통해서 가능하다.
- 매개변수가 헷갈릴때는 print(help(pd.melt))를 통해 주석에 매개변수를 옮겨두고 풀이하면 편하다.
- pd.melt(df, id_vars=['연도', '구분'], var_name='범죄유형', value_name='건수') 이렇게 하면 id_vars에 입력하지 않은 모든 칼럼이 var_name이름의 칼럼 안으로 빨려들어가서 id_vars와 유형 조합 별로 컬럼이 길어진다.
- 이때 행과 열이 만나야 값을 특정할 수 있는데 모든 열이 행으로 빨려들어갔으므로 열의 역할을 해줄 칼럼이 필요한데 그 칼럼 이름이 value_name이다.
- df_long = pd.melt(df, id_vars=['연도' ], value_vars='구분', var_name='범죄유형', value_name='건수') 만약 이렇게 value값을 지정해주게 되면 원래 있던 건수값이 아니라 구분이 새로운 value가 되어 범죄유형별 구분값이 뭔지 보는 테이블이 된다.
- 이후 pivot_table을 통해 다시 행과 열을 재배치한다. index에는 그룹화 기준, columns에는 연산하고 보여줄 값들을 펼치면 된다. 여기선 검거건수와 발생건수를 나눠야했으므로 구분 카테고리를 열로 올리고 나머지를 index로 삼았다. 어쩌피 그룹별 값이 하나이기 때문에 aggfunc는 무엇으로 하건 상관이 없다.
- idxmax()는 인덱스 라벨을 반환한다. 즉 그 인덱스의 값을 반환한다. 순서가 아니라 하여 이를 사용할때는 loc에 넣어서 사용해야한다.
- pivot_df.loc[max_idx] 이런식으로 loc에 인덱스 series를 넣으면 그 인덱스들에 맞는 행들만 모은 dataframe을 반환한다.
- pivot_df.groupby(['연도'])['검거율'].idxmax() 멀티 인덱스일때 이렇게 하면 포함되지 않은 인덱스인 범죄유형에 대한 인덱스들이 담기게 된다.
```py
df_long = pd.melt(df, id_vars=['연도', '구분' ], var_name='범죄유형', value_name='건수')

pivot_df = pd.pivot_table(data=df_long, values='건수', index=['연도', '범죄유형'], columns='구분', aggfunc = 'mean').reset_index()

pivot_df['검거율'] = pivot_df['검거건수'] / pivot_df['발생건수']

max_idx = pivot_df.groupby(['연도'])['검거율'].idxmax()

print(int(pivot_df.loc[max_idx]['검거건수'].sum()))
```
- 추가로 listbox = df3.idxmax(axis=1)는 행별로 가장 큰값을 갖는 열이름을 반환하는데 이를 이용해서 검거율 테이블을 따로 만든 후 인덱스를 찾아서 다시 검거건수 테이블에서 탐색하는 방법도 있다.
- 이때 df1 = df1.reset_index(drop=True) 이렇게 하는 이유는 두 테이블의 인덱스를 정확히 똑같이 하기 위해서 이다. drop=True를 주지 않았을 때에는 기존의 인덱스가 index칼럼으로 들어와버리는 일이 발생한다.
```py
# 1) "발생건수"와 "검거건수"를 따로 분리
cond1 = df['구분'] == "발생건수"
cond2 = df['구분'] == "검거건수"
df1 = df[cond1].iloc[:, 2:]
df2 = df[cond2].iloc[:, 2:]

# 2) 검거율 계산 (검거건수 / 발생건수)
df1 = df1.reset_index(drop=True)
df2 = df2.reset_index(drop=True)

df3 = df2 / df1
df3

# 3) 각 연도에서 검거율이 가장 높은 범죄유형 찾기
listbox = df3.idxmax(axis=1)
listbox
# 4) 가장 높은 검거율을 기록한 범죄유형의 검거건수 가져오기
result = 0
for index, item in enumerate(listbox):
    result = result + df2.loc[index, item]
print(result)
```

## 1-2. 3유형
- 2유형은 일반적이어서 생략한다.
- model.pvalues 했을때 도출되는 series의 맨 첫번째 값이 intercept이므로 ((model.pvalues < 0.05)[1:]) 이렇게 자르면 절편을 제외한 변수중에 조건을 만족하는 변수들만 고를 수 있다.
- train데이터에 정답이 있어도 정답을 모른척하고 train데이터로 예측하는 것이 가능하다. 그냥 pred = model.predict(train) 이렇게 하면 된다.
- C(HasTechInsurance)[T.1] 이렇게 나타나는 건 해당 범주형변수에서 0일때보다 1일때 얼마나 유의미한 차이를 나타내는지를 물어보는것이고 0은 기준이 되므로 따로 pvalue를 구하지 않는다.
- C(HasPhoneService, Treatment(reference=1)) 이런식으로 적으면 1을 기준으로 삼을 수 있다. treatment는 처치라는 의미로 reference 즉 기준을 뭘로 두고 처치할지를 정해주는 함수이다.

| pearsonr() 반환값 | 의미              |
| -------------- | --------------- |
| c              | 피어슨 상관계수 r  검정통계량은 내부에서 따로 계산  |
| p              | t-검정 기반 p-value |

- 일반 판다스의 corr() 에 method를 prearson으로 주어도 되지만 stats의 pearsonr을 사용해도 된다. 아래는 사용한 버전이다.
```py
# 1) 유의한 변수만 사용한 회귀 분석
from statsmodels.formula.api import ols
model = ols("design ~ c1 + c2 + c4", data=train).fit()

# 학습데이터에서 예측값 계산
train['pred_design'] = model.predict(train)
train

# 예측값과 실제 design 값 사이의 피어슨 상관계수 계산
from scipy.stats import pearsonr
c, p = pearsonr(train['design'], train['pred_design'])
print(round(c, 3),p)
```
- 앞에서 언급했던 로지스틱 회귀 부분이다.
- 만약 C를 생략하면 수치형 변수로 해석하여 0에서 1로 바뀔때 얼마나 변화가 일어나는가로 해석된다. 따라서 0과 1 로 나눠져 있는 변수일때는 C를 하나안하나 다르지만 만에 하나 다른 56, 89, 12 뭐 이런 식으로 되어있다면 의미가 완전 달라질 수 있다.
- 오즈비는 역시 e의 계수승으로 계산한다.
```py
import pandas as pd
df = pd.read_csv("https://raw.githubusercontent.com/lovedlim/inf/refs/heads/main/p4/9_3/retention.csv")

from statsmodels.formula.api import logit
formula = 'Churn ~ MonthlyCharges + CustomerTenure + C(HasPhoneService) + C(HasTechInsurance)'
model = logit(formula, data=df).fit()

print(round(model.pvalues['MonthlyCharges'],3)) # 0.008
print(model.summary())
```
```bash
C(HasPhoneService)
→ 0=기준군, 1=처치군 차이 검정 → 결과가 '집단 비교'라는 게 명확함
HasPhoneService
→ 숫자 기울기 해석 → 1 증가 효과 → 해석을 사람이 다시 변환해야 함
```

# 📌 2. 카이제곱 검정
## 2-1. 적합도 검정
- 귀무가설(H0): 이 도시의 교통사고 경험 수 분포는 전국적인 경향을 따른다.
- 대립가설(H1): 이 도시의 교통사고 경험 수 분포는 전국적인 경향을 따르지 않는다.
- 적합도 검정은 위와 같은 가설들을 대상으로 진행된다.
```py
ob = [550, 250, 100, 70, 30]
ex = [600, 250, 80, 50, 20]

print(30/1000)

from scipy import stats
st, pv = stats.chisquare(ob,ex)
print(st, pv) 
```


## 2-2. 독립성 검정
- print(pd.pivot_table(df, index='캠프', columns='등록여부', values='등록여부', aggfunc='count')) 이게 crosstab과 결과가 달랐던 이유는 피벗테이블은 기본적으로 수치형 변수만 계산하기 때문이었다. 따라서 같은 효과를 내려면 aggfunc를 size로 하면 같은 효과를 얻을 수 있다.
- 또한 values가 없으면 집계함수를 사용한 행을 알 수 없기때문에 계산 대상을 꼭 지정해주어야한다. 이땐 행개수를 세는거라 values에 뭐가와도 딱히 상관은 없었다.
- 교차표를 chi2_contingency에 넣어서 계산하면 된다.
```py
import pandas as pd
df = pd.read_csv("https://raw.githubusercontent.com/lovedlim/inf/refs/heads/main/p3/camp.csv")
ct = pd.crosstab(df['캠프'], df['등록여부'])
print(stats.chi2_contingency(ct)) # 0.8508492527705047 -> 귀무가설 채택

print(pd.pivot_table(df, index='캠프', columns='등록여부', values='등록여부', aggfunc='size'))
```