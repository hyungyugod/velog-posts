# 📌 1. 작업형 1 유형 – Pandas / 전처리 치트시트

## 1-1. 기본 문법 & 데이터 구조

| 구분 | 기능/개념 | 설명 | 예시 코드 |
|------|-----------|------|-----------|
| 기본 문법 | 불리언 값 | 두 값 중 하나 선택. True, False는 중괄호 없이 그대로 사용 | ascending=True |
| 데이터 구조 | 데이터프레임 생성 | 2차원 테이블 구조 | pd.DataFrame() |
| 데이터 구조 | 시리즈 생성 | 1차원 데이터 구조 | pd.Series() |
| 데이터 구조 | 컬럼 선택 (시리즈) | 1개 컬럼 선택 → 시리즈 반환 | df['컬럼'] |
| 데이터 구조 | 컬럼 선택 (데이터프레임) | 1개 컬럼을 데이터프레임 형태로 | df[['컬럼']] |
| 데이터 구조 | 여러 컬럼 선택 | 리스트로 다중 컬럼 선택 | df[['컬럼1','컬럼2']] |

---

## 1-2. 데이터 입출력 & EDA 기본

### (1) 데이터 입출력

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 입출력 | CSV 불러오기 | CSV → 데이터프레임 | pd.read_csv('파일명') |
| 입출력 | CSV 저장 | 데이터프레임 → CSV (인덱스 저장 X) | df.to_csv('파일명', index=False) |

### (2) EDA – 구조 / 자료형 / 샘플

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| EDA | 샘플 확인 | 처음/끝/임의 행 일부 확인 | df.head(), df.tail(), df.sample() |
| EDA | 데이터 크기 | (행, 열) 튜플. 괄호 없음 | df.shape |
| EDA | 자료형 확인 | 컬럼별 타입, 결측 개요 | df.info() |
| EDA | 객체 타입 확인 | 파이썬 객체 타입 확인 | type(변수명) |

### (3) EDA – 통계, 상관, 범주형

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| EDA | 상관관계 | 수치형 컬럼 간 상관계수 | df.corr(numeric_only=True) |
| EDA | 카테고리별 항목 수 | 범주형 빈도표 | df['컬럼'].value_counts() |
| EDA | 고유값 수 | 카테고리 개수 | df['컬럼'].nunique() |
| EDA | 고유값 목록 | 유니크 값 리스트 | df['컬럼'].unique() |
| EDA | 기초 통계 (수치형) | 수치형 요약 통계 | df.describe() |
| EDA | 기초 통계 (범주형) | 범주형 요약 통계 (include='O') | df.describe(include='O') |

---

## 1-3. 전처리 – 자료형·행열 삭제·인덱싱·정렬·필터링

### (1) 자료형 변경 & 행/열 삭제

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 전처리 | 자료형 변경 | 컬럼 타입을 float/int/category 등으로 변경 | df['컬럼'].astype('float') |
| 전처리 | 행 삭제 | 특정 행 삭제 (인덱스 기준) | df.drop(index, axis=0) |
| 전처리 | 열 삭제 | 특정 컬럼 삭제 | df.drop('컬럼', axis=1) |

### (2) 인덱싱 – loc / iloc

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 인덱싱 | loc | 라벨 기반 슬라이싱, 끝 포함 | df.loc[행시작:행끝, 열시작:열끝] |
| 인덱싱 | iloc | 정수 인덱스 기반, 끝 제외 | df.iloc[0:3, 0:2] |

### (3) 정렬

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 정렬 | 컬럼 기준 정렬 | 특정 컬럼 기준 오름/내림차순 | df.sort_values('컬럼', ascending=True) |
| 정렬 | 인덱스 기준 정렬 | 인덱스 기준 정렬 | df.sort_index() |

### (4) 필터링 – 조건식

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 필터링 | 단일 조건 | 조건이 참인 행만 선택 | cond = df['컬럼'] < 100; df[cond] |
| 필터링 | OR 조건 | 여러 조건 중 하나라도 참 | df[(조건1) \| (조건2)] |
| 필터링 | AND 조건 | 모든 조건이 참 | df[(조건1) & (조건2)] |

---

## 1-4. 결측치 처리 & 값 변경 & 문자열 처리

### (1) 결측치 처리

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 결측치 | 결측값 확인 | 컬럼별 결측 개수 확인 | df.isnull().sum() |
| 결측치 | 결측값 채우기 | 특정 값으로 결측 채우기 | df['컬럼'].fillna(0) |

### (2) 값 변경 – replace, map

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 값 변경 | replace | 특정 값을 다른 값으로 치환 | df['컬럼'].replace('이전','이후') |
| 값 변경 | map | 딕셔너리로 여러 값 매핑 | df['컬럼'].map({'이전':'이후'}) |

### (3) 문자열 처리 – str 접근자

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 문자열 | 문자 치환 | 문자열에서 문자 바꾸기 | df['컬럼'].str.replace('-', '') |
| 문자열 | 포함 여부 | 키워드 포함 여부(True/False) | df['컬럼'].str.contains('키워드') |
| 문자열 | 길이 | 문자열 길이 | df['컬럼'].str.len() |
| 문자열 | 대/소문자 | 대문자/소문자로 변환 | df['컬럼'].str.upper(), df['컬럼'].str.lower() |
| 문자열 | 문자열 분리 | 구분자 기준 분리 후 요소 선택 | df['컬럼'].str.split().str[0] |

---

## 1-5. 그룹핑 & 인덱스 복원 & 내장함수

### (1) 그룹핑 / 집계 / transform / 인덱스 복원

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 그룹핑 | 집계 | 그룹별 합/평균 등 집계 | df.groupby('컬럼')['값컬럼'].mean() |
| 그룹핑 | transform | 그룹별 통계를 원래 행 구조 유지한 채 붙임 | df.groupby('컬럼')['값컬럼'].transform('mean') |
| 인덱스 | 인덱스 복원 | groupby 후 인덱스로 간 컬럼을 다시 일반 컬럼으로 | df.groupby('컬럼').sum().reset_index() |

### (2) 내장함수 – 행 수, 합계, 조건 개수 등

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 내장함수 | 행 수 | 행 개수 | len(df) |
| 내장함수 | 합계 | axis=0 세로합, axis=1 가로합 | df.sum(axis=1, numeric_only=True) |
| 내장함수 | 조건 개수 | 조건 만족 행 수 | (df['컬럼'] > 100).sum() |
| 내장함수 | 기초 통계 | max, min, mean, median, std, var | df['컬럼'].mean() 등 |
| 내장함수 | 사분위수 | 1사분위수(0.25), 3사분위수(0.75) | df['컬럼'].quantile(0.25), df['컬럼'].quantile(0.75) |
| 내장함수 | 최빈값 | mode()[0] | df['컬럼'].mode()[0] |
| 내장함수 | 최대값 인덱스 | 최대값인 행의 인덱스 | df['컬럼'].idxmax() |
| 내장함수 | melt (데이터 구조 재정의) | wide → long 변환 (기본형) | pd.melt(df) |

---

## 1-6. 시계열 처리 (datetime)

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 시계열 | 날짜 포맷 문자열 | datetime 파싱 포맷 | '%Y-%m-%d %H:%M:%S' |
| 시계열 | 연/월/일/시/분/초 | 날짜 컬럼에서 각 요소 추출 | df['날짜'].dt.year 등 |
| 시계열 | 요일 | 0=월, …, 6=일 | df['날짜'].dt.dayofweek |
| 시계열 | 시간 차이 | 두 날짜 차이 | diff = df['날짜1'] - df['날짜2'] |
| 시계열 | 시간 차이 변환 | 초 기준 → 분/시간/일 등 변환 | diff.dt.total_seconds()/60 (분) |

---

## 1-7. 데이터 재구조화 – melt / unstack / pivot_table

| 구분 | 기능 | 설명 | 예시 |
|------|------|------|------|
| 재구조화 | melt | wide → long, 여러 컬럼을 하나의 변수로 통합 | pd.melt(df, id_vars=['id'], value_vars=['A','B']) |
| 재구조화 | melt 옵션 | 변수 이름/값 컬럼 이름 지정 | pd.melt(df, var_name='항목', value_name='값') |
| 재구조화 | unstack | 인덱스를 열로 변환 (멀티 인덱스 사용 시) | df.groupby(['A','B']).sum().unstack() |
| 재구조화 | pivot_table | 여러 축 기준으로 집계 | df.pivot_table(index='A', columns='B', values='C', aggfunc='sum') |
| 재구조화 | pivot_table 옵션 | 결측치 처리, 다중 통계 | df.pivot_table(..., fill_value=0, aggfunc=['sum','mean']) |

---

# 📌 2. 작업형 2 유형 – 머신러닝 워크플로우 & 상황별 패턴

## 2-1. 문제 정의 – 분류 / 회귀 구분 & 평가 지표

### (1) 문제 유형 판단

| 기준 | 회귀 | 분류 |
|------|------|------|
| 평가지표 | RMSE, MAE, R2 등 | f1, roc_auc, accuracy 등 |
| 타깃 형태 | 연속값 (금액, 수량 등) | 이산값 (0/1, A/B/C 등) |
| 문제 설명 | 예측할 값이 연속적이면 회귀 | 클래스를 맞추는 문제면 분류 |

### (2) 제출 형식 확인

- 제출할 CSV 형태
  - 컬럼 개수
  - 컬럼 이름 (예: id, pred 등)
  - 파일 이름
- test 행 수 == 제출 csv 행 수 반드시 확인

---

## 2-2. 데이터 불러오기 & Pandas EDA

### (1) 데이터 불러오기

- import pandas as pd
- pd.read_csv()로 train / test 불러오기

### (2) EDA 기본 옵션

| 기능 | 설명 | 예시 |
|------|------|------|
| 컬럼 표시 옵션 | 많은 컬럼도 다 보이게 | pd.set_option('display.max_columns', None) |
| 자료형 확인 | train/test 컬럼 타입 확인 | train.info(), test.info() |
| 기초 통계(숫자) | 최소/최대/평균 등 확인 | train.describe() |
| 기초 통계(범주형) | 범주형 요약 | train.describe(include='object') |
| 카테고리 수 비교 | train/test 범주 값 비교 | set(train[col]) == set(test[col]) |
| 데이터 크기 | test 행 수 확인 → 제출 csv 행 수와 일치 | train.shape, test.shape |
| 결측치 | 컬럼별 결측치 수 | train.isnull().sum() |
| numeric_only | sum, corr, mean 등에서 에러 날 때 | df.corr(numeric_only=True) |

---

## 2-3. 데이터 전처리 – 타깃, 인코딩, 결측, 컬럼 삭제, 스케일링

### (1) 타깃 컬럼 분리

```python
target = train.pop('컬럼명')
```

---

### (2) 원-핫 인코딩(One-Hot Encoding)

#### 기본 – train/test 카테고리 같을 때

```python
train = pd.get_dummies(train)
test = pd.get_dummies(test)
```

#### 카테고리 다를 때 – 방법 1 (concat 후 인코딩)

```python
df = pd.concat([train, test])
df = pd.get_dummies(df)
train = df.iloc[:len(train)]
test  = df.iloc[len(train):]
```

#### 카테고리 다를 때 – 방법 2 (align)

```python
train = pd.get_dummies(train)
test  = pd.get_dummies(test)

train, test = train.align(
    test, join='left', axis=1, fill_value=0
)
```

---

### (3) 결측치 처리

#### (a) 결측치 삭제

```python
train = train.dropna()                         # 행 전체 삭제
train = train.dropna(subset=['컬럼1','컬럼2'])  # 특정 컬럼에 결측 있으면 행 삭제
train = train.dropna(axis=1)                   # 결측 있는 컬럼 삭제 (권장 X)
```

#### (b) 결측치 채우기

```python
train['컬럼'] = train['컬럼'].fillna(값)
train['컬럼'] = train['컬럼'].fillna(train['컬럼'].mean())    # 평균
train['컬럼'] = train['컬럼'].fillna(train['컬럼'].median())  # 중앙값
train['컬럼'] = train['컬럼'].fillna(train['컬럼'].mode()[0]) # 최빈값
```

---

### (4) 레이블 인코딩(Label Encoding)

1. 범주형 컬럼만 골라내기

```python
cols = train.select_dtypes(include='object').columns
# 또는
cols = list(train.columns[train.dtypes == object])
# 또는
cols = ['컬럼1', '컬럼2']
```

2. LabelEncoder 불러오기 & 적용

```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
```

- 카테고리가 같을 때 (또는 train 카테고리가 test를 포함할 때):

```python
for col in cols:
    train[col] = le.fit_transform(train[col])
    test[col]  = le.transform(test[col])
```

- 카테고리가 다를 때 (concat 후 인코딩):

```python
df = pd.concat([train, test])

for col in cols:
    df[col] = le.fit_transform(df[col])
```

---

### (5) 컬럼 삭제 기준

```python
train = train.drop('컬럼명', axis=1)
test  = test.drop('컬럼명', axis=1)
```

- 모든 값이 서로 다른 문자형 ID (랜덤한 유니크 문자열) → 보통 삭제 후보
- 상품 ID처럼 의미 있는 식별자일 수도 있으니 무조건 삭제 X
- 최종적으로는 검증 데이터 평가지표를 보고 포함/제외 판단

---

### (6) 스케일링 (StandardScaler / MinMax / Robust)

```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler

scaler = StandardScaler()
train[cols] = scaler.fit_transform(train[cols])
test[cols]  = scaler.transform(test[cols])
```

- 랜덤포레스트, LightGBM 같은 트리 계열 모델은 스케일 영향이 크지 않음

---

## 2-4. 모델 학습 전략 – 입문(빠른제출) vs 기본(검증 포함)

### (1) 공통 – 대표 모델들

| 계열 | 불러오기 | 분류용 | 회귀용 |
|------|----------|--------|--------|
| Ensemble | from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor | RandomForestClassifier() | RandomForestRegressor() |
| LightGBM | import lightgbm as lgb | lgb.LGBMClassifier() | lgb.LGBMRegressor() |
| 선형 | from sklearn.linear_model import LogisticRegression, LinearRegression | LogisticRegression() | LinearRegression() |

### (2) [찐 입문] – 검증 분할 없이 바로 학습 & 제출

```python
model = RandomForestClassifier()   # 또는 Regressor
model.fit(train, target)           # 학습
pred = model.predict(test)         # 예측
# roc_auc를 쓸 때:
proba = model.predict_proba(test)  # 확률 예측
```

- 분류 문제에 회귀 모델, 회귀 문제에 분류 모델 쓰는 실수 조심

---

## 2-5. 기본 전략 – train/val 분할, 평가, 모델 선정

### (1) train/val 분할

```python
from sklearn.model_selection import train_test_split

X_tr, X_val, y_tr, y_val = train_test_split(
    X, y, test_size=0.2, random_state=0
)

X_tr.shape, X_val.shape, y_tr.shape, y_val.shape
```

### (2) 학습 & 검증 예측

```python
model = RandomForestClassifier()   # 혹은 다른 모델

model.fit(X_tr, y_tr)
pred      = model.predict(X_val)
proba_val = model.predict_proba(X_val)  # roc_auc용 (이진분류)
```

### (3) 평가 지표 (metrics)

```python
from sklearn.metrics import *
```

#### 분류

| 지표 | 사용 예시 | 비고 |
|------|-----------|------|
| accuracy | accuracy_score(y_val, pred) | 단순 정확도 |
| roc-auc | roc_auc_score(y_val, proba_val[:,1]) | 양성클래스 확률만 사용 (이진) |
| f1 | f1_score(y_val, pred) | 이진분류 |
| f1 (다중) | f1_score(y_val, pred, average='macro') | micro, macro, weighted 옵션 |
| precision | precision_score(y_val, pred, average='macro') 등 | 다중분류 평균 방식 선택 |
| recall | recall_score(y_val, pred, average='macro') 등 | 〃 |

#### 회귀

| 지표 | 사용 예시 |
|------|-----------|
| MSE | mean_squared_error(y_val, pred) |
| RMSE | root_mean_squared_error(y_val, pred) |
| RMSLE | root_mean_squared_log_error(y_val, pred) |
| MAE | mean_absolute_error(y_val, pred) |
| MSLE | mean_squared_log_error(y_val, pred) |
| R2 | r2_score(y_val, pred) |

---

## 2-6. 최종 test 예측 & 제출 파일 생성

### (1) test 예측

```python
pred = model.predict(test)

# roc_auc를 평가지표로 사용할 때만 확률 사용
proba = model.predict_proba(test)
```

### (2) 제출용 DataFrame 만들기

```python
# 단순 예측값
s = pd.DataFrame({'pred': pred})

# roc_auc_score를 쓸 때 (양성 클래스 1의 확률만)
s = pd.DataFrame({'pred': proba[:, 1]})
```

- 문자 레이블(A/B 등)일 때는 model.classes_로 양성 클래스가 몇 번째 열인지 확인 후 [:, index] 사용

### (3) CSV 저장 & 검증

```python
s.to_csv('result.csv', index=False)

check = pd.read_csv('result.csv')
```

- test 행 수 == 예측 행 수
- 컬럼명/컬럼 수 문제 요구사항과 일치 확인

---

# 📌 3. 작업형 3 유형 – 통계 분석 / 가설검정 치트시트

## 3-1. 전반 원칙

- 작업형 3은 정답이 정해져 있는 통계 검정 문제
- 문제에서 요구한 검정만 수행
- 가설 설정 → 적절한 검정 선택 → p-value 기준으로 결론

---

## 3-2. 단일 표본 검정 (1-Sample Test)

```python
from scipy import stats
```

### (1) 모수 검정 – t검정 (one-sample t-test)

```python
stats.ttest_1samp(표본, 기대값, alternative='two-sided')
```

- alternative: 'two-sided', 'greater', 'less'

### (2) 비모수 검정 – Wilcoxon (단일 표본)

```python
stats.wilcoxon(표본 - 기대값, alternative='two-sided')
```

---

## 3-3. 대응 표본 검정 (Paired Test)

### (1) 대응 t검정 (모수)

```python
stats.ttest_rel(표본1, 표본2, alternative='two-sided')
```

### (2) 대응 Wilcoxon (비모수)

```python
stats.wilcoxon(표본1, 표본2, alternative='two-sided')
```

- D = 표본1 - 표본2 관점
  - 표본1이 더 크다는 대립 → alternative='greater'
  - 표본1이 더 작다는 대립 → alternative='less'

---

## 3-4. 독립 표본 검정 (Independent Two-Sample Test)

### (1) F-검정 (등분산 검정용, 직접 계산)

```python
varA = df['A'].var()
varB = df['B'].var()

f_stat = max(varA, varB) / min(varA, varB)
```

- 합동 분산(공분산 추정):

합동 분산 = ((A그룹 자유도 × A그룹 분산) + (B그룹 자유도 × B그룹 분산)) / (A그룹 자유도 + B그룹 자유도)

- 자유도 = 각 그룹 데이터 수 - 1

### (2) 독립 2표본 t검정 (모수)

```python
stats.ttest_ind(표본1, 표본2,
                alternative='two-sided',
                equal_var=True)  # 또는 False
```

- alternative: 'less', 'greater', 'two-sided'
- equal_var=True → 등분산 가정
- equal_var=False → Welch t-test

### (3) Mann-Whitney U 검정 (비모수)

```python
stats.mannwhitneyu(A, B, alternative='two-sided')
```

### (4) 정규성·등분산성 검정

```python
stats.shapiro(표본)              # 정규성 검정
stats.levene(표본1, 표본2)       # 등분산 검정
```

---

## 3-5. 회귀 분석 (선형 회귀 / 상관 계수)

### (1) 상관계수

```python
df.corr(numeric_only=True, method='pearson')
```

- method='pearson' : 피어슨 상관 (기본)
- method='kendall' : 켄달의 타우
- method='spearman': 스피어만

### (2) 선형 회귀 (OLS)

```python
from statsmodels.formula.api import ols

model = ols('종속 ~ 독립1 + 독립2', data=df).fit()
model.summary()
```

- 독립변수에 C()를 쓸 때:
  - 문제에서 범주형 변수라고 명시된 경우에만 사용
  - 예: ols('y ~ C(group)', data=df)

- 요약 결과 주요 항목:
  - R-squared : 결정계수
  - coef      : 회귀계수
  - P>|t|     : p-value (계수 유의성)

---

## 3-6. 로지스틱 회귀 분석

### (1) 모델 적합

```python
from statsmodels.formula.api import logit

model = logit('종속 ~ 독립1 + 독립2', data=df).fit()
```

### (2) 오즈비(odds ratio)

```python
import numpy as np

np.exp(model.params['변수'])     # 변수 1단위 증가 시 오즈비
np.exp(model.params['변수'] * 5) # 5단위 증가 시 오즈비
```

### (3) 기타 지표

| 항목 | 의미 | 예시 코드 |
|------|------|----------|
| 오류율 | 1 - accuracy | 1 - accuracy |
| AIC | 정보 기준 (작을수록 좋음) | model.aic |
| BIC | 베이지안 정보 기준 | model.bic |
| 잔차 | 모델 잔차(오차) | model.resid |
| 잔차 평균 | 잔차의 평균 | model.resid.mean() |
| 잔차 표준편차 | 잔차의 표준편차 | model.resid.std() |
| 잔차 MSE | 잔차 평균제곱오차 | model.mse_resid |
| 로그 우도 | Log-Likelihood | model.llf |
| 잔차 이탈도 | -2 * model.llf | 로그 우도 기반 |

---

## 3-7. 카이제곱 검정 (적합도 / 독립성·동질성)

### (1) 적합도 검정 (Goodness-of-Fit)

```python
from scipy import stats
stats.chisquare(observed, expected)
```

- 빈도 데이터 사용 (비율 X, 실제 count)

### (2) 독립성 / 동질성 검정

```python
import pandas as pd
from scipy import stats

table = pd.crosstab(변수1, 변수2)
chi2, p, dof, expected = stats.chi2_contingency(
    table, correction=True
)
```

- correction=True: 연속성 보정 (기본값)
- correction=False: 보정 없음

---

## 3-8. 분산분석 (ANOVA)

### (1) 일원 분산분석 (One-Way ANOVA)

#### 방법 1 – SciPy

```python
from scipy import stats
stats.f_oneway(표본1, 표본2, ...)
```

#### 방법 2 – statsmodels (회귀식 표현)

```python
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

model = ols('종속 ~ 독립', data=데이터).fit()
anova_lm(model)
```

- 비모수 대안: Kruskal-Wallis

```python
stats.kruskal(표본1, 표본2, ...)
```

### (2) 이원 분산분석 (Two-Way ANOVA)

```python
import statsmodels.api as sm
from statsmodels.formula.api import ols

model = ols('종속 ~ C(요인1) * C(요인2)', data=데이터).fit()
sm.stats.anova_lm(model, typ=숫자)
```

- typ 옵션:
  - typ=1 : 변수 순서에 따라 분석 (기본)
  - typ=2 : 각 변수의 독립적인 효과 분석
  - typ=3 : 모든 변수와 상호작용을 동시에 고려
