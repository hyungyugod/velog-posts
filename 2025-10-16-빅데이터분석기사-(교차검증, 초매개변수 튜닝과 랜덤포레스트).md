
# 📌 1. 평균제곱오차 (MSE)와 `mean_squared_error()`

## 1-1. 정의  
- **MSE (Mean Squared Error)**: 예측값과 실제값의 차이를 제곱해 평균낸 값.  
- 공식:  
  $$ \text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 $$

## 1-2. 특징  
- 값이 0일수록 완벽한 예측.  
- 큰 오차에 민감.  
- `squared=False` 옵션으로 RMSE(제곱근 MSE) 계산 가능.

---

# 📌 2. 교차 검증과 `cross_val_score()`

## 2-1. 개념  
- 데이터를 여러 번 나누어 모델을 학습·평가 → 일반화 성능을 더 정확히 측정.

## 2-2. 사용법  
```python
cross_val_score(model, X, y, cv=5)
```
- `cv=5`: 5-겹 교차검증  
- 반환: 각 폴드 점수 배열 → 평균이 모델 성능의 지표

---

# 📌 3. `neg_root_mean_squared_error`

## 3-1. 이유  
- RMSE는 작을수록 좋지만, `scikit-learn`은 “클수록 좋은 점수”를 기대 → 음수 반환.  
- 실제 RMSE 확인: `-score.mean()`

---

# 📌 4. 초매개변수 (Hyperparameter)

## 4-1. 정의  
- 학습 전 사람이 정하는 설정값.  
- 예: 학습률, 트리 깊이, 이웃 수 등.  
- 모델의 복잡도, 학습 속도, 일반화 성능을 조절.

---

# 📌 5. 랜덤 포레스트 주요 초매개변수

| 파라미터 | 의미 | 효과 |
|----------|------|------|
| `n_estimators` | 트리 개수 | ↑ 정확도 ↑ 학습시간 |
| `max_depth` | 트리 최대 깊이 | ↓ 과적합 방지 |
| `min_samples_split` | 분할 최소 샘플 수 | ↑ 일반화 |
| `min_samples_leaf` | 리프 최소 샘플 수 | ↑ 일반화 |
| `max_features` | 각 분할 시 사용할 특징 수 | ↑ 다양성 ↑ 성능 |
| `bootstrap` | 복원추출 여부 | 다양성 증가 |
| `class_weight` | 불균형 데이터 보정 | 성능 향상 |

---

# 📌 6. `max_features`를 줄이면 과적합이 줄어드는 이유

- 모든 특성을 사용하면 트리 구조가 비슷해짐 → 다양성 ↓  
- 일부 특성만 사용하면 트리들이 서로 다르게 학습 → **앙상블 효과 ↑** → 과적합 ↓

---

# 📌 7. `ccp_alpha` (가지치기 매개변수)

## 7-1. 개념  
- **비용 복잡도 가지치기(Cost Complexity Pruning)**에서 쓰이는 파라미터.  
- 공식:  
  $$ \text{Cost} = \text{오차} + \alpha \times T $$  
  (T: 리프 노드 수)

## 7-2. 효과  
- `ccp_alpha` ↑ → 가지를 많이 잘라 단순한 트리 → 과적합 ↓  
- `ccp_alpha` ↓ → 트리가 복잡 → 과적합 ↑

---

# 📌 8. SVR (Support Vector Regression)

## 8-1. 개념  
- SVM을 회귀 문제에 적용한 모델.  
- **ε-관(epsilon tube)** 내의 오차는 무시하고, 그 밖의 큰 오차만 줄이는 전략.

## 8-2. 주요 하이퍼파라미터  

| 파라미터 | 역할 |
|----------|------|
| `C` | 오차 허용 정도 (크면 과적합↑) |
| `epsilon` | 무시할 오차 범위 설정 |
| `kernel` | 비선형 변환 방법 |
| `gamma` | RBF 커널 폭 |

---

# 📌 9. `GridSearchCV`

## 9-1. 역할  
- 모든 초매개변수 조합을 탐색 → 최적 조합을 선택 → **최적 모델로 재학습까지 자동 수행**

## 9-2. 주요 속성  

| 속성 | 의미 |
|------|------|
| `best_params_` | 최적 초매개변수 |
| `best_estimator_` | 최적 조합으로 학습된 모델 |
| `best_score_` | 최고 교차검증 점수 |

---

# 📌 10. `grid.predict()` 사용 가능 여부

- 가능하다. `GridSearchCV` 객체는 내부적으로 `best_estimator_`를 호출한다.  
- 즉, 다음 두 코드는 동일:

```python
grid.predict(X_test)
grid.best_estimator_.predict(X_test)
```

- 단, `refit=False`일 경우 학습이 안 되어 있어 `.predict()` 사용 불가.

---


# 📌 11. Random Forest (랜덤 포레스트)

## 1-1. 개념  
랜덤 포레스트(Random Forest)는 **결정 트리(Decision Tree)** 여러 개를 만들어 **앙상블(ensemble)** 기법으로 예측하는 모델이다.  
특히, 각 트리를 만들 때 전체 특성을 사용하는 것이 아니라 **일부 특성만 랜덤하게 선택**하여 학습한다는 점에서 단순 Bagging과 차이가 있다.

---

# 📌 12. 작동 원리

## 12-1. 과정
1. **부트스트랩 샘플링(Bootstrapping)**  
   - 원본 데이터를 무작위 복원추출하여 여러 개의 데이터셋을 생성한다.
2. **특성 부분 선택(Feature Sampling)**  
   - 각 트리마다 일부 특성만 랜덤하게 선택해 학습시킨다.
3. **트리 학습 (Decision Tree)**  
   - 선택된 데이터와 특성으로 여러 개의 결정 트리를 학습한다.
4. **결합 (Aggregating)**  
   - 각 트리의 예측을 평균(회귀) 또는 투표(분류)하여 최종 예측을 만든다.

---

# 📌 13. 일부 특성만 사용하는 이유

| 이유 | 설명 |
|------|------|
| **학습 속도 향상** | 모든 특성을 쓰지 않아 계산량이 줄어듦 |
| **성능 개선** | 서로 다른 특성을 사용해 트리 간 다양성이 증가 |
| **과적합 방지** | 트리들이 비슷하지 않게 되어 Bagging의 분산 감소 효과 극대화 |

---

# 📌 14. 랜덤 포레스트의 장점

- 단일 결정 트리보다 **과적합 위험이 낮고 일반화 성능이 높음**  
- 특성 선택이 자동으로 수행됨  
- 대용량 데이터셋에서도 성능이 안정적  
- 이상치와 결측치에 비교적 강함

---

# 📌 15. 기본 모델 파이프라인 예제

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline

# 전처리 + 모델 파이프라인 구성
full_pipe = Pipeline([
    ("preprocess", preprocess),
    ("regressor", RandomForestRegressor())
])
```

---

# 📌 16. 주요 하이퍼파라미터

| 파라미터 | 설명 |
|----------|------|
| `n_estimators` | 생성할 트리 개수 (많을수록 안정성 ↑, 속도 ↓) |
| `max_depth` | 트리 최대 깊이 (과적합 방지) |
| `max_features` | 각 트리가 학습에 사용할 특성 개수 |
| `min_samples_split` | 노드를 분할하기 위한 최소 샘플 수 |
| `min_samples_leaf` | 리프 노드에 필요한 최소 샘플 수 |
| `bootstrap` | 부트스트랩 샘플링 사용 여부 |
| `ccp_alpha` | 가지치기 강도 (과적합 방지용) |

- `max_features='sqrt'`: 전체 특성 수의 제곱근만 랜덤 선택  
- `max_features='log2'`: 전체 특성 수의 로그2만 랜덤 선택

---
# 📌 17. 하이퍼파라미터 탐색 (GridSearchCV)

```python
import numpy as np
from sklearn.model_selection import GridSearchCV

# 탐색할 파라미터 정의
RandomForest_param = {
    'regressor__n_estimators': np.arange(100, 500, 100),
    'regressor__max_features': ['sqrt'],
    'regressor__random_state': [0]
}

# GridSearchCV 설정
RandomForest_search = GridSearchCV(
    estimator=full_pipe,
    param_grid=RandomForest_param,
    cv=5,
    scoring='neg_mean_squared_error'
)

# 모델 학습 및 탐색
RandomForest_search.fit(train_X, train_y)

# 최적 파라미터 및 점수 확인
print("Best 파라미터 조합:", RandomForest_search.best_params_)
print("교차검증 RMSE score:", RandomForest_search.best_score_)
```

---

# 📌 18. 최종 모델 평가

```python
from sklearn.metrics import mean_squared_error, r2_score

# 최적 모델로 예측
rf_pred = RandomForest_search.predict(test_X)

# 평가 지표 계산
mse = mean_squared_error(test_y, rf_pred)
r2 = r2_score(test_y, rf_pred)

print("테스트 MSE:", mse)
print("결정계수 R²:", r2)
```

예시 출력:
```
Best 파라미터 조합: {'regressor__max_features': 'sqrt', 'regressor__n_estimators': 400, 'regressor__random_state': 0}
테스트 MSE: 9.99
결정계수 R²: 0.91
```

**해석:**  
- `n_estimators=400`일 때 성능이 가장 좋았고,  
- `max_features='sqrt'` 설정으로 과적합 위험을 줄이면서 예측력이 향상되었다.  
- R² = 0.91 → 모델이 전체 변동성의 약 91%를 설명하고 있다는 의미.

---

# 📌 19. 결정계수 (R²) 개념

## 19-1. 정의  
**R² (결정계수)**는 모델이 데이터의 변동성을 얼마나 설명하는지 비율로 나타낸 지표다.

$$
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
$$

- \( SS_{res} = \sum (y_i - \hat{y}_i)^2 \): 잔차 제곱합 (예측 오차)
- \( SS_{tot} = \sum (y_i - \bar{y})^2 \): 총 제곱합 (데이터 전체 변동)

## 19-2. 해석

| R² 값 | 의미 |
|-------|------|
| 1 | 완벽한 예측 |
| 0.8 이상 | 매우 우수한 모델 |
| 0 ~ 0.5 | 설명력이 낮음 |
| 0 이하 | 평균으로 예측하는 것보다 못함 |

**조정된 결정계수**: 변수 개수가 많아져도 설명력이 실제로 향상되지 않으면 값이 낮아지는 보정된 지표.

$$
R^2_{adj} = 1 - \frac{(1 - R^2)(n - 1)}{n - p - 1}
$$

- \( n \): 데이터 수, \( p \): 독립변수 수

---

# 📌 20. 핵심 요약

| 항목 | 내용 |
|------|------|
| 알고리즘 | 여러 결정 트리를 만들어 예측 평균 |
| 차이점 | 전체 특성이 아닌 일부만 랜덤 선택 |
| 장점 | 과적합 방지, 일반화 향상, 다양성 증가 |
| 주요 튜닝 포인트 | `n_estimators`, `max_depth`, `max_features` |
| 평가 지표 | MSE, RMSE, **R² (결정계수)** |
| 실무 팁 | `GridSearchCV`로 최적 파라미터 탐색 후 성능 평가 |

---
