# 📌 0. 모델을 학습한다는 것의 의미
### 0-1. 학습의 본질: 데이터를 통해 **패턴(함수)** 를 찾는 과정

머신러닝에서 "학습한다"는 것은 단순히 데이터를 **외우는 것**이 아니라, 데이터를 통해 숨겨진 **규칙(패턴)** 을 찾아서 새로운 데이터에 대해 **예측하거나 판단할 수 있는 함수**를 만드는 것이다.  
즉, 학습이란 다음 과정을 의미한다:

- 입력(`X`)과 출력(`y`) 간의 관계를 수학적으로 모델링한다.
- 이때 모델이 만든 수학적 함수는 `f(X) ≈ y` 형태로 나타난다.
- 새로운 입력이 들어오면 이 함수를 통해 예측값 `ŷ = f(new_X)`를 계산한다.

---

### 0-2. 학습의 수학적 표현

학습 과정을 수학적으로 쓰면 다음과 같다:

- 목표:  
  주어진 데이터 `(X, y)`에 대해  
  $$ \hat{y} = f(X; \theta) $$  
  를 만족하도록 **파라미터(θ)** 를 학습한다.

- 손실함수(loss):  
  $$ L = \sum (y - \hat{y})^2 $$  
  예측값과 실제값의 차이를 최소화하도록 계수를 조정한다.

- 최적화 과정:  
  $$ \theta^\* = \arg\min_\theta L $$  
  즉, 손실을 최소화하는 계수(파라미터)를 찾는 것이 학습이다.

---

### 0-3. 학습의 핵심: **계수(weight) 조정**

모델은 다음과 같은 형태의 수학식을 내부적으로 학습한다:

$$ \hat{y} = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b $$

- `w1, w2, ... wn`: 모델이 데이터를 통해 찾아낸 **계수(coefficient)**  
- `b`: 절편(bias), 기준점을 조절하는 역할  
- `θ = {w, b}`: 이 계수 집합을 **파라미터(parameter)** 라고 부른다.

**핵심 요약:**  
학습이란 데이터를 보고 이 계수들을 “조정”하여, 입력을 적절한 출력으로 바꾸는 규칙을 만들어내는 과정이다.

---

### 0-4. 학습과 예측의 차이

| 단계 | 역할 | 설명 |
|------|------|------|
| **학습(Training)** | 규칙 찾기 | 기존 데이터로 계수를 조정하여 함수 `f`를 만든다. |
| **예측(Prediction)** | 규칙 적용 | 새 데이터 `X_new`를 `f`에 넣어 `ŷ = f(X_new)`를 계산한다. |

학습 단계에서는 **데이터를 사용해 규칙을 만든다.**  
예측 단계에서는 **만들어진 규칙만 사용**하고, 데이터 자체는 다시 쓰이지 않는다.

---

### 0-5. 새로운 데이터 판단의 의미

“새로운 데이터를 판단한다”는 말은 **학습한 함수에 입력을 넣어 출력을 계산한다**는 뜻이다.

예시:

- 학습:  
  $$ \hat{y} = 0.8x_1 - 0.5x_2 + 2.1 $$
- 새로운 입력:  
  $$ x_1 = 10, x_2 = 4 $$
- 예측:  
  $$ \hat{y} = 0.8(10) - 0.5(4) + 2.1 = 8.1 $$

즉, 새로운 데이터를 보고 “판단한다”는 건 **이미 학습한 수학식에 값을 대입해 결과를 계산**하는 것이다.

---

### 0-6. 학습의 구조

1. **데이터 수집**: 입력(X), 출력(y)을 확보한다.  
2. **전처리**: 스케일링, 결측치 처리 등으로 학습 준비  
3. **학습(Training)**: 모델이 데이터를 통해 계수를 찾는다.  
4. **검증(Validation)**: 조정된 규칙이 잘 작동하는지 확인한다.  
5. **예측(Prediction)**: 새 데이터를 입력해 결과를 산출한다.

---

### 0-7. 학습의 본질
- “학습” = 데이터로부터 **함수(규칙)** 를 스스로 찾아내는 과정  
- “계수 조정” = 데이터를 설명하는 **최적의 파라미터**를 찾는 일  
- “판단” = 그 함수를 **새 데이터에 적용**하여 출력(예측)을 만드는 일

- 인간 학습과 비슷하다:  
- 공부할 때 문제를 외우는 게 아니라 **공식을 익히는 것**이 진짜 학습이다.  
- 시험(새로운 문제)을 보면 **공식을 적용**해서 푸는 것처럼, 모델도 같은 원리로 동작한다.

---

# 📌 1. 정규화
### 1-1. 표준화 - StandardScaler
- 표준화는 데이터의 평균을 0, 표준편차를 1로 조정하여 변수별 스케일을 통일시킨다.
```py
from sklearn.preprocessing import StandardScaler
train_x9 = train_x.copy()
test_x9 = test_x.copy()

train_x9_num = train_x9.select_dtypes('number')
test_x9_num = test_x9.select_dtypes('number')

ss = StandardScaler().set_output(transform = "pandas")

train_x9 = ss.fit_transform(train_x9_num)
test_x9 = ss.transform(test_x9_num)
```

### 1-2. Min-Max 정규화: $\dfrac{(x - min(x))}{(max(x) - min(x))}$ - MinMaxScaler
- 데이터들을 일정 범위안으로 넣을때 사용하며 z-score 방법에 비해 이상치에 매우 민감하다.
```py
from sklearn.preprocessing import MinMaxScaler
train_x10 = train_x.copy()
test_x10 = test_x.copy()

train_x10_num = train_x9.select_dtypes('number')
test_x10_num = test_x9.select_dtypes('number')

ms = MinMaxScaler().set_output(transform = "pandas")

train_x10 = ms.fit_transform(train_x10_num)
test_x10 = ms.transform(test_x10_num)
```

# 📌 2. 이상치 처리
### 2-1. box-plot를 활용한 방법
- warpbreaks.boxplot()에서 boxplot()은 데이터 프레임의 메서드로 해당 데이터프레임의 값을 박스플롯으로 그려준다.
- series, dataframe은 quentile 함수가 없다. 따라서 numpy의 quentile 함수를 사용해야한다.
- quentile 함수에 series와 분위를 넣으면 해당 수를 반환한다. 이후 이상치 기준 값을 구해서 loc에 이상치에 해당하는 값을 기준으로 새로 dataframe을 구성한다.
- 조건은 &혹은 |로 연결한다. 
```py
warpbreaks = pd.read_csv("https://raw.githubusercontent.com/YoungjinBD/data/main/warpbreaks.csv")
warpbreaks.boxplot(column=['breaks'])

Q1 = np.quantile(warpbreaks['breaks'], 0.25)
Q3 = np.quantile(warpbreaks['breaks'], 0.75)

IQR = Q3 - Q1

UC = Q3 + 1.5 * IQR
LC = Q1 - 1.5 * IQR

warpbreaks2 = warpbreaks.loc[(LC <= warpbreaks['breaks']) & (warpbreaks['breaks']<= UC), :]
```

### 2-2. z-score를 이용한 방법
- 이상치를 선별하는 기준은 보통 z > 3이다.
- 3 표준편차 이상인 값들만 선택한다.
```py
upper = warpbreaks['breaks'].mean() + 3 * warpbreaks['breaks'].std()
lower = warpbreaks['breaks'].mean() - 3 * warpbreaks['breaks'].std()

warpbreaks3 = warpbreaks.loc[(warpbreaks['breaks'] <= upper) & (warpbreaks['breaks'] >= lower), :]
```

# 📌 3. 이산화 - KBinsDiscretizer 
### 3-1. 구간의 길이가 같도록 이산화
- bin은 영어에서 “상자, 통, 저장함”이라는 뜻이다. 
- K: K개의 구간
- Bins: 칸(구간)
- Discretizer: “연속형” 데이터를 “이산형(불연속적인)”으로 만드는 도구
- 또 배열로 보기 위해선 뒤에 toarray를 붙여주어야한다.
```py
from sklearn.preprocessing import KBinsDiscretizer
x = np.array([[0,1,1,2,5,10,11,14,18]]).T # 열벡터로 만들기 위해 전치

kbd = KBinsDiscretizer(n_bins=3,
                       strategy = 'uniform')

x_bin = kbd.fit_transform(x).toarray()
```
- 전략을 uniform으로 정하면 이는 모든 구간의 크기를 동일하게 한다는 것을 의미한다.
- fit은 전략을 학습하여 구간 크기가 동일하도록 구간을 나누고 transform을 통해 각 구간에 맞게 데이터를 분배한다.
- 변환을 적용하면 아래처럼 one hot encoding을 적용한 범주형 변수처럼 변경된다.(기본값)
```py
array([[1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [0., 1., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 0., 1.]])
```

### 3-2. 분위수를 기준으로 이산화
- strategy의 기본값은 quatile이다. 이를 지정하면 사분위수를 계산하여 해당 사분위수를 기준으로 범주를 설정하고 범주화한다.
```py
kbd2 = KBinsDiscretizer(n_bins=4,
                       strategy = 'quantile')

x_bin2 = kbd2.fit_transform(x).toarray()
```

### 3-3. 사용자 지정 구간 설정법
- bin_labels 값으로 labels를 주어서 범주에 라벨을 붙일 수 있다.
```py
labels = ['A', 'B', 'C', 'D']

kbd3 = KBinsDiscretizer(n_bins=4,
                       strategy = 'uniform',
                       bin_labels = labels)
```
- cut()는 pandas의 구간화 함수다. 즉, 숫자를 지정한 구간(bin)으로 분류한다.
- x.reshape(-1) x가 원래 2차원 배열(예: (9, 1))이라면 1차원 벡터로 평탄화(flatten)하는 과정이다.
```py
bins = [0,4,7,11,18]
labels = ['A', 'B', 'C', 'D']

pd.cut(x.reshape(-1),
       bins = bins,
       labels = labels)
```

# 📌 4. make_column_transformer와 ColumnTransformer
### 4-1. make_column_transformer
- make_column_transformer = “열별로 다른 변환기를 만드는 함수”
- y = dat['grade'] : 타깃 벡터를 분리한다. 여기선 성적(grade)을 예측하거나 분류하려는 목적이라 가정. y는 Series이다.
- drop=None: 각 범주를 모두 유지한다(기준 범주를 떨구지 않음). 회귀에서는 다중공선성 이슈로 보통 하나를 drop하기도 한다.
- (oe, cat_columns): 범주형 열들엔 원-핫 인코딩 적용.
- (sc, num_columns): 수치형 열들엔 표준화 적용.
- remainder='passthrough': 위에서 지정하지 않은 나머지 열(혹시 있다면)을 그대로 보존. 일반적으로 cat/num으로 다 커버되면 남는 열은 없다.
- 스케일러·인코더 등은 반드시 train에서만 fit하고 test에는 transform만 해야 데이터 누수를 피할 수 있다.
- make_column_transformer()에는 (전처리객체, 전처리를 적용한 컬럼)형식으로 입력한다.
```py
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split

dat = pd.read_csv('https://raw.githubusercontent.com/YoungjinBD/data/main/bda1.csv')
y = dat['grade']
x = dat.drop(['grade'], axis=1)
train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=0)

cat_columns = train_x.select_dtypes('object').columns
num_columns = train_x.select_dtypes('number').columns

oe = OneHotEncoder(sparse_output=False,
                   drop = None,
                   handle_unknown='ignore')

sc = StandardScaler()

mc_transformer = make_column_transformer(
    (oe, cat_columns),
    (sc, num_columns),
    remainder = 'passthrough'
).set_output(transform = 'pandas')

train_x = mc_transformer.fit_transform(train_x)
test_x = mc_transformer.transform(test_x)
```


### 4-2. ColumnTransformer
- make_column_transformer의 인자에 별칭까지 넣을 수 있는 도구이다.
```py
from sklearn.compose import ColumnTransformer
oe = OnehotEncoder(sparse_output=False, handle_unknown='ignore')
ss = StandardScaler()

c_transformer = ColumnTransformer(
    [('onehot', oe, cat_columns),
     ('standard', ss, num_columns)],
    remainder = 'passthrough'
).set_output(transform = 'pandas')

train_x = c_transformer.fit_transform(train_x)
test_x = c_transformer.transform(test_x)
```

# 📌 5. 차원축소
### 5-1. 차원의 저주와 데이터 누수 
- 데이터 희소성: 고차원으로 갈수록 공간을 설명하기 위해서는 무수히 많은 데이터가 필요하다. -> 훈련데이터가 전체 공간을 대표하지 못한다.
- 계산복잡성 증가: 차원이 증가할수록 계산해야할 양이 기하급수적으로 증가한다.
- 과적합: 고차원데이터에서는 과적합 가능성이 높아진다.
- 미래의 테스트가 현재 학습으로 스포되는 것을 데이터 누수라고 하고 이를 예방하기 위해 학습과 테스트를 나눠야 한다.
- 머신러닝에서 **학습(train)**과 **테스트(test)**를 나누는 이유는 간단하다.
- train 데이터: 지금까지 알고 있는 과거 데이터 → 모델을 학습시키는 데 사용
- test 데이터: “아직 모르는 미래 데이터”를 흉내낸 것 → 모델의 성능을 평가하는 데 사용
- 즉, 테스트 데이터는 아직 세상에 나오지 않은 “미래 상황”을 대표한다고 생각해야 한다.

### 5-2. 주성분분석(PCA)
- 주성분 분석이란 데이터의 차원을 줄이면서도 **정보(분산)**를 최대한 보존하기 위해 새로운 축(주성분)을 만드는 기법이다.
- PCA는 분산이 가장 큰 방향을 찾아 새로운 축(주성분)을 만드는 방법인데, 변수마다 단위나 범위가 다르면 분산이 큰 변수(예: 값이 큰 단위)가 주성분을 지배해버린다.
- 그래서 모든 변수를 평균 0, 표준편차 1로 표준화해 스케일을 같게 맞춰야 진짜 데이터 구조를 반영한 주성분을 찾을 수 있다.
- train_pca = pca.fit_transform(train_scaled) 이 줄에서 하는 일은:
- fit → train_scaled에서 공분산 구조를 분석해서 주성분 축(고유벡터)을 찾는다.
- transform → 원래 데이터를 그 주성분 축 위로 투영(projection) 해서 좌표를 구한다.
- 결과적으로 train_pca는 이런 성질을 가진 새로운 행렬이다: 크기(shape): (샘플 수, 선택된 주성분 개수)
- 실수를 입력하면 PCA가 자동으로 분산 누적합이 0.8 이상이 되는 최소 개수의 주성분을 찾아준다. 이때는 svd_solver='full'로 입력해두어야한다.
- PCA 내부에서는 사실상 SVD(특이값 분해, Singular Value Decomposition) 라는 수학적 방법을 사용해서
데이터의 주성분(=공분산 행렬의 고유벡터)을 계산한다.
- svd_solver는 이 SVD를 어떤 방식으로 계산할 것인지를 정하는 옵션이다:
- 'full': 가장 정확하고 고전적인 방법으로 모든 주성분을 계산 (기본적이고 안정적)
- 'randomized': 대규모 데이터에서 빠른 근사 계산
- 'arpack': 일부 주성분만 필요한 경우 (Truncated SVD)
- 'auto': 데이터 크기에 따라 자동으로 방법 선택
```py
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

df = pd.read_csv('https://raw.githubusercontent.com/YoungjinBD/data/main/df_5.csv')
df.columns = ['index', 'X100m', 'Long.jump', 'Shot.put', 'High.jump', 'X400m','X110m.hurdle', 'Discus', 'Pole.vault', 'Javeline', 'X1500m']

df.set_index('index', inplace=True) # 특정 열을 행 이름으로 변경

train, test = train_test_split(df, test_size=0.3, random_state=42) # 실행할때마다 같은 분할 -> 42는 은하수 히치하이커에서 나온 표현때문, 그냥 랜덤 숫자

ss = StandardScaler()

train_scaled = ss.fit_transform(train)
test_scaled = ss.transform(test)

pca = PCA(n_components=0.8,
          svd_solver='full')

train_pca = pca.fit_transform(train_scaled)
test_pca = pca.transform(test_scaled)

print(pca.explained_variance_ratio_)
print(pca.n_components_)
```
- train_pca의 한 행은 아래처럼 형성된다.
- 이때 이는 새로 생긴 각 축으로부터의 거리를 의미한다.
- train_pca에서 첫 번째 주성분(PC1) 값이 크다는 건 그 데이터가 데이터 전체의 주요 패턴을 따르는 방향으로 멀리 떨어져 있다는 의미다.
- 주성분은 원래 변수들의 선형 결합이기 때문에, PC1 값이 크다는 건 여러 변수들의 특정 조합이 평균보다 훨씬 크거나 작다는 것을 나타낸다.
```css
[-2.13, 0.54, 1.01, -0.27]
```