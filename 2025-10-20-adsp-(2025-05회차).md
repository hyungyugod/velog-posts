# 📌 1. ADsp 2025.05 회차

---

## 1-1. 향상도 (Lift)

**정의:**  
예측 모델이나 연관 규칙이 무작위보다 얼마나 더 잘 작동하는지를 나타내는 지표.

**공식 (연관규칙):**  
Lift = P(A ∩ B) / (P(A) × P(B))

**공식 (마케팅 모델):**  
Lift = (모델 응답률) / (전체 응답률)

**해석:**  
- Lift > 1 → 무작위보다 성능 우수  
- Lift = 1 → 무작위 수준  
- Lift < 1 → 무작위보다 못함

---

## 1-2. 회귀 트리에서 분산으로 노드를 분할한다는 의미

- 종속변수가 **연속형**일 때, 트리는 “클래스 순수도” 대신 “분산 감소”를 기준으로 데이터를 나눈다.
- 각 분할 후보마다 그룹 내 **분산(variance)**을 계산하고, **분산이 가장 크게 줄어드는 기준**으로 분할을 선택한다.
- 분산이 작아진다는 것은 그룹 내부 데이터가 서로 더 **유사해지고**, 예측 오차가 작아짐을 의미한다.

---

## 1-3. SOM vs CNN

| 항목 | SOM(Self-Organizing Map) | CNN(Convolutional Neural Network) |
|------|----------------------------|------------------------------------|
| 학습 방식 | 비지도학습 | 지도학습 |
| 목적 | 군집화, 시각화, 패턴 구조 파악 | 이미지·영상 분류, 예측 |
| 입력 | 수치형, 벡터 | 이미지, 영상, 시계열 |
| 출력 | 2D 맵, 군집 구조 | 클래스 확률, 회귀값 |
| 구조 | 입력층-출력맵(격자) | 합성곱층, 풀링층, 완전연결층 |
| 특징 | 데이터 유사성 기반 지도 생성 | 특징 추출 후 분류/회귀 수행 |

- **SOM:** 정답 없이 데이터의 구조를 탐색하고 시각화.  
- **CNN:** 레이블이 있는 데이터에서 패턴을 학습하여 분류/예측 수행.

---

## 1-4. 완전 연결층(fully connected layer)의 의미

- “입력층과 출력층이 전체 연결된다”는 말은 **모든 입력 뉴런이 모든 출력 뉴런과 연결**되어 있다는 뜻이다.
- 출력은 모든 입력의 가중합으로 계산된다:  
  y₁ = w₁₁·x₁ + w₂₁·x₂ + ... + b₁
- 이는 입력의 모든 특징이 예측에 영향을 미친다는 의미이며, CNN에서는 마지막 단계에서 전체 정보를 종합하기 위해 사용된다.

---

## 1-5. 거리 지표 정리

### 5-1. 맨해튼 거리 (Manhattan)
D = Σ |xi - yi|  
격자형 이동거리. 이상치에 강하고 단순.

### 5-2. 체비셰프 거리 (Chebyshev)
D = max(|xi - yi|)  
가장 큰 축 차이에만 집중.

### 5-3. 자카드 거리 (Jaccard)
D = 1 - (공통 요소 / 합집합 요소)  
이진 데이터, 집합 간 유사도 측정에 적합.

### 5-4. 마할라노비스 거리 (Mahalanobis)
D = √((x - μ)ᵀ Σ⁻¹ (x - μ))  
공분산을 고려하여 변수 간 상관관계 반영. 이상치 탐지에 활용.

---

## 1-6. 회귀 분석에서의 해석

### 6-1. Intercept (절편)
- 모든 독립변수가 0일 때 종속변수의 예측값.
- 실제 의미보다 회귀평면의 시작점 역할. 평균 중심화 시 해석력이 커짐.

---

### 6-2. t-value
- 공식: t = (추정계수) / (표준오차)
- 계수가 표준오차 대비 얼마나 큰지 나타내는 값.
- 절댓값이 클수록 “계수가 0일 가능성”이 낮음.
- 부호는 효과의 방향을 의미(양수: 정적, 음수: 부적).

---

### 6-3. Pr(>|t|)
- Pr = Probability(확률)
- 의미: **귀무가설이 참일 때**, 관찰된 t값보다 극단적인 값이 나올 확률.
- 별표(`*`)는 유의성 수준 표시:
  - ***: p < 0.001  
  - **: p < 0.01  
  - *: p < 0.05  
  - .: p < 0.1  
- p값이 작을수록 귀무가설을 기각할 근거가 강해진다.

---

## 1-7. R² vs F-통계량

| 항목 | R² (결정계수) | F-통계량 |
|------|---------------|-----------|
| 성격 | 기술통계 | 추론통계 |
| 의미 | 종속변수 변동 중 모델이 설명하는 비율 | 모델 전체가 유의한지 검정 |
| 해석 | 높을수록 설명력 큼 | p값 작을수록 적어도 한 계수가 0 아님 |
| 한계 | 변수 늘리면 커짐 (과적합 가능성) | 모형 전체만 판단, 변수별 영향은 알 수 없음 |

- R²: “얼마나 잘 설명했나?”  
- F: “설명이 통계적으로 의미 있나?”

---

## 1-8. t값과 계수의 관계
- t값이 크다고 무조건 계수가 큰 것은 아니다.
- 계수가 크더라도 표준오차가 크면 t는 작을 수 있음.
- t는 “계수가 불확실성 대비 얼마나 유의한가”를 나타낸다.

---

## 1-9. p값과 귀무가설의 관계

- **중요:** p ≠ P(귀무가설이 틀릴 확률)  
- p = P(데이터가 이렇게 나오거나 더 극단적으로 나올 확률 | 귀무가설이 참)

즉, 우리가 흔히 알고 싶은 “P(H₀ | 데이터)”가 아니라 “P(데이터 | H₀)”를 말한다.

- p값이 작다 → 귀무가설이 참이라면 이런 데이터가 나올 확률이 작다 → 귀무가설 기각  
- p값이 크다 → 데이터가 귀무가설과 모순되지 않는다 → 기각할 근거 없음

---

## 1-10. 왜 p값은 “t보다 큰 값이 나올 확률”인가?

- t-분포 하에서 “귀무가설이 참일 때” 현재보다 극단적인 t가 나올 확률을 구한다.
- 이게 바로 우리가 본 데이터가 “우연히” 나올 가능성이다.
- p가 작을수록 “우연일 리 없다”는 증거가 된다.

---

## 1-11. p값이 커진다고 해서 귀무가설이 틀릴 확률이 커지지 않는 이유

- p는 조건이 반대다. “귀무가설이 참일 때 데이터가 나올 확률”이지 “귀무가설이 틀릴 확률”이 아니다.
- p값이 크다는 건 단지 “귀무가설과 충돌하지 않는다”는 뜻이지, “맞다”는 증거는 아니다.

---

## 핵심 요약

- **향상도:** 무작위 대비 모델의 효과성 지표  
- **회귀트리 분산:** 그룹 내부 데이터 유사성을 극대화하는 분할 기준  
- **SOM vs CNN:** 비지도 군집화 vs 지도 예측 모델  
- **완전연결층:** 모든 입력이 모든 출력과 연결 → 전체 정보 반영  
- **거리 지표:** 맨해튼(절댓값 합), 체비셰프(최대 차이), 자카드(집합 유사도), 마할라노비스(공분산 반영)  
- **t-value:** 계수가 0과 얼마나 멀리 떨어졌는지 (유의성 판단 핵심)  
- **p-value:** 귀무가설이 참일 때 현재 데이터가 나올 확률  
- **R²:** 설명력 (기술통계)  
- **F-통계량:** 모델 전체 유의성 (추론통계)  
- **p ≠ 귀무가설이 틀릴 확률:** 조건이 반대임을 항상 기억

