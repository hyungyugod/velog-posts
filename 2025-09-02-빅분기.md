# 📌 0. XGBoostd와 LightGBM 
- (Extreme Gradient Boosting)
- LightGBM (Light Gradient Boosting Machine)

| 구분     | XGBoost            | LightGBM                 |
| ------ | ------------------ | ------------------------ |
| 분할 방식  | Level-wise (수평 확장) | Leaf-wise (정보이득 큰 리프 확장) |
| 속도     | 상대적으로 느림           | 훨씬 빠름                    |
| 메모리    | 더 많이 사용            | 메모리 효율적                  |
| 과적합    | 비교적 안정적            | 과적합 위험 있음                |
| 데이터 크기 | 중·소규모 데이터에 적합      | 대규모 데이터에 강함              |

# 📌 1. 경사하강법

| 알고리즘       | 핵심 아이디어          | 장점               | 단점            |
| ---------- | ---------------- | ---------------- | ------------- |
| Batch      | 전체 데이터로 기울기      | 안정적              | 느림            |
| SGD        | 1개 데이터로 기울기      | 빠름, 지역최적 탈출      | 불안정           |
| Mini-batch | 일부 데이터           | 속도+안정성 균형        | 배치 크기 설정 필요   |
| Momentum   | 이전 방향 고려         | 빠르고 안정적          | 최적점에서 튕길 수 있음 |
| NAG        | 미리 앞서서 기울기 계산    | 더 정확한 방향         | 계산량 조금 증가     |
| AdaGrad    | 변수별 학습률 조정       | 드문 변수 학습 잘함      | 학습률 소멸 문제     |
| RMSProp    | AdaGrad 개선       | 딥러닝에 적합          | 하이퍼파라미터 조정 필요 |
| Adam       | Momentum+RMSProp | 자동 학습률 조절, 최고 인기 | 메모리 많이 씀      |
| Nadam      | Adam+NAG         | 빠른 수렴            | 복잡            |

# 📌 2. 지수이동평균과 단순지수평활법

| 구분    | 지수이동평균 (EMA)                        | 단순지수평활법 (SES)                  | 이동평균법 명칭 이유                                         |
| ----- | ----------------------------------- | ------------------------------ | --------------------------------------------------- |
| 정의    | 과거 데이터에 지수적으로 줄어드는 가중치를 주어 평균을 계산   | 지수이동평균을 이용해 **다음 시점 예측값**으로 활용 | 평균을 낼 때 사용하는 구간이 **시간이 흐름에 따라 앞으로 이동**하기 때문에 붙여진 이름 |
| 목적    | 데이터 추세 확인, 노이즈 제거                   | 시계열 데이터 **미래값 예측**             | 데이터 흐름을 따라가면서 추세를 부드럽게 표현                           |
| 사용 분야 | 주가 지표(EMA선), 딥러닝 최적화(Adam, RMSProp) | 수요/매출/날씨 같은 시계열 예측             | 모든 이동평균법 공통 특징                                      |
| 특징    | 최근 데이터 반영 ↑, 과거 데이터 영향 ↓            | 추세·계절성이 없는 평탄한 데이터 예측에 적합      | 구간이 “슬라이딩 윈도우”처럼 움직이며 평균 계산                         |
| 핵심 차이 | “평균 계산” 중심                          | “예측 모델” 중심                     | **‘이동’ = 구간이 계속 앞으로 이동**                            |

# 📌 3. 랜덤포레스트와 배깅

| 구분      | 배깅(Bagging)               | 랜덤포레스트(Random Forest)  |
| ------- | ------------------------- | ---------------------- |
| 기본 아이디어 | 부트스트랩 표본으로 여러 모델 훈련 후 앙상블 | 배깅 + 변수 무작위 선택 추가      |
| 사용 모델   | 어떤 모델이든 가능 (보통 의사결정나무)    | 항상 의사결정나무              |
| 랜덤성     | 데이터 샘플만 무작위               | 데이터 샘플 + 변수 선택 모두 무작위  |
| 효과      | 분산 감소, 과적합 방지             | 분산 감소 + 나무 간 상관 줄여 성능↑ |
