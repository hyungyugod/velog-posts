# ğŸ“Œ 1. t-ê°’ì˜ ì˜ë¯¸ì™€ êµ¬ì¡°
## 1-1. t-ê°’ì˜ ê¸°ë³¸ ì •ì˜

t-ê°’ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤:

$$
t = \frac{\hat{\beta}_1 - \beta_{0}}{SE(\hat{\beta}_1)}
$$

- $\hat{\beta}_1$: í˜„ì¬ í‘œë³¸ì—ì„œ ì¶”ì •ëœ íšŒê·€ê³„ìˆ˜(ê¸°ìš¸ê¸°)  
- $\beta_{0}$: ê·€ë¬´ê°€ì„¤ì´ ì£¼ì¥í•˜ëŠ” ê³„ìˆ˜ ê°’(ë³´í†µ 0)  
- $SE(\hat{\beta}_1)$: í•´ë‹¹ ê³„ìˆ˜ì˜ í‘œì¤€ì˜¤ì°¨(Standard Error) - (í‘œë³¸ì´ ë°”ë€Œë©´ì„œ ë‹¤ë¥´ê²Œ ê³„ì‚°ë˜ëŠ” ì •ë„)

ì¦‰, **"í˜„ì¬ í‘œë³¸ì—ì„œ ì–»ì€ ê³„ìˆ˜ ì¶”ì •ì¹˜"ê°€ "ê·€ë¬´ê°€ì„¤ì´ ì£¼ì¥í•˜ëŠ” ê°’(ë³´í†µ 0)"ê³¼ ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€**ë¥¼, **"ê·¸ ê³„ìˆ˜ê°€ í‘œë³¸ë§ˆë‹¤ í”ë“¤ë¦´ ìˆ˜ ìˆëŠ” ì •ë„(í‘œì¤€ì˜¤ì°¨)"**ë¡œ ë‚˜ëˆˆ ë¹„ìœ¨ì´ë‹¤.

---

## 1-2. í˜„ì¬ ì¶”ì •ëœ ê³„ìˆ˜ë€ ë¬´ì—‡ì¸ê°€

ë‹¨ìˆœíšŒê·€ëª¨í˜•ì„ ìƒê°í•´ë³´ì:

$$
y = \beta_0 + \beta_1 x + \varepsilon
$$

ë°ì´í„°ë¥¼ ì´ìš©í•´ íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ë©´ $\beta_1$ì˜ ì¶”ì •ì¹˜ $\hat{\beta}_1$ë¥¼ ì–»ëŠ”ë‹¤.  
ì´ ê°’ì€ **í˜„ì¬ í‘œë³¸ ë°ì´í„°ê°€ ë§í•´ì£¼ëŠ” "ìµœì„ ì˜ ê¸°ìš¸ê¸°"**ì´ë‹¤.

ì˜ˆì‹œ:  
- $\hat{\beta}_1 = 2.3$  
â†’ $x$ê°€ 1 ì¦ê°€í•  ë•Œ $y$ê°€ í‰ê· ì ìœ¼ë¡œ ì•½ 2.3 ì¦ê°€í•œë‹¤ëŠ” ì˜ë¯¸.

---

## 1-3. ë°ì´í„°ê°€ ë°”ë€Œë©´ ë°”ë€ŒëŠ” ë¶„ì‚°

í•œ ë²ˆë§Œ í‘œë³¸ì„ ìˆ˜ì§‘í•˜ë©´ $\hat{\beta}_1$ì€ í•˜ë‚˜ì§€ë§Œ,  
**í‘œë³¸ì„ ì—¬ëŸ¬ ë²ˆ ë°”ê¿”ì„œ ë°˜ë³µ ì¸¡ì •í•˜ë©´ ë§¤ë²ˆ ì¡°ê¸ˆì”© ë‹¤ë¥¸ $\hat{\beta}_1$ì´ ë‚˜ì˜¨ë‹¤.**  

ì´ë ‡ê²Œ ì¶”ì •ì¹˜ê°€ ë‹¬ë¼ì§€ëŠ” ì •ë„(í”ë“¤ë¦¼)ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒì´ ë°”ë¡œ ë¶„ì‚°ì´ë‹¤:

$$
Var(\hat{\beta}_1) = \frac{\sigma^2}{\sum (x_i - \bar{x})^2}
$$

- $\sigma^2$: ì˜¤ì°¨í•­ì˜ ë¶„ì‚°  
- $\sum (x_i - \bar{x})^2$: $x$ ê°’ë“¤ì˜ í¼ì§ ì •ë„(ë¶„ì‚°ì˜ ë¶„ì ì—­í• )

ì´ ë¶„ì‚°ì˜ ì œê³±ê·¼ì´ ë°”ë¡œ í‘œì¤€ì˜¤ì°¨ë‹¤:

$$
SE(\hat{\beta}_1) = \sqrt{\frac{\sigma^2}{\sum (x_i - \bar{x})^2}}
$$

í˜„ì‹¤ì—ì„œëŠ” $\sigma^2$ë¥¼ ëª¨ë¥´ë¯€ë¡œ ì”ì°¨ë¥¼ ì´ìš©í•œ ì¶”ì •ì¹˜ $s^2$ë¥¼ ì‚¬ìš©í•œë‹¤:

$$
SE(\hat{\beta}_1) = \sqrt{\frac{s^2}{\sum (x_i - \bar{x})^2}}
$$

---

## 1-4. t-ê°’ì´ ì˜ë¯¸í•˜ëŠ” ê²ƒ

ì´ì œ ê²€ì •í†µê³„ëŸ‰ì„ ë‹¤ì‹œ ì“°ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
t = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)}
$$

- **ë¶„ì:** í˜„ì¬ í‘œë³¸ì—ì„œ ê´€ì¸¡ëœ ê³„ìˆ˜ê°€ 0ê³¼ ì–¼ë§ˆë‚˜ ì°¨ì´ ë‚˜ëŠ”ê°€  
- **ë¶„ëª¨:** ê·¸ ê³„ìˆ˜ê°€ í‘œë³¸ë§ˆë‹¤ ì–¼ë§ˆë‚˜ í”ë“¤ë¦´ ìˆ˜ ìˆëŠ”ê°€ (ë¶ˆí™•ì‹¤ì„±)

ì˜ˆì‹œ:  
- $\hat{\beta}_1 = 2.3$  
- $SE(\hat{\beta}_1) = 0.7$

$$
t = \frac{2.3}{0.7} \approx 3.29
$$

â†’ ë¶ˆí™•ì‹¤ì„±(0.7)ì— ë¹„í•´ ì¶”ì •ì¹˜(2.3)ê°€ ì•½ 3.29ë°° í¬ë‹¤ëŠ” ëœ»ì´ë©°, ì´ëŠ” ë‹¨ìˆœí•œ ìš°ì—°ìœ¼ë¡œ ë³´ê¸° ì–´ë µë‹¤.

---

## 1-5. ì§ê´€ì ì¸ ì´í•´: ì‹ í˜¸ ëŒ€ ë…¸ì´ì¦ˆ

- ë¶„ì: **ì‹ í˜¸(signal)** â€“ ìš°ë¦¬ê°€ ë°œê²¬í•œ íš¨ê³¼ì˜ í¬ê¸°  
- ë¶„ëª¨: **ë…¸ì´ì¦ˆ(noise)** â€“ ê·¸ íš¨ê³¼ê°€ ì–¼ë§ˆë‚˜ í”ë“¤ë¦´ ìˆ˜ ìˆëŠ”ê°€

$$
t = \frac{\text{ì‹ í˜¸}}{\text{ë…¸ì´ì¦ˆ}}
$$

â†’ ì‹ í˜¸ê°€ ë…¸ì´ì¦ˆë³´ë‹¤ ì¶©ë¶„íˆ í¬ë©´(ì¦‰, $t$ê°€ í¬ë©´) â€œì´ íš¨ê³¼ëŠ” ì§„ì§œë‹¤â€ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤.

---

## 1-6. ìš”ì•½

- $t$-ê°’ì€ "ê³„ìˆ˜ì˜ í¬ê¸°"ë¥¼ ì§ì ‘ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, "ê³„ìˆ˜ì˜ í¬ê¸° vs ë¶ˆí™•ì‹¤ì„±(SE)"ì˜ ë¹„ìœ¨ì„ ë³´ëŠ” ê²ƒì´ë‹¤.  
- ì´ ë¹„ìœ¨ì´ í¬ë©´ â†’ íš¨ê³¼ê°€ ìš°ì—°ì´ ì•„ë‹ ê°€ëŠ¥ì„±ì´ ë†’ìŒ â†’ ê·€ë¬´ê°€ì„¤ ê¸°ê°.  
- ì´ ë¹„ìœ¨ì´ ì‘ìœ¼ë©´ â†’ íš¨ê³¼ê°€ ë°ì´í„°ì˜ í”ë“¤ë¦¼ ì†ì—ì„œ ì„¤ëª… ê°€ëŠ¥ â†’ ê·€ë¬´ê°€ì„¤ ì±„íƒ.

---

# ğŸ“Œ 2. ì•„ì´ë¦¬ìŠ¤ ë°ì´í„° ë¶„ì„
## 2-1. ì•„ì´ë¦¬ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬
```py
from sklearn.datasets import load_iris
import pandas as pd

iris = load_iris()

# ê¸°ì¡´ iris ë°ì´í„°ì— ë©”íƒ€ë°ì´í„°ì²˜ëŸ¼ iris["feature_names"]ë¡œ íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë”°ë¡œ ë§Œë“¤ì–´ë‘ì—ˆë‹¤.
df_iris = pd.DataFrame(iris["data"], columns=iris["feature_names"])
df_iris["label"] = iris["target"]
```

---

## 2-2. í›ˆë ¨(+ê²€ì¦), í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
- model_selectionìœ¼ë¡œ trainë°ì´í„°ì™€ test ë°ì´í„°ë¥¼ ì„ íƒí•˜ì—¬ ë¶„ë¦¬í•  ìˆ˜ ìˆë‹¤.
- ì¼ë‹¨ train, test ë‘˜ë¡œ ë‚˜ëˆ„ë©´ ì´ëŠ” hold-out ê¸°ë²•ì´ê³  ì¼ë‹¨ ì´ë ‡ê²Œ ë¶„ë¦¬í•œë‹¤.
```py
from sklearn.model_selection import train_test_split # hold out

y = df_iris["label"]
X = df_iris.drop(columns="label") # ëŒ€ë¬¸ì xê°€ ë²¡í„°ë¥¼ ì˜ë¯¸
print(y.shape, X.shape)

train_x, test_x, train_y, test_y = train_test_split(X,y,train_size = 0.8, random_state=42) # íŒŒë ˆí†  í™•ë¥ 
print(train_x.shape, test_x.shape,train_y.shape, test_y.shape)
```

---

## 2-3. ì¼ë°˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ì‚¬ìš©
- dt_clf.get_params()ë¥¼ í†µí•´ íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•´ë³¼ ìˆ˜ ìˆë‹¤.
- ì˜ì‚¬ê²°ì • ë‚˜ë¬´ëŠ” tree íŒ¨í‚¤ì§€ì— ìˆë‹¤.
```py
from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(random_state = 42)
dt_clf.fit(train_x, train_y)
```
- ë²”ì£¼í˜• ë¶„ë¥˜ì´ë¯€ë¡œ roc_auc_scoreë¡œ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•œë‹¤. test_y, pred_y ìˆœìœ¼ë¡œ ì…ë ¥í•œë‹¤.
- accuracy_scoreëŠ” ì •í™•ë„ë¥¼ í‰ê°€í•˜ëŠ” ì§€í‘œì´ë‹¤.
- ë‹¤ì¤‘ë¶„ë¥˜ì´ê¸° ë•Œë¬¸ì— predict ëŒ€ì‹  predict_proba(í™•ë¥ í‘œ)ë¥¼ ì‚¬ìš©í•˜ì—¬ì•¼í•œë‹¤.
- ë˜í•œ 1ëŒ€ ë‹¤ ë¹„êµë¥¼ ìœ„í•´ ovrì„ ë§¤ê°œë³€ìˆ˜ë¡œ ë„£ëŠ”ë‹¤.
- roc_auc_scoreì™€ ê°™ì€ í‰ê°€ì§€í‘œëŠ” metrics íŒ¨í‚¤ì§€ì— ì¡´ì¬í•œë‹¤. 
```py
from sklearn.metrics import roc_auc_score

pred_y = dt_clf.predict_proba(test_x) # ë‹¤ì¤‘ë¶„ë¥˜ì´ê¸° ë•Œë¬¸ì— predict ëŒ€ì‹  predict_probaë¥¼ ì‚¬ìš©í•˜ì—¬ì•¼í•œë‹¤.

print(roc_auc_score(test_y, pred_y, multi_class='ovr'))
```

---

## 2-3. ëœë¤í¬ë ˆìŠ¤íŠ¸ ì‚¬ìš©
- ë°ì´í„°ë¥¼ ë¶„í• í•˜ëŠ” GridSearchCVì™€ StratifiedKFoldëŠ” trian_test_splitê³¼ í•¨ê»˜ model_selection íŒ¨í‚¤ì§€ì— ìˆë‹¤.
- ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ì•™ìƒë¸” ê¸°ë²•ì´ë¯€ë¡œ ensemble íŒ¨í‚¤ì§€ì— ìˆë‹¤.
- n_estimators ì¦‰ íŠ¸ë¦¬ê°œìˆ˜ëŠ” 100ì´ ê¸°ë³¸ê°’ì´ë‹¤.
- ë„ˆë¬´ ë§ì€ ì´ˆë§¤ê°œë³€ìˆ˜ í›„ë³´ë¥¼ ì£¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¬ë¯€ë¡œ ì ë‹¹íˆ 1~3ê°œë§Œ í•˜ë©´ 1ë¶„ ì´ë‚´ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆëŠ”ê²ƒ ê°™ë‹¤.
- ë‹¤ì¤‘ ë¹„êµì¼ë•ŒëŠ” ì¸µí™”ì¶”ì¶œí•˜ì—¬ ê° í´ë˜ìŠ¤ë¥¼ ê³¨ê³ ë£¨ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.
- OVRì„ ì‚¬ìš©í•˜ë©´ ì¼ëŒ€ë‹¤ë¡œ ë‹¤ì¤‘ë¶„ë¥˜ë¬¸ì œë¥¼ ì´ì§„ë¶„ë¥˜ì²˜ëŸ¼ ë‹¤ë£¨ê²Œ ëœë‹¤.
```py
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold # cross validation

rf_clf = RandomForestClassifier(random_state=42, n_jobs=-1) # ë³‘ë ¬ë¡œ ëŒë¦¬ê¸° -> ì•ˆí•˜ë‹ˆê¹Œ grid_searchí• ë•Œ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦¼

randomforest_param = {
    "n_estimators": [100, 200, 400],         # íŠ¸ë¦¬ ê°œìˆ˜ -> 100ì´ ê¸°ë³¸ê°’
    "max_features": ["sqrt"]            # íŠ¹ì„± ìƒ˜í”Œë§ ë¹„ìœ¨/ì „ëµ
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ì •ë ¬ëœ ìƒíƒœê°€ ì•„ë‹Œ ì„ì¸ ìƒíƒœì—ì„œ ìƒ˜í”Œë§

rf_clf_search = GridSearchCV(
    estimator = rf_clf,
    param_grid = randomforest_param, # íƒìƒ‰í•  íŒŒë¼ë¯¸í„°ê°€ ë§ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¼
    cv = cv,
    scoring = "roc_auc_ovr",
    n_jobs=-1
)

rf_clf_search.fit(train_x, train_y) # ê·¸ëŸ¼ì—ë„ hold out ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤

pred_y = rf_clf_search.predict_proba(test_x) # ë‹¤ì¤‘ë¶„ë¥˜ì´ê¸° ë•Œë¬¸ì— predict ëŒ€ì‹  predict_probaë¥¼ ì‚¬ìš©í•˜ì—¬ì•¼í•œë‹¤.

print(roc_auc_score(test_y, pred_y, multi_class='ovr'))
print(rf_clf_search.best_params_, rf_clf_search.best_estimator_) # {'max_features': 'sqrt', 'n_estimators': 200} RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)
```

---

# ğŸ“Œ 3. kaggle ë°ì´í„° ë¶„ì„ êµ¬ì„±
## 3-1. ì œê³µ ë°ì´í„° êµ¬ì„±
- train íŒŒì¼: x, y í¬í•¨í•œ ë°˜ë©´
- test íŒŒì¼: x ë§Œ ì¤€ë‹¤. yê°’ì€ kaggleì—ì„œ ê°€ì§€ê³  ìˆê³  ì´ yê°’ì„ ì˜ˆì¸¡í•˜ëŠ” êµ¬ì„±ì´ë‹¤.
- submission íŒŒì¼: ì •ë‹µë§Œ í‘œê¸°í•´ì„œ í•©ì³ì£¼ëŠ” ì •ë‹µì§€ì´ë‹¤.