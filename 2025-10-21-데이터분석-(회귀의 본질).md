# 📌 1. 회귀·SST·SSR·SSE 핵심

## 1-1. 평균에서 출발하는 회귀의 본질
- **선형회귀는 “평균이라는 출발점($y$절편)”에서 시작하여 “설명변수들이 만드는 벡터 공간($\mathrm{col}(X)$)”으로 이동한 점($\hat y$)을 찾는 과정이다.**  
- 이 점까지의 이동(SSR)이 우리가 **설명한 부분**이며, 남은 거리(SSE)는 아직 **설명하지 못한 부분**이다.  
- 즉, 회귀는 단순한 함수 맞추기가 아니라, 데이터 벡터를 “평균에서 시작해 설명공간으로 정사영(projection)하는 과정”이다.
- 이때 잔차들은 회귀식이 만드는 설명공간과 직교하므로 완벽한 설명을 위해선 새로운 축, 즉 새로운 변수가 필요하다.

## 1-2. 독립변수 = 새로운 방향 벡터
- 독립변수는 평균에서 뻗어나가는 새로운 **기저 벡터**이며, 회귀식은 이들의 **선형결합**이다.  
- 절편은 정보가 없는 상태에서의 기준점이고, 독립변수는 그 기준점에서 출발한 **정보의 방향성**을 나타낸다.

## 1-3. 변동 분해의 기하학적 구조
- **SST (Total Sum of Squares):** 실제값과 평균의 차이 → 데이터 전체 변동량.  
- **SSR (Regression Sum of Squares):** 예측값과 평균의 차이 → 설명된 변동.  
- **SSE (Error Sum of Squares):** 실제값과 예측값의 차이 → 설명되지 않은 변동.  
- 관계식:  
  $$
  SST = SSR + SSE
  $$

## 1-4. 예측과 잔차의 직교 조건
- 최소제곱해를 구하는 과정은 “잔차 벡터와 설명공간이 직교하도록 하는 조건”을 세우는 것이다.  
- 수학적으로는 다음을 만족하도록 $\hat\beta$를 찾는다:
  $$
  X'(y - X\hat\beta) = 0
  $$
- 이 식은 잔차 벡터 $e = y - X\hat\beta$ 가 $X$의 각 열벡터(설명변수 축)와 직교한다는 뜻이다.  
- 직교 조건은 곧 SSE를 최소화하는 조건이며, 이는 데이터가 설명공간에 **직교투영**되었음을 의미한다.
- 또한 이렇기 때문에 $SSR$과 $SSE$ 는 항상 합쳐서 $SST$가 된다.

## 1-5. 직교투영과 결정계수
- 평균을 제거한 데이터 벡터 $y_c$는 다음과 같이 분해된다:
  $$
  y_c = \hat{y}_c + e
  $$
- 이때 $\hat{y}_c$는 $y_c$가 설명공간 $\mathrm{col}(X)$ 위로 정사영된 벡터이며, $e$는 그 공간에 직교하는 잔차다.  
- 투영된 벡터의 길이는 다음과 같다:
  $$
  \|\hat y_c\| = \|y_c\|\cos\theta
  $$
- 따라서 결정계수는 다음과 같이 표현된다:
  $$
  R^2 = \frac{SSR}{SST} = \cos^2\theta
  $$
- 이는 곧 데이터 벡터가 설명공간과 얼마나 같은 방향을 향하는지를 나타낸다.

## 1-6. 회귀의 본질
- 회귀는 “평균에서 시작해 설명변수들이 만드는 벡터공간 위로 데이터를 투영하고, 잔차를 직교화하여 SSE를 최소화하는 과정”이다.  
- **SSR:** 평균에서 예측까지 이동한 설명량  
- **SSE:** 예측에서 실제까지 남은 오차  
- **$R^2$:** 전체 변동 중 설명된 비율 = 벡터 각도의 코사인 제곱

<br>
<br>
<br>

# 📌 2. 자유도 → 평균제곱 → 에너지 밀도 → F → p-value 

## 2-1. 자유도(df): 변동이 퍼질 수 있는 차원의 수

자유도는 **데이터가 제약 없이 움직일 수 있는 축(차원)의 개수**를 뜻한다.  
예를 들어, 점이 3차원 공간에서 다음과 같이 있다고 하자:

- A = (2, 5, 7)  
- B = (3, 4, 8)  
- C = (6, 5, 9)

이때 각 좌표축 $(x, y, z)$는 독립적인 차원을 의미한다.  
하지만 평균을 한 번 정하면(예: $ \bar{z} $를 고정) $x$와 $y$를 정하는 순간 $z$도 자동으로 결정된다.  
즉, **세 축이 있지만 자유롭게 변할 수 있는 축은 $3 - 1 = 2$개**뿐이다. 이것이 총 자유도 $n - 1$의 의미다.

- **총 자유도 (Total df):** $n - 1$  
  → 평균을 고정하면 한 차원이 결정되므로 총 변동의 자유도는 하나 줄어든다.
- **회귀 자유도 (Regression df):** $p$  
  → 설명공간(col(X))의 차원 수, 즉 독립변수의 개수만큼 벡터가 자유롭게 움직일 수 있다.
- **오차 자유도 (Error df):** $n - p - 1$  
  → 전체 공간에서 설명된 축을 제외하고 남은 잔차공간의 차원이다.

항상 다음이 성립한다:

$$
n - 1 = p + (n - p - 1)
$$

---

## 2-2. 평균제곱(Mean Square): 한 차원당 평균 변동량

제곱합(SS)은 데이터 개수에 따라 커지기 때문에, 공정한 비교를 위해 차원의 개수(자유도)로 나눈다.  
이렇게 하면 “한 축(차원)에서 평균적으로 발생하는 변동”을 볼 수 있다.

- 회귀 평균제곱:  
  $$
  MSR = \frac{SSR}{p}
  $$  
  → 독립변수 하나(축 하나)당 평균적으로 설명한 변동량

- 오차 평균제곱:  
  $$
  MSE = \frac{SSE}{n - p - 1}
  $$  
  → 잔차공간 한 축당 평균적으로 남은 변동량

---

## 2-3. 에너지 밀도 해석: 공간당 에너지의 농도

벡터의 제곱합(SS)은 물리적으로 보면 에너지 총량과 같다.  
자유도로 나누면 “한 축당 에너지”가 되는데, 이것이 바로 **에너지 밀도**다.

| 항목 | 의미 | 해석 |
|------|------|------|
| $SSR$ | 설명된 총 에너지 | 모델이 끌어온 전체 변동량 |
| $MSR$ | 설명공간 한 축당 에너지 밀도 | 한 변수당 설명력 |
| $SSE$ | 설명되지 않은 총 에너지 | 남은 변동량 |
| $MSE$ | 잔차공간 한 축당 에너지 밀도 | 평균 잔차 크기 |

예를 들어, 3차원 공간에서 설명공간이 2차원 평면이라면 $SSR$은 그 평면 위에서 설명한 총 에너지이고, $MSR$은 그 **2개의 축당 에너지 농도**다.

---

## 2-4. F-통계량: 에너지 밀도의 비율

F는 두 에너지 밀도를 비교하는 지표다:

$$
F = \frac{MSR}{MSE} = \frac{SSR / p}{SSE / (n - p - 1)}
$$

- 분자: 설명공간 한 축당 평균 변동량  
- 분모: 잔차공간 한 축당 평균 변동량  

직관적으로 해석하면 다음과 같다:

- $F \approx 1$: 독립변수가 없어도 이 정도 비율은 우연히 나온다.  
- $F \gg 1$: 독립변수가 없었다면 이런 비율은 거의 나오지 않는다 → 모델이 유의미하다.

---

## 2-5. F-분포와 p-value: 우연일 확률을 측정

- 귀무가설 $H_0$: 모든 계수 $\beta_1 = \beta_2 = \dots = \beta_p = 0$  
- 귀무가설이 참이라면 $MSR$과 $MSE$는 비슷해져서 $F$는 1 근처에서 분포한다.
- 실제 계산한 $F$가 크다면, 귀무가설 아래에서 그런 값이 나올 확률(꼬리 확률)은 작아진다.

이 확률이 바로 **p-value**다.

- $p > 0.05$: 귀무가설 기각 불가 (설명력이 유의하지 않다)  
- $p < 0.05$: 귀무가설 기각 (설명력이 통계적으로 유의하다)

---

## 2-6. 전체 구조 요약

| 단계 | 개념 | 의미 |
|------|------|------|
| 자유도 | $n - 1 = p + (n - p - 1)$ | 변동이 퍼질 수 있는 축(차원)의 수 |
| 평균제곱 | $MSR$, $MSE$ | 한 축당 변동의 평균 크기 |
| 에너지 밀도 | $MSR$, $MSE$ | 공간당 에너지의 농도 |
| F-통계량 | $F = MSR / MSE$ | 두 공간의 에너지 밀도 비율 |
| p-value | $P(F > F_{obs})$ | 그 비율이 우연히 나올 확률 |

---

-핵심 문장:  
**F-검정은 “설명공간의 한 축(차원)당 변동이 잔차공간보다 유의하게 큰가?”를 묻는 것이며, p-value는 “그런 비율이 그냥 절편만 있는 차원에서 우연히 나올 확률”을 나타낸다.**  
벡터 좌표계에서 보면, 자유도는 우리가 움직일 수 있는 축의 수이며, 평균제곱은 각 축에서 평균적으로 발생한 에너지이고, F는 이 에너지의 비율을 비교하는 검정이다.

<br>
<br>
<br>

# 📌 3. t-분포, p-value, 그리고 전체 모형 유의성의 의미

## 3-1. 전체 모형의 유의성과 개별 변수의 유의성은 다르다

회귀 분석에서 F-검정은 다음의 귀무가설을 세운다:

- $H_0$: 모든 계수 $\beta_1 = \beta_2 = \dots = \beta_p = 0$ (즉, 어떤 변수도 필요 없다)

이 검정은 **"모델 전체가 평균 예측보다 유의하게 나은가?"**를 묻는다.  
따라서 $R^2$가 작거나 개별 변수의 t-값이 유의하지 않아도,  
**변수 전체가 함께 평균 예측을 넘어서는 설명력을 보이면 F-검정에서는 유의미**하다고 판단된다.

예시:

- $R^2 = 0.03$ (설명력은 낮음)  
- 그러나 $p < 0.05$ (F-검정 유의) → 평균만 쓰는 모델보다 낫다는 뜻

이는 곧 "작은 효과라도 우연히 일어날 확률이 매우 낮으면 전체 모델은 유의미하다"는 것을 의미한다.

---

## 3-2. 표준오차와 t-통계량의 의미

개별 변수의 유의성을 판단할 때 우리는 다음 검정을 수행한다:

- $H_0: \beta_i = 0$

그리고 t-통계량을 계산한다:

- $t = \dfrac{\hat{\beta}_i}{SE(\hat{\beta}_i)}$

여기서 표준오차 $SE(\hat{\beta}_i)$는 **계수 추정값의 불확실성**을 나타낸다.

- 잔차 분산 $\sigma^2$가 클수록 → 데이터가 산개 → 불확실성 ↑  
- 독립변수 분산이 작을수록 → 정보 부족 → 불확실성 ↑

따라서 $t$는 단순한 크기 비교가 아니라 **"신호 대 잡음 비율"**이다.  
$|t|$가 클수록 계수가 단순한 우연이 아니라 실제 효과일 가능성이 높아진다.

---

## 3-3. t-분포의 탄생

우리가 모르는 모집단 분산 $\sigma^2$를 표본으로부터 추정하기 때문에  
분모(표준오차)는 확률적으로 흔들리는 값이 된다.  
이로 인해 정규분포보다 꼬리가 두꺼운 **t-분포**가 등장한다.

- 자유도가 작을수록 → 표본 분산의 변동성 ↑ → 꼬리 두꺼움  
- 자유도가 커질수록 → 표본 분산이 안정 → 정규분포에 수렴

자유도는 $n - p - 1$이며, 이는 “모수 추정에 사용한 축의 개수(p+1)”를 제외한 정보량을 의미한다.

---

## 3-4. p-value의 확률적 의미

계산된 $t$를 t-분포 위에 올려놓고 해석한다:

- $p$는 “귀무가설이 참이라면 이 정도 혹은 더 극단적인 $t$값이 나올 확률”이다.  
- $p < 0.05$라면, “이건 우연히 생길 가능성이 5% 미만” → 귀무가설 기각

즉, $p$는 단순히 “작다/크다”가 아니라,  
**“데이터가 우연으로 설명되기 어려울 정도로 특이하다”**를 의미한다.

---

## 3-5. 전체 구조 요약

| 검정 | 질문 | 귀무가설 | 통계량 | 해석 |
|------|------|----------|--------|------|
| F-검정 | 모델 전체가 유의한가? | $\beta_1 = \cdots = \beta_p = 0$ | $F = MSR / MSE$ | 평균 모델보다 나으면 유의미 |
| t-검정 | 특정 계수가 유의한가? | $\beta_i = 0$ | $t = \hat{\beta}_i / SE(\hat{\beta}_i)$ | 특정 축이 의미 있는가 |

---

-핵심 요약:

- 전체 모형은 개별 변수 유의성과 무관하게 유의할 수 있다.  
- t-검정은 계수의 “신호 대 잡음 비율”을 통해 개별 변수가 의미 있는지 판단한다.  
- t-분포는 표본 분산의 불확실성 때문에 등장하며, 자유도가 클수록 정규분포에 가까워진다.  
- p-value는 “귀무가설이 참일 때 이 정도 결과가 나올 확률”을 뜻하며, 작을수록 우연의 가능성이 낮다.

<br>
<br>
<br>

# 📌 4. 자유도 → MSE → 표준오차(SE) → t·F 검정으로 이어지는 구조

## 4-1. 추정이란 “축을 고정한다”는 뜻이다

회귀 분석에서 어떤 파라미터를 **추정한다**는 것은 단순히 계산했다는 뜻이 아니라,  
데이터가 움직일 수 있는 차원을 하나 줄였다는 의미이다.  
예를 들어:

- 평균 하나를 추정 → 자유도 1 소모 → 자유도 = n - 1  
- 절편과 기울기 두 개를 추정 → 자유도 2 소모 → 자유도 = n - 2  
- 절편 + p개의 계수를 추정 → 자유도 (p + 1) 소모 → 자유도 = n - p - 1

즉, **추정 = 제약 부여 = 자유도 감소**라는 구조가 항상 성립한다.

---

## 4-2. SSE에서 MSE로: 자유도 반영한 오차 분산 추정

잔차 제곱합(SSE)은 모델이 설명하지 못한 변동량을 나타낸다:

- SSE = Σ(yᵢ - ŷᵢ)²

하지만 이 자체로는 데이터 수나 추정된 파라미터 개수를 고려하지 않기 때문에,  
공정한 비교를 위해 잔차의 자유도로 나눈 평균 제곱오차(MSE)를 사용한다:

- MSE = SSE / (n - p - 1)

여기서 (n - p - 1)은 모델이 추정한 파라미터 수(p + 1)를 제외하고 잔차가 자유롭게 움직일 수 있는 차원이다.  
즉, MSE는 “잔차 한 축당 평균 변동”이며, 오차 분산의 표본 추정량이다.

---

## 4-3. 표준오차(SE): 불확실성의 크기

계수 추정값은 표본마다 달라지므로 불확실성을 가진다.  
이 불확실성을 정량화한 것이 표준오차(SE)이며, 다음과 같이 계산된다:

- Var(β̂) = MSE × (XᵀX)⁻¹  
- SE(β̂) = √Var(β̂)

즉, 표준오차는 두 가지 정보로 만들어진다:

1. **MSE** – 잔차 변동의 크기 (데이터의 불확실성)  
2. **(XᵀX)⁻¹** – 독립변수의 정보량 (분산·공분산 구조)

MSE가 클수록, 또는 X의 정보가 부족할수록 SE는 커진다.  
SE가 크다는 것은 계수가 표본마다 요동친다는 뜻이다.

---

## 4-4. t-검정의 분모가 SE인 이유

t-검정은 계수의 절대 크기가 아니라 **신호 대 잡음 비율**을 본다:

- t = β̂ᵢ / SE(β̂ᵢ)

SE가 바로 그 “잡음”이다.  
즉, MSE에서 출발한 표준오차를 통해 계수가 단순히 커서 유의한 것인지,  
아니면 표본의 불확실성을 고려해도 여전히 크다고 말할 수 있는지를 판단한다.

이때 SE의 근원은 MSE이며, MSE의 근원은 SSE이고, SSE의 자유도는 “추정된 파라미터 개수”로부터 온다.  
모든 것이 연결되어 있다.

---

## 4-5. F-검정의 분모도 MSE인 이유

F-검정은 설명된 변동과 설명되지 않은 변동의 평균 에너지를 비교한다:

- F = (SSR / p) / (SSE / (n - p - 1))

여기서 SSE / (n - p - 1)이 바로 MSE이며, “잔차 1차원당 평균 변동”이다.  
단순히 SSE를 비교하는 대신, 자유도를 고려한 평균 에너지로 비교해야 모델 간 공정한 판단이 가능하다.

- SSR / p → 설명된 변동의 평균 (축 하나당 설명력)  
- SSE / (n - p - 1) → 설명되지 않은 변동의 평균 (잔차 1축당 변동)

이 비교를 통해 모델이 단순히 우연히 좋은 것인지, 진짜로 설명력이 있는지를 알 수 있다.

---

## 4-6. 전체 구조 요약

회귀의 통계 검정은 아래처럼 하나의 흐름으로 연결된다:

1. **추정**: 파라미터를 고정 → 자유도 감소  
2. **SSE**: 설명되지 않은 총 변동  
3. **MSE**: SSE를 자유도로 나눈 평균 변동  
4. **SE**: MSE를 기반으로 계수의 불확실성 계산  
5. **t**: 계수 / SE → 개별 변수 유의성 판단  
6. **F**: (SSR/p) / (SSE/(n-p-1)) → 전체 모델 유의성 판단

---

-핵심 문장:

> 회귀에서 자유도, 잔차, MSE, 표준오차, t·F 통계량은 모두 하나의 수학적 고리 위에 있다.  
> 파라미터를 추정하는 순간 자유도가 줄고, 그 줄어든 자유도를 기준으로 변동을 평균화한 값이 MSE이며,  
> 이 MSE가 다시 표준오차와 검정 통계량을 만들어낸다.

<br>
<br>
<br>

# 📌 5. 공선성, (XᵀX), 고유값·고유벡터, 조건수

## 5-1. (XᵀX)는 데이터 공간의 기하학이다

회귀에서 등장하는 (XᵀX)는 단순한 계산용 행렬이 아니다.  
이 행렬은 독립변수들이 만드는 **데이터 공간의 구조**를 그대로 담고 있다.

- **대각 원소**: 각 변수의 제곱합 → “변수의 분산(퍼짐 정도)”  
- **비대각 원소**: 두 변수의 곱의 합 → “변수 간 공분산(함께 움직임)”

즉, (XᵀX)는 데이터가 어떻게 퍼져 있고 서로 어떤 각도로 기울어져 있는지를 보여주는 **공간의 지도**다.

---

## 5-2. 기울기(geometry)와 공선성

데이터 공간의 축(변수 벡터)이 서로 직각이라면(내적=0), 각 변수는 독립적인 정보를 제공한다.  
하지만 두 벡터가 서로 평행하거나 매우 가까운 방향이라면 정보가 겹친다.

- 축이 직각 → 정보 독립 → 공간이 안정 → 추정 안정  
- 축이 평행 → 정보 중복 → 공간이 기울어짐 → 추정 불안정

이렇게 축이 기울어져 있을 때, (XᵀX)는 **특이(singular)**에 가까워지고 역행렬이 불안정해진다.  
이는 계수 추정 β̂가 데이터의 작은 변화에도 크게 흔들린다는 뜻이다.

---

## 5-3. 기하학적 의미: 좌표계가 찌그러진다

벡터 공간을 좌표계로 보면, 변수들이 직교할 때는 좌표 분리가 쉽다.  
하지만 축이 기울어져 있으면 하나의 좌표를 바꾸는 것이 다른 좌표에도 영향을 준다.  
이는 회귀계수 추정이 “민감한 좌표 해석”이 되는 것을 의미한다.

예시:

- 축이 독립적이면: β₁, β₂ 각각이 독립적으로 y를 설명  
- 축이 기울어져 있으면: β₁과 β₂의 영향이 섞여서 정확히 분리하기 어렵다

---

## 5-4. 고유값·고유벡터란 무엇인가

행렬 A(예: XᵀX)는 벡터를 변형하는 기계라고 생각할 수 있다.  
보통 벡터는 행렬을 곱하면 방향이 바뀌지만, 아주 특별한 방향에서는 방향이 바뀌지 않고 길이만 변한다.  
그 **방향이 고유벡터(eigenvector)**이고, **길이 변화 비율이 고유값(eigenvalue)**이다.

- 고유벡터 v: 행렬 변환 후에도 방향이 변하지 않는 벡터  
- 고유값 λ: 그 벡터가 늘어나거나 줄어드는 비율 (스칼라)

고유값 문제의 정의는 다음 한 줄이다:

```
A v = λ v
```

여기서 A는 행렬, v는 고유벡터(벡터), λ는 고유값(스칼라)이다.  
이 식은 “행렬이 벡터를 λ배만큼 늘이거나 줄이되 방향은 바꾸지 않는다”는 것을 뜻한다.

---

## 5-5. 왜 det(A - λI) = 0을 푸는가

고유값·고유벡터를 구하기 위해 다음 과정을 따른다:

1. 정의에서 시작: A v = λ v  
2. (A - λI)v = 0 으로 정리  
3. 영벡터(0벡터) 외의 해가 존재하려면 det(A - λI) = 0 이어야 함  
4. 이 조건을 풀면 λ(고유값)를 얻는다  
5. 얻은 λ를 다시 넣고 (A - λI)v = 0을 풀면 v(고유벡터)를 얻는다

즉, det(A - λI) = 0을 푸는 이유는 “0이 아닌 해가 존재하는 조건”을 찾기 위해서이다.  
그 조건이 바로 고유값 조건이며, 이때의 v가 고유벡터다.

---

## 5-6. 실제 계산 예시

다음과 같은 행렬 A를 생각하자:

```
A =
[ 2   1 ]
[ 1   2 ]
```

1) 고유값 구하기:

```
det(A - λI) = det([2-λ   1; 1   2-λ]) 
            = (2 - λ)² - 1 = λ² - 4λ + 3 = 0
```

따라서 λ₁ = 3, λ₂ = 1

2) 고유벡터 구하기:

- λ₁ = 3일 때: (A - 3I)v = 0 → [-1 1; 1 -1]v = 0 → v₁ = v₂ → 고유벡터 ∝ [1, 1]
- λ₂ = 1일 때: (A - I)v = 0 → [1 1; 1 1]v = 0 → v₁ = -v₂ → 고유벡터 ∝ [1, -1]

즉,  
- λ₁ = 3: [1, 1] 방향 → 가장 큰 변동  
- λ₂ = 1: [1, -1] 방향 → 상대적으로 작은 변동

---

## 5-7. 고유값과 정보량

고유값이 크다는 건 해당 축 방향으로 데이터가 많이 퍼져 있다는 뜻이다.  
고유값이 작다는 건 데이터가 거의 없다는 뜻이며, 그 방향에서 계수를 추정하면 불안정하다.

- 큰 고유값 → 정보가 풍부 → 추정 안정  
- 작은 고유값 → 정보가 부족 → 추정 불안정

고유값이 0이라면? → 해당 방향에는 정보가 전혀 없어 계수를 추정할 수 없다. (역행렬 존재 불가)

---

## 5-8. 조건수: 기울어진 정도를 수치화

조건수(condition number)는 (XᵀX)의 “공간이 얼마나 찌그러졌는가”를 보여주는 수치다.

```
조건수 = (가장 큰 고유값) / (가장 작은 고유값)
```

- 조건수가 작다 → 모든 방향에서 정보가 충분 → 추정 안정  
- 조건수가 크다 → 일부 방향에서 정보 부족 → 추정 민감

예:
- λ₁=3, λ₂=1 → 조건수 = 3 (양호)  
- λ₁=70, λ₂=0.03 → 조건수 ≈ 2345 (심각한 공선성)

---

## 5-9. PCA, 릿지, 라쏘로의 연결

고유값·고유벡터의 개념은 회귀 고급 기법의 기초다.

- PCA: 고유값이 큰 축만 남겨 차원을 줄임  
- 릿지 회귀: 고유값이 작은 축에 규제를 걸어 불안정을 줄임  
- 라쏘 회귀: 거의 정보 없는 축을 아예 제거하여 변수 선택

---

-핵심 문장

> (XᵀX)는 데이터 공간의 기하학이며, 고유벡터는 변하지 않는 본질적인 축이다.  
> 고유값은 각 축의 정보량을 나타내며, 작을수록 불안정성이 커진다.  
> det(A - λI) = 0을 푸는 이유는 “0이 아닌 해”가 존재하는 조건을 찾기 위함이며,  
> 그 결과가 회귀모형의 안정성, 공선성, 조건수, PCA, 정규화까지 이어진다.
