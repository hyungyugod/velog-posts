# 📌 1. F1 Score
## 1-1. 정의
- F1 Score는 분류 문제에서 정밀도(Precision)와 재현율(Recall)을 동시에 고려하기 위한 성능 지표이다.
- 두 지표 중 하나라도 낮으면 F1 점수도 낮아지도록 설계된 지표이다.
- 특히 클래스 불균형이 있는 데이터에서 유용하다.

## 1-2. 식
- 기본식:  
  $$F_1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$
- 여기서  
  - 정밀도(Precision) = $$\frac{TP}{TP + FP}$$  
  - 재현율(Recall) = $$\frac{TP}{TP + FN}$$  
- $TP$: True Positive, $FP$: False Positive, $FN$: False Negative

## 1-3. 해석
- Precision과 Recall이 모두 높아야 F1이 높아진다.
- 둘 중 하나라도 0에 가까우면 분모가 커져서 F1이 작아진다.
- 모델이 “잘 맞추는가”뿐 아니라 “놓치지 않는가”도 함께 보겠다는 의도이다.

## 1-4. 확장형 $F_\beta$
- 일반형은 다음과 같다.  
  $$F_\beta = (1 + \beta^2) \times \frac{Precision \times Recall}{\beta^2 \times Precision + Recall}$$
- $\beta > 1$이면 Recall을 더 중요하게, $\beta < 1$이면 Precision을 더 중요하게 본다.
- F1은 $\beta = 1$인 특수한 경우이다.

# 📌 2. 조화평균(Harmonic Mean)
## 2-1. 일반식
- 조화평균은 여러 값 중 작은 값의 영향을 크게 반영하는 평균이다.
- $n$개의 값 $x_1, x_2, \dots, x_n$에 대한 조화평균 $H$는  
  $$H = \frac{n}{\sum_{i=1}^{n} \frac{1}{x_i}}$$

## 2-2. 두 값일 때
- 값이 두 개 $x, y$일 때 조화평균은  
  $$H = \frac{2xy}{x + y}$$
- F1 Score는 이 조화평균을 Precision과 Recall에 적용한 것이라고 보면 된다.

## 2-3. 왜 조화평균을 쓰는가
- 산술평균은 한쪽이 커도 다른 한쪽이 매우 작으면 “괜찮아 보이는” 값이 나올 수 있다.
- 조화평균은 둘 중 작은 값에 끌려 내려가므로, 두 값의 “균형”을 본다.
- F1이 산술평균이 아닌 이유는 “정확하게도, 고르게도 잘 맞추는 모델”을 선호하기 때문이다.

# 📌 3. 시계열에서 약한 정상성 vs 강한 정상성
## 3-1. 강한 정상성(Strong Stationarity)
- 모든 시점의 **결합분포(joint distribution)**가 시간 이동에 대해 불변이다.
- 즉 임의의 시점 $t_1, t_2, ..., t_k$에 대해  
  $$(X_{t_1}, X_{t_2}, \dots, X_{t_k}) \overset{d}{=} (X_{t_1+h}, X_{t_2+h}, \dots, X_{t_k+h})$$
- 확률분포 전체가 변하지 않는 가장 엄격한 정상성이다.

## 3-2. 약한 정상성(Weak / Covariance Stationarity)
- 실무에서 보통 말하는 “정상”은 이 약한 정상성을 말한다.
- 조건은 3가지다.
  1. 평균이 일정하다: $$E[X_t] = \mu$$
  2. 분산이 일정하다: $$Var(X_t) = \sigma^2$$
  3. 자기공분산이 시점 $t$가 아니라 시차 $h$에만 의존한다:  
     $$Cov(X_t, X_{t+h}) = \gamma(h)$$
- 즉, “전체 모양”까지 같을 필요는 없고, 평균·분산·공분산 구조만 일정하면 된다.

## 3-3. 비정상성 판단 기준이 자기공분산이라는 말의 뜻
- 자기공분산이 시점 $t$에 따라 바뀌면, 그 시계열은 시간에 따라 의존구조가 변하는 것이므로 비정상으로 본다.
- 반대로 자기공분산이 오직 시차 $h$에만 의존한다면, 과거와 현재의 관계가 시간 전체에 대해 일정하므로 정상으로 본다.
- 따라서 “비정상성의 핵심은 공분산 구조가 시간에 따라 변하느냐”이다.

## 3-4. “특정 시기의 분산이 상대적으로 큰 것은 문제되지 않는다”의 의미
- 정상성은 “대체로” 평균과 분산과 자기공분산이 일정하냐를 본다.
- 한두 시점에서만 급변(이상치, 특이점)이 있는 것은 구조적 비정상으로 보지 않을 수 있다.
- 문제는 “계속해서” 분산이 커지거나 작아지는 추세가 있는 경우다.

# 📌 4. 자기공분산(Autocovariance)
## 4-1. 정의
- 시계열 $X_t$에서 서로 다른 시점 $t$와 $t+h$ 사이의 공분산이다.
- 식은  
  $$\gamma(h) = Cov(X_t, X_{t+h}) = E[(X_t - \mu)(X_{t+h} - \mu)]$$
- 여기서 $h$는 시차(lag)이다.

## 4-2. 의미
- 시차가 $h$만큼 떨어진 두 시점의 값이 얼마나 함께(같은 방향으로) 움직이는지 보는 척도이다.
- $\gamma(h)$가 크고 양수면 과거 값이 현재 값에 영향을 주는 구조이고, 0이면 시간적으로 독립적이다.
- 약한 정상성에서는 이 $\gamma(h)$가 시점 $t$가 아니라 “오로지 $h$만”의 함수여야 한다.

## 4-3. 자기상관과의 관계
- 자기상관계수는 자기공분산을 분산으로 나누어 표준화한 것:  
  $$\rho(h) = \frac{\gamma(h)}{\gamma(0)}$$
- $\gamma(0)$는 분산이다.
- ACF(자기상관함수)는 이 $\rho(h)$ 값을 시차별로 나열한 것이다.

# 📌 5. 분산 안정화 변환 (로그/지수 변환)
## 5-1. 문제 상황
- 시계열에서 시간이 지나면서 분산이 점점 달라지는 경우(이분산)가 많다.
- 예: 초기에는 값이 크게 흔들리다가 나중에는 작게 흔들리거나, 그 반대.
- 이런 경우 ARIMA나 회귀처럼 “오차의 분산이 일정하다”는 가정을 쓰는 모델이 잘 안 맞는다.

## 5-2. 로그 변환
- 일반식: $$Y' = \log(Y)$$
- 값이 클수록 변동폭도 같이 커지는 경우(분산이 증가하는 경우)에 사용.
- 곱셈적 변동을 덧셈적 변동으로 바꿔 분산을 안정화시키는 효과가 있다.
- 금융시계열, 지수성장형 데이터에서 자주 쓴다.

## 5-3. 지수 변환
- 일반식: $$Y' = e^{Y} \quad \text{혹은} \quad Y' = Y^k \; (k>1)$$
- 값이 작아질수록 변동폭도 너무 작아져버리는 경우(분산이 과도하게 줄어드는 경우)에 쓴다.
- 즉, 로그는 “너무 큰 변동을 눌러주는” 변환, 지수는 “너무 작아진 변동을 키워주는” 변환이다.

## 5-4. 목적
- 이들 변환의 목적은 결국 “시간에 따라 변하는 분산을 비교적 일정하게 만들어서” 정상성 가정에 가깝게 하기 위함이다.
- 변환 후 다시 정상성 검정(ADF, KPSS 등)을 하는 이유가 여기에 있다.

# 📌 6. 의사결정나무의 목적함수와 비용복잡도, 가지치기
## 6-1. 목적함수
- 의사결정나무는 “데이터를 가장 잘 분리하는 기준”을 찾는 것이 목적이다.
- 분류 문제에서는 노드의 불순도를 최소화하는 분할을 찾는다.
  - 지니지수: $$G = \sum_k p_k (1 - p_k)$$
  - 엔트로피: $$H = - \sum_k p_k \log_2 p_k$$
- 회귀 문제에서는 노드 내 제곱오차합(SSE) 또는 분산을 최소화하는 분할을 찾는다.

## 6-2. 비용복잡도 함수 (Cost-Complexity Function)
- 트리가 너무 깊어지면 과적합한다.
- 이를 막기 위해 오차 항과 복잡도(노드 수)에 대한 패널티를 합친다.
- 대표식:  
  $$C_\alpha(T) = R(T) + \alpha \times |T|$$
  - $R(T)$: 트리의 오차(불순도 합, MSE 합 등)
  - $|T|$: 리프 노드(단말노드) 수
  - $\alpha$: 복잡도에 대한 벌점 계수
- $\alpha$가 클수록 단순한 트리를 선호한다.

## 6-3. 가지치기(Pruning)와의 관계
- 보통 트리는 처음에 끝까지 다 자라게(fully grown) 만든다.
- 그 다음 비용복잡도 함수를 기준으로 불필요한 가지를 잘라낸다. 이게 사후 가지치기(post-pruning)이다.
- 즉, “가지치기”의 수학적 기준이 바로 이 비용복잡도 함수이다.
- 목적: 과적합을 줄이고 일반화 성능을 높이는 것.

# 📌 7. 서로 배반인 사건과 독립 사건
## 7-1. 배반과 독립의 차이
- 배반(mutually exclusive): 두 사건이 동시에 일어날 수 없다 → $$P(A \cap B) = 0$$
- 독립(independent): 한 사건의 발생이 다른 사건의 확률에 영향을 주지 않는다 → $$P(A \cap B) = P(A)P(B)$$

## 7-2. 왜 배반이면 항상 독립이 아닌가
- 배반이면 $P(A \cap B)=0$이다.
- 독립 조건에 대입하면  
  $$0 = P(A)P(B)$$
- 이 식이 성립하려면 $P(A)=0$이거나 $P(B)=0$이어야 한다.
- 즉, 두 사건이 모두 발생 가능한 경우($P(A)>0$, $P(B)>0$)에는 배반과 독립이 동시에 성립할 수 없다.
- 이유: A가 일어났다는 사실을 알게 되면 B는 일어나지 못하므로(확률이 0이 되므로) 서로에게 “영향”을 주게 되기 때문이다.

# 📌 8. 상관계수에 대한 통계적 검정의 올바른 구조
## 8-1. 올바른 귀무가설
- 두 수치형 변수 $X, Y$가 서로 상관이 없다는 것을 보이려면 귀무가설을  
  $$H_0: \rho = 0$$  
  로 세운다. 여기서 $\rho$는 모집단 상관계수이다.
- 대립가설은 보통  
  $$H_1: \rho \neq 0$$  
  으로 설정한다(양측검정).

## 8-2. “두 수치형 변수의 교차표에 대한 검정을 수행한다”가 틀린 이유
- 교차표(분할표)는 범주형 변수 간 관계를 볼 때 사용하는 것이다.
- 수치형 변수의 상관성 검정은 피어슨 상관계수를 쓰고, t분포로 검정한다.
- 따라서 “수치형 → 교차표”라는 연결은 변수 유형을 잘못 매칭한 것이다.

## 8-3. “입증하고 싶은 주장을 대립가설로 설정한다”가 불완전한 이유
- 통계적 가설검정은 “대립가설을 입증”하는 구조가 아니라,  
  “귀무가설을 기각할 만한 증거가 충분한지”를 보는 구조이다.
- 즉, 대립가설은 우리가 믿고 싶은 주장에 해당하긴 하지만, 그걸 ‘입증’하는 것이 아니라 ‘귀무가설을 버릴 수 있는가’를 보는 것이다.
- 그래서 “입증한다”는 표현은 부정확하다.

## 8-4. “유의수준을 계산하고 귀무가설 기각여부를 판단한다”가 틀린 이유
- 유의수준 $\alpha$는 계산하는 게 아니라 **사전에 설정**하는 값이다(보통 0.05).
- 계산하는 것은 p-value이다.
- 판단은 “p-value < α 인가?”로 한다.
- 따라서 올바른 순서는  
  1) 귀무·대립가설 설정  
  2) 유의수준 $\alpha$ 설정  
  3) 표본상관계수로부터 검정통계량(t값) 계산  
  4) p-value 계산  
  5) p-value와 $\alpha$ 비교 후 기각 여부 결정  
  이다.
