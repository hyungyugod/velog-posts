# 📌 1. 로지스틱 회귀와 다중 로지스틱 회귀

## 1-1. 로지스틱 회귀의 기본 개념

로지스틱 회귀(Logistic Regression)는 **연속형 입력 변수**를 이용해  
**이진 범주형 결과(0 또는 1)** 를 예측하는 통계 모델이다.  

즉, “사건이 발생할 확률($P(Y=1)$)”을 모델링하며,  
이 확률을 선형결합으로 직접 예측하지 않고 **로그오즈(log-odds)** 로 변환하여 예측한다.

$$
\log\frac{p}{1-p} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_kx_k
$$

- $p = P(Y=1|X)$  
- 좌변의 $\log\frac{p}{1-p}$ 는 **log-odds(로그 승산)** 이다.  
  즉, 사건이 일어날 확률과 일어나지 않을 확률의 비율을 로그로 바꾼 값이다.

---

## 1-2. 승산(odds)과 Exp(β)의 해석

- **승산(odds)**:  
  사건이 일어날 확률 $p$ 와 일어나지 않을 확률 $(1-p)$ 의 비율이다.  
  $$
  \text{odds} = \frac{p}{1-p}
  $$

- **회귀계수의 의미**:  
  $\beta_1$이 1 증가할 때 log-odds가 $\beta_1$만큼 증가하므로,  
  odds는 $e^{\beta_1}$배가 된다.  

  즉,  
  $$
  \text{odds(new)} = e^{\beta_1} \times \text{odds(old)}
  $$

- **해석**  
  - $e^{\beta_1} > 1$ → 사건 발생 odds 증가 (양의 효과)  
  - $e^{\beta_1} < 1$ → 사건 발생 odds 감소 (음의 효과)

예를 들어 $Exp(\beta_1)=0.45$라면,  
사건의 승산이 **0.45배로 줄어듦**, 즉 **55% 감소**한 것이다.  
(감소율은 $(1 - 0.45) \times 100\% = 55\%$)

---

## 1-3. 로지스틱 회귀의 학습 원리 (우도와 로그우도)

로지스틱 회귀는 **최대우도추정(MLE, Maximum Likelihood Estimation)** 으로 계수를 추정한다.

- 각 데이터가 실제 결과 $y_i$ (0 또는 1)를 가질 확률은  
  $$
  P(y_i|x_i, \beta) = p_i^{y_i} (1 - p_i)^{1 - y_i}
  $$
  - $y_i = 1$ → $p_i$ 
  - $y_i = 0$ → $1 - p_i$

| 실제값 $y_i$ | 해당 항의 의미         | 수식      | 직관                                     |
| :-------- | :--------------- | :------ | :------------------------------------- |
| $y_i=1$   | 모델이 ‘1’로 예측할 확률  | $p_i$   | “이 데이터는 실제로 1이었고, 모델도 1일 확률을 이렇게 예측함.” |
| $y_i=0$   | 모델이 ‘0’으로 예측할 확률 | $1-p_i$ | “이 데이터는 실제로 0이었고, 모델도 0일 확률을 이렇게 예측함.” |


- 전체 데이터의 우도함수:
  $$
  L(\beta) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
  $$

- 로그우도함수:
  $$
  \ell(\beta) = \log L(\beta) = \sum_{i=1}^{n} [y_i \log(p_i) + (1 - y_i)\log(1 - p_i)]
  $$

$\ell(\beta)$가 최대가 되는 $\beta$가 **데이터를 가장 잘 설명하는 계수**이다.

---

## 1-4. 로지스틱 회귀의 입력과 출력 형태

| 구분 | 입력(독립변수) | 출력(종속변수) | 예시 | 사용 모델 |
|------|----------------|----------------|-------|------------|
| 연속형 → 연속형 | O | O | 공부시간 → 점수 | 선형회귀 |
| 연속형 → 범주형 | O | X | 혈압 → 질병 유무 | 로지스틱회귀 |
| 범주형 → 연속형 | O(더미변환) | O | 성별, 전공 → 월급 | 선형회귀 |
| 범주형 → 범주형 | O(더미변환) | X | 성별, 전공 → 취업유무 | 로지스틱회귀 |

- **선형회귀:** 출력이 연속형 (값 예측)
- **로지스틱회귀:** 출력이 범주형 (분류 예측)

---

## 1-5. 다중 로지스틱 회귀(Multinomial Logistic Regression)

출력 변수가 3개 이상인 범주형일 때 사용된다.  
예를 들어, $Y = \{A, B, C\}$.

### (1) 기준 범주 설정

C를 기준(reference)으로 두고 각 범주의 log-odds를 다음처럼 설정한다.

$$
\begin{aligned}
\log\frac{P(Y=A)}{P(Y=C)} &= \beta_{0A} + \beta_{1A}x_1 + \cdots + \beta_{kA}x_k \\
\log\frac{P(Y=B)}{P(Y=C)} &= \beta_{0B} + \beta_{1B}x_1 + \cdots + \beta_{kB}x_k
\end{aligned}
$$

### (2) 지수 취하기

$$
\frac{P(Y=A)}{P(Y=C)} = e^{z_A}, \quad \frac{P(Y=B)}{P(Y(C))} = e^{z_B}
$$
$$
P(Y=A) = e^{z_A} P(Y=C), \quad P(Y=B) = e^{z_B} P(Y(C))
$$

### (3) 전체 확률의 합은 1

$$
P(Y=A) + P(Y=B) + P(Y=C) = 1
$$
$$
P(Y(C)) (1 + e^{z_A} + e^{z_B}) = 1
$$
$$
P(Y(C)) = \frac{1}{1 + e^{z_A} + e^{z_B}}
$$

### (4) 나머지 확률 계산

$$
P(Y=A) = \frac{e^{z_A}}{1 + e^{z_A} + e^{z_B}}, \quad
P(Y=B) = \frac{e^{z_B}}{1 + e^{z_A} + e^{z_B}}
$$

### (5) 일반화된 Softmax 함수

범주가 $K$개일 때:

$$
P(Y=j) = \frac{e^{z_j}}{\sum_{c=1}^{K} e^{z_c}}
$$

---

## 1-6. 핵심 요약

- $e^{z_j}$는 각 범주의 “강도(score)”를 지수로 변환한 것.  
- 모든 범주의 $e^{z_j}$를 더해서 1로 정규화하면 확률이 된다.  
- 확률의 합이 1이 되도록 설계된 게 **Softmax 함수**다.  
- 이진 로지스틱 회귀는 Softmax의 특수한 경우 ($K=2$) 이다.

---

<br>
<br>
<br>

# 📌 2. 로지스틱 회귀의 정보이론적 해석

## 2-1. 정보량의 기본 개념

정보이론에서 **어떤 사건의 정보량(Information Content)** 은 다음으로 정의된다.

$$
I(x) = -\log P(x)
$$

- 확률이 높을수록 정보량이 작다 (예상 가능한 사건).  
- 확률이 낮을수록 정보량이 크다 (예상 밖의 사건).

예시:
- “내일 태양이 뜬다” → $P=0.9999$, $I=-\log(0.9999)\approx0$  
- “내일 유성이 서울에 떨어진다” → $P=0.0001$, $I=-\log(0.0001)=4$

즉, **정보량은 사건의 놀라움 정도를 수치화**한 값이다.

---

## 2-2. 로지스틱 회귀에서의 확률과 정보량

로지스틱 회귀는 각 데이터에 대해 사건이 일어날 확률 $p_i$ 를 예측한다.  
이때 실제 데이터 $y_i$ 가 1(발생)일 수도, 0(비발생)일 수도 있다.

- 실제 $y_i = 1$ → 모델이 준 확률 $p_i$ 가 높을수록 정보량이 작음.  
- 실제 $y_i = 0$ → 모델이 준 확률 $(1 - p_i)$ 가 높을수록 정보량이 작음.

---

## 2-3. 로그우도와 교차엔트로피

모델의 로그우도(log-likelihood)는 다음과 같다.

$$
\ell(\beta) = \sum_{i=1}^{n} [y_i \log p_i + (1 - y_i)\log(1 - p_i)]
$$

이 값을 **최대화**하는 것은 모델이 데이터를 가장 잘 설명하는 확률을 찾는 것이다.  
여기에 음수를 붙이면 **교차엔트로피 손실(cross-entropy loss)** 이 된다.

$$
\text{Loss} = -\ell(\beta)
= -\sum_{i=1}^{n} [y_i\log p_i + (1 - y_i)\log(1 - p_i)]
$$

교차엔트로피는 모델이 실제 데이터를 얼마나 **비효율적으로** 설명하는지 측정하는 척도이다.  
즉, 손실이 작을수록 모델이 실제 세계를 더 효율적으로 설명한다.

---

## 2-4. “짧은 코드”와 “긴 코드”의 의미

정보이론에서는 확률이 높은 사건일수록 **짧은 코드**로,  
확률이 낮은 사건일수록 **긴 코드**로 표현할 수 있다.

| 예측 상황 | 실제 결과 | 정보량 ($- \log p$) | 코드 길이 비유 | 의미 |
|------------|-------------|----------------|----------------|------|
| $p=0.9$, 실제 발생 | 1 | 작음 (0.1 정도) | 짧은 코드 | 효율적으로 설명 |
| $p=0.1$, 실제 발생 | 1 | 큼 (2.3 정도) | 긴 코드 | 비효율적 설명 |

즉, **모델이 높은 확률로 맞춘다면 데이터 전체를 짧은 코드로 표현 가능**하다.  
이는 곧 모델이 데이터를 압축적으로, 효율적으로 설명한다는 의미이다.

---

## 2-5. 정보 거리(Information Distance)로의 해석

교차엔트로피는 실제 데이터 분포 $P$ 와 모델의 예측 분포 $Q$ 사이의 거리로 해석된다.

$$
H(P, Q) = H(P) + D_{\mathrm{KL}}(P\|Q)
$$

여기서 $D_{\mathrm{KL}}(P\|Q)$ 는 **Kullback–Leibler 발산(KL Divergence)** 이며,  
두 확률분포의 정보 거리(비효율성 차이)를 나타낸다.

로지스틱 회귀는 다음을 최소화하는 것이다.

$$
\min_\beta H(P, Q)
\quad \Longleftrightarrow \quad
\min_\beta D_{\mathrm{KL}}(P\|Q)
$$

즉, **모델이 실제 분포에 가장 가깝도록 정보 손실을 최소화하는 과정**이다.

---

## 2-6. 핵심 요약

> 확률이 높을수록 정보량은 작고,  
> 정보량이 작을수록 모델은 데이터를 효율적으로 설명하며,  
> 로지스틱 회귀는 바로 그 정보 손실(=교차엔트로피)을 최소화한다.

---

