# 📌 0. 빅데이터 분석기사 공부 및 대화
### 📌 1. 딥러닝 & 신경망
- **은닉층**: 입력과 출력 사이에서 특징 추출하는 층  
- **가중치(Weight)**: 연결의 세기 → 학습하면서 조정  
- **퍼셉트론**: 단순한 뉴런 모델, 계단 함수 출력  
- **딥러닝 ANN**: 여러 은닉층 + 다양한 활성화 함수(시그모이드, ReLU 등)  
- **오차역전파**: 출력 오차를 거꾸로 전파해 가중치 수정  
- **기울기 소실**: 미분값이 계속 곱해져 작아져서 학습이 안 되는 문제  

---

### 📌 2. CNN (합성곱 신경망)
- **합성곱 층**: 작은 필터로 지역 특징 추출  
- **풀링 층**: 크기 줄이고 중요한 특징만 남김 (예: max pooling)  
- **스트라이드**: 필터 이동 간격  
- **패딩**: 경계 부분 채우기  
- **채널**: 입력 데이터의 층 (예: RGB 3채널)  

---

### 📌 3. RNN & LSTM
- **RNN**: 시퀀스 데이터(순서 있는 데이터)에 적합, 가중치 공유, 순환 구조  
- **LSTM**: 장단기 기억, 망각/입력/출력 게이트로 긴 의존성 문제 해결  

---

### 📌 4. 오토인코더 & GAN
- **오토인코더**: 인코더(차원 축소) + 디코더(복원). 특징 추출/노이즈 제거  
- **GAN**: 생성자(가짜 생성) vs 판별자(진짜/가짜 판별) 경쟁  

---

### 📌 5. SVM (서포트 벡터 머신)
- **초평면**: 데이터를 나누는 경계선  
- **서포트 벡터**: 초평면에 가장 가까운 점들  
- **마진**: 초평면과 서포트 벡터 사이 거리 (최대화 목표)  
- **장점**: 적은 데이터로도 학습 가능  

---

### 📌 6. 연관성 분석
- **지지도(Support)**: A,B가 함께 발생할 확률  
- **신뢰도(Confidence)**: A가 발생했을 때 B도 발생할 확률  
- **향상도(Lift)**: A→B가 독립일 때보다 얼마나 더 잘 일어나는지 (1보다 크면 의미 있음)  
- **Apriori**: 빈발항목집합을 기반으로 규칙 생성  

---

### 📌 7. 거리 & 군집
- **맨해튼 거리**: |x₁-x₂| + |y₁-y₂| (직각 경로 합)  
- **덴드로그램**: 나무(diagram) 형태로 군집 시각화  
- **Ward 연결법**: 그룹 합치면서 분산 최소화  

---

### 📌 8. 회귀 & 분산분석
- **다중 회귀**: 독립변수 많아질수록 R² ↑ (설명력 ↑, 과적합 위험)  
- **Adjusted R²**: 변수 수 보정한 결정계수  
- **ANOVA**: 집단 평균 비교  
- **MANOVA**: 여러 종속변수를 동시에 비교  
- **판별분석**: 새 데이터의 집단 분류  

---

### 📌 9. 시계열 분석
- **정상성**: 평균, 분산, 공분산이 시간에 따라 일정  
- **차분(Differencing)**: 변화량으로 변환해 평균 일정하게 만듦  
- **분산 안정화 변환**: 로그, 제곱근 → 분산 일정화  
- **지수평활법**: 최근 데이터에 큰 가중치, 과거는 지수적으로 감소  
- **계절성 분리**: 주기적 패턴을 따로 떼어내 분석  
- **ARIMA**: AR + MA + 차분(d) (Integrated=누적 복원 의미)  

---

### 📌 10. 확률 & 베이즈
- **조건부확률**: P(A|B) = B일 때 A의 확률  
- **우도(likelihood)**: 같은 식 P(B|A)를 원인 A 평가용으로 해석  
- **베이즈 정리**: 사후확률 = (우도 × 사전확률) / 정규화  
- **나이브 베이즈**: 모든 특성 독립 가정(순진한 단순화)  
- **사전확률 균등 가정**: 정보 없을 때 클래스 동일 확률로 둠  
- **훈련 데이터 기반 추정**: 각 클래스 샘플 수 비율로 사전확률 수정  

---

### 📌 11. 기타
- **배반사건**: 동시에 발생 불가 (P(A∩B)=0)  
- **독립사건**: 서로 영향 없음 (P(A∩B)=P(A)P(B))  
- **TF-IDF**: 흔한 단어는 가중치↓, 특정 문서에만 자주 나오는 단어는 가중치↑  
- **워드 임베딩**: 단어를 실수 밀집벡터로 표현해 의미 공간에 배치  
- **홀드아웃 기법**: 일부 데이터를 보류(hold out)해 학습 안 쓰고 평가용으로만 사용  
- **DBN**: RBM을 여러 층 쌓은 신경망, 잠재변수 계층적 구조  
