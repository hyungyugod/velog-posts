# 📌 1. 시계열 데이터

## 1-1. 차분(Differencing)의 개념

* **정의:** 현재 시점의 값에서 이전 시점의 값을 빼는 것. 즉, $\text{차분} = X_t - X_{t-1}$.
* **목적:** 시계열의 추세(Trend)를 제거하여 데이터를 **정상화(Stationary)** 상태로 만드는 것.
* **의미:** '현재 값이 얼마나 변했는가'를 측정하는 **변화량(∆)**.

### 1차 차분

$$\nabla X_t = X_t - X_{t-1}$$

* 평균이 일정하지 않은 비정상 시계열의 **추세 제거** 목적.

### 2차 차분

$$\nabla^2 X_t = X_t - 2X_{t-1} + X_{t-2}$$

* **변화의 변화(가속도)** 를 측정하며, 곡선 형태의 추세를 제거.

> 즉, '추가적인 변화를 계산한다'는 말은 **변화량의 변화를 본다**는 뜻이다.

---

## 1-2. 비정상성 해결 방법 비교

| 방법            | 목적     | 수식/방법                           | 효과       |
| ------------- | ------ | ------------------------------- | -------- |
| **차분**        | 추세 제거  | $X_t - X_{t-1}$                 | 평균 안정화   |
| **로그변환**      | 분산 안정화 | $\log(X_t)$                     | 변동폭 완화   |
| **이동평균(통계적)** | 잡음 제거  | $\frac{X_t + X_{t-1} + ...}{n}$ | 데이터 부드럽게 |

* **차분:** 데이터의 평균을 일정하게 만든다.
* **로그변환:** 큰 값의 스케일 차이를 완화한다.
* **이동평균:** 단기 변동(노이즈)을 줄여서 추세를 보기 쉽게 한다.

---

## 1-3. ARIMA 모델의 개념

> **ARIMA (AutoRegressive Integrated Moving Average)** = 자기회귀 + 차분 + 이동평균

* **p:** AR(자기회귀) 차수 → 과거 값의 영향을 반영
* **d:** I(차분) 차수 → 추세 제거
* **q:** MA(이동평균) 차수 → 과거 예측 오차의 영향을 반영

### AR(p): 자기회귀(Autoregressive)

$$X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + ... + \phi_p X_{t-p} + \varepsilon_t$$

* 과거 값들이 현재에 미치는 영향을 모델링.
* $p$가 클수록 과거 데이터를 더 길게 참고.

### I(d): 차분(Integrated)

$\nabla^d X_t = X_t - X_{t-1}$ (1차) <br>
$\nabla^2 X_t = X_t - 2X_{t-1} + X_{t-2}$ (2차)

* 비정상 시계열을 정상 상태로 변환.

### MA(q): 이동평균(Moving Average)

$$X_t = \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... + \theta_q \varepsilon_{t-q}$$

* 과거 예측 오차의 패턴을 학습하여 현재 예측을 보정.

### ARIMA(p, d, q)의 결합식

$$\nabla^d X_t = \phi_1 \nabla^d X_{t-1} + ... + \phi_p \nabla^d X_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + ... + \theta_q \varepsilon_{t-q}$$

---

## 1-4. 이동평균(MA)의 두 가지 의미 구분

| 구분    | 일반 통계의 이동평균                            | ARIMA의 이동평균                                              |
| ----- | -------------------------------------- | -------------------------------------------------------- |
| 계산 대상 | 실제 값 $X_t$                             | 오차항 $\varepsilon_t$                                      |
| 목적    | 노이즈 제거 (스무딩)                           | 오차의 패턴을 반영하여 예측 개선                                       |
| 수식    | $MA_t = \frac{X_t + X_{t-1} + ...}{n}$ | $X_t = \varepsilon_t + \theta_1 \varepsilon_{t-1} + ...$ |
| 쓰임    | 시각화, 전처리                               | 통계적 모델링 내부                                               |

> **정리:** 일반 이동평균은 데이터를 보기 좋게 부드럽게 만들고, ARIMA의 이동평균은 오차를 보정하여 예측 정확도를 높인다.

---

## 1-5. 실제 데이터에서의 차분 활용

| 데이터 유형 | 원시 데이터 특징   | 차분 후 의미           |
| ------ | ----------- | ----------------- |
| 주가     | 꾸준히 증가      | **수익률** (오늘 - 어제) |
| 환율     | 장기 상승/하락 추세 | **변동성** (변화폭)     |
| GDP/물가 | 지속적 증가      | **성장률** (증가량)     |

> 즉, 차분은 데이터의 '수준(level)'이 아닌 '변화(change)'를 분석하게 한다.

---

## 1-6. SARIMA (Seasonal ARIMA)

> **SARIMA (Seasonal ARIMA)**는 계절성을 포함한 시계열 모델이다.
> 즉, ARIMA 모델에 '주기적(Seasonal) 패턴'을 반영한 확장형.

### SARIMA의 구조

SARIMA는 $ARIMA(p, d, q)(P, D, Q)_s$ 형태로 표현된다.

| 인자        | 의미     | 설명                                |
| --------- | ------ | --------------------------------- |
| $p, d, q$ | 비계절 성분 | 일반 ARIMA의 구성요소 (단기 패턴)            |
| $P, D, Q$ | 계절 성분  | 주기적(Seasonal) 패턴을 반영하는 ARIMA 구성요소 |
| $s$       | 계절 주기  | 예: 월별 데이터는 $s=12$, 분기별 데이터는 $s=4$ |

### SARIMA의 수식적 개념

SARIMA는 **비계절 + 계절 성분의 곱(convolution)** 으로 표현된다.

$$
\Phi_P(B^s) \phi_p(B) \nabla^d \nabla_s^D X_t = \Theta_Q(B^s) \theta_q(B) \varepsilon_t
$$

여기서,

* $B$는 시프트 연산자 ($BX_t = X_{t-1}$)
* $\Phi_P(B^s)$ : 계절 자기회귀(Seasonal AR)
* $\phi_p(B)$ : 비계절 자기회귀(Non-seasonal AR)
* $\nabla_s^D$ : 계절 차분 (주기별 차분)
* $\Theta_Q(B^s)$ : 계절 이동평균(Seasonal MA)
* $\theta_q(B)$ : 비계절 이동평균(Non-seasonal MA)

### 예시

* 월별 판매량이 12개월마다 반복되는 패턴이라면 $s = 12$.
* 만약 추세와 계절성이 함께 존재한다면, $(p, d, q)(P, D, Q)_{12}$ 형태의 SARIMA 모델을 사용한다.

**예:** $SARIMA(1,1,1)(0,1,1)_{12}$
→ 1차 차분으로 추세 제거, 12개월 주기의 계절 차분으로 반복 패턴 제거.

### SARIMA의 핵심 역할

1. **ARIMA로 단기 추세 예측**
2. **계절항으로 장기 주기 패턴 보정**

> 즉, SARIMA는 추세(Trend) + 계절성(Seasonality)를 동시에 처리하는 시계열 예측의 완성형 모델이다.

---

## 1-7. 한 줄 요약

> **ARIMA(p, d, q)**는 차분($d$)으로 추세를 제거한 시계열에서, 과거 값($p$)과 과거 오차($q$)를 결합해 미래를 예측한다.
> **SARIMA(p, d, q)(P, D, Q)_s**는 여기에 계절 주기($s$)와 계절적 패턴(P, D, Q)을 더해, 반복적인 변동까지 포착하는 고급형 모델이다.

<br>
<br>
<br>

# 📌 2. CNN과 활성화 함수

## 2-1. 풀링(Pooling)

### 개념
- **Pooling**은 CNN(합성곱 신경망)에서 **특징을 유지하면서 데이터 크기를 줄이는 연산**이다.  
- 입력의 지역 영역(window)을 요약(summarization)해, 중요한 정보만 남기고 불필요한 세부 정보를 제거한다.

### 목적
1. **차원 축소(Dimensionality Reduction)** → 연산량과 메모리 감소  
2. **변형 불변성(Translation Invariance)** → 입력의 작은 이동에도 특징 유지  
3. **과적합 방지(Regularization)** → 노이즈 제거 및 패턴 중심 학습

---

### 작동 방식
예: 2×2 윈도우, stride=2

입력:
1 3 2 4
5 6 7 8
3 2 1 0
1 2 3 4

markdown
코드 복사

#### (1) **최댓값 풀링 (Max Pooling)**
윈도우별 최대값 선택:
6 8
3 4

markdown
코드 복사
→ 가장 강한 특징만 남긴다.

#### (2) **평균 풀링 (Average Pooling)**
윈도우 평균값 계산:
3.75 5.25
2.00 2.00

yaml
코드 복사
→ 부드럽게 정보를 요약하며 노이즈를 완화한다.

#### (3) **글로벌 풀링 (Global Pooling)**
- 전체 특성맵에 대해 한 번만 풀링.
- **Global Average Pooling (GAP)**: 전체 평균 1개 값  
- **Global Max Pooling (GMP)**: 전체 중 최댓값 1개만
- CNN 마지막 단계에서 Flatten 대신 자주 사용 (예: ResNet, MobileNet)

---

### 수식 표현
$$
Y_{ij} = f_{pool}(X_{pq} \, | \, (p,q) \in R_{ij})
$$

- **Max Pooling:**  $\ f_{pool}(R) = \max(R)$ 
- **Average Pooling:**  $\ f_{pool}(R) = \frac{1}{|R|}\sum_{(p,q)\in R} X_{pq}$

---

### 한 줄 요약
> **Pooling**은 “특징을 유지하며 데이터 크기를 줄이는 연산”으로, Max는 강한 특징을, Average는 전체적 패턴을 남긴다.

---

## 2-2. 활성화 함수(Activation Function)

### ① 계단 함수 (Step Function)
$$
f(x) = 
\begin{cases}
0, & x < 0 \\\\
1, & x \ge 0
\end{cases}
$$

- 입력이 0보다 작으면 0, 크면 1  
- 출력이 단순히 **ON/OFF**처럼 동작 (이진 분류)  
- **미분 불가능 → 역전파 불가**  
- 초기 퍼셉트론(Perceptron)에서 사용

**문제점:**  
- 변화에 민감하지 않음 (0 근처에서 불연속)  
- 학습 불가능 → 현대 신경망에서는 거의 사용되지 않음

---

### ② ReLU 함수 (Rectified Linear Unit)
$$
f(x) = \max(0, x)
$$

- 입력이 0보다 작으면 0, 크면 그대로 통과  
- 계산이 단순, **기울기 소실(vanishing gradient)** 문제를 완화  
- **미분 가능 (x > 0 구간)** → 학습 가능  
- CNN, DNN, Transformer 등 대부분의 현대 신경망에서 표준으로 사용

---

### ③ Step vs ReLU 비교

| 구분 | Step Function | ReLU Function |
|------|----------------|---------------|
| 수식 | \( f(x)=0/1 \) | \( f(x)=\max(0,x) \) |
| 미분 가능 여부 | ✗ (0 근처 불연속) | ✓ (x>0 구간에서 가능) |
| 출력 범위 | {0, 1} | [0, ∞) |
| 학습 사용 | ✗ (퍼셉트론용) | ✓ (딥러닝 표준) |
| 특징 | 단순 ON/OFF | 연속적 활성화 |
| 대표 사용 | 고전적 퍼셉트론 | CNN, DNN, Transformer |

---

### 비유
| 함수 | 비유 | 설명 |
|------|------|------|
| Step | 스위치 | 꺼짐(0)/켜짐(1) |
| ReLU | 문턱 | 음수 차단, 양수는 그대로 통과 |

---

### 결론
> **ReLU는 계단함수의 미분 가능한 확장형**으로,  
> 단순한 이진 판단이 아닌, **입력 강도에 비례한 활성화**를 가능하게 만든다.

---

<br>
<br>
<br>

# 📌 3. 베이즈 정리의 의미와 활용 

## 3-1. 베이즈 정리의 의미

**베이즈 정리(Bayes’ Theorem)**란, *어떤 사건이 일어났다는 새로운 정보(조건)가 주어졌을 때, 그 사건이 실제로 일어날 확률을 다시 계산하는 방법*이다.

즉, **‘사후확률(Posterior Probability)’을 계산하는 공식**이다.

---

### 수식

$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$

- $P(A|B)$ : 사건 B가 일어난 뒤 A가 일어날 확률 (사후확률)
- $P(B|A)$ : A일 때 B가 일어날 확률 (우도, Likelihood)
- $P(A)$ : A의 사전확률 (Prior Probability)
- $P(B)$ : B의 전체 확률 (Evidence, 정규화 항)

---

## 3-2. 어떤 문제를 푸는가  

- “검사에서 양성이라면 실제로 질병일 확률은?”  
- “광고 클릭한 사람이 실제 구매자일 확률은?”  
- “스팸메일로 분류된 메일이 실제 스팸일 확률은?”

즉, **조건부 확률을 ‘뒤집는 문제’**를 해결한다.

---

## 3-3. 예시 문제 (질병 검사)

| 구분 | 질병 있음 (D) | 질병 없음 (~D) | 합계 |
|------|---------------|----------------|------|
| **양성 (T⁺)** | 45 | 105 | 150 |
| **음성 (T⁻)** | 5 | 845 | 850 |
| **합계** | 50 | 950 | 1000 |

- $P(D) = 0.05$  
- $P(T⁺) = 0.15$  
- $P(T⁺|D) = 0.9$  

$$
P(D|T⁺) = \frac{P(T⁺|D)P(D)}{P(T⁺)} = \frac{0.9 \times 0.05}{0.15} = 0.30
$$

→ 양성일 때 실제 환자일 확률은 **30%**.

---

## 3-4. 표의 의미와 해석

| 구분 | 전체 인원 중 해당되는 비율 |
|------|--------------------|
| $P(D)$ | 질병을 가진 사람 비율 |
| $P(T⁺)$ | 양성 판정 받은 전체 비율 |
| $P(T⁺|D)$ | 실제 질병자가 양성일 확률 |
| $P(D|T⁺)$ | 양성 중 실제 질병일 확률 (**우리가 구하는 값**) |

> 즉, 표는 전체 인원(모집단)을 기준으로, 각 조건의 비율을 나눈 것이다.  
> 베이즈 정리는 이 비율을 **‘전체 중 일부가 실제 어떤 범주에 속할 확률’**로 바꿔주는 공식이다.

---

<br>
<br>
<br>

# 📌 4. 결측치 대치법과 ESD 이상치 탐지 정리본

## 4-1. 조건부 평균 대치법 (Conditional Mean Imputation)

### 개념
결측값을 단순히 전체 평균으로 채우는 대신,  
**다른 변수의 조건에 따라 달라지는 평균(조건부 기대값)** 으로 대치하는 방법이다.

예:  
소득(income) 결측치를 나이(age)에 따라 예측해 채운다면  
→ “나이가 많을수록 소득이 높다”는 관계를 반영할 수 있음.

---

### 회귀분석을 활용한 조건부 대치법 (Regression Imputation)
1. **결측치가 없는 데이터**로 회귀모델을 학습한다.  
   $$ \text{income} = \beta_0 + \beta_1 \times \text{age} $$
2. 결측치가 있는 샘플에 독립변수(age 등)를 넣어 예측값 $\hat{y}$를 계산.
3. 결측치 위치에 그 예측값을 채운다.

즉, **다른 변수들의 조건(관계식)에 따라 평균을 예측하는 방식**이다.

| 방법 | 설명 | 장점 | 단점 |
|------|------|------|------|
| 단순 평균 대치 | 전체 평균으로 대체 | 빠름 | 분산 손실 |
| 조건부 평균 대치 | 그룹 또는 회귀예측값으로 대체 | 관계 보존 | 모델 필요 |

---

## 4-2. ESD (Extreme Studentized Deviate Test)

### 개념
**ESD 검정**은 데이터의 평균으로부터 너무 멀리 떨어진 “극단값(outlier)”을  
통계적으로 검정하는 이상치 탐지 방법이다.

---

### 이름의 의미
- **Extreme** → 극단적인 값  
- **Studentized Deviate** → 표준화된 편차 (Z-score와 유사)

즉, ESD는 각 데이터가 평균에서 얼마나 멀리 떨어졌는지를 표준편차 단위로 측정하여,  
그 값이 “통계적으로 유의하게 멀리 있는가?”를 판별한다.

$$
t_i = \frac{|x_i - \bar{x}|}{s}
$$

---

### 정규분포 가정의 의미
ESD는 데이터가 **정규분포(Normal Distribution)** 를 따른다고 가정한다.  
왜냐하면 정규분포에서는 데이터의 분포 범위(±1σ, ±2σ, ±3σ)가 확률적으로 정의되기 때문이다.

| 구간 | 포함 확률 |
|------|------------|
| 평균 ± 1σ | 약 68% |
| 평균 ± 2σ | 약 95% |
| 평균 ± 3σ | 약 99.7% |

따라서 평균 ±3σ 밖에 있는 값은  
“정규분포 상 거의 발생하지 않을 확률”이므로 이상치로 간주한다.

---

### 절차 요약
1. 각 데이터에 대해 $t_i$ 값을 계산  
2. 가장 큰 $t_i$ 값(가장 극단적인 점)을 찾음  
3. 유의수준(예: α=0.05) 기준과 비교하여 이상치 여부 결정  
4. 이상치로 판정되면 제거 후 다시 반복

---

### 요약 비교

| 구분 | 조건부 평균 대치법 | ESD 이상치 탐지 |
|------|--------------------|----------------|
| 목적 | 결측치 채움 | 이상치 검정 |
| 방법 | 회귀모델 등으로 평균 예측 | 표준화된 편차의 극단값 검정 |
| 가정 | 변수 간 관계 존재 | 정규분포 가정 |
| 산출 | 예측된 평균값으로 대치 | 이상치 여부(Yes/No) 판단 |

---

<br>
<br>
<br>

# 📌 5. 모델 선택 기준 AIC·BIC·Mallow’s Cp 

## 5-1. 공통 목표
- 세 지표 모두 **적합도**와 **복잡도**의 균형을 통해 과적합을 피하고 **일반화 성능**을 높인다.

## 5-2. AIC (Akaike Information Criterion)
- 정의: $AIC=-2\log L+2k$
- 해석: 값이 작을수록 좋다. 예측력 중심, 복잡도 패널티는 상수항 $2k$.
- 성향: 비교적 **관대**(복잡 모델을 더 허용).
- 로그 우도는 현재 적합된 모델의 계수를 넣고 계산된 우도이다. 즉 현재 모델이 데이터를 얼마나 잘 설명하고 있는가를 의미한다.
- 상수 k는 현재 해당 모델이 추정하고 있는 독립된 축 개수이다.

## 5-3. BIC (Bayesian Information Criterion)와 ‘베이지안 관점’
- 정의: $BIC=-2\log L+k\log n$
- ‘베이지안 관점’ 의미: 여러 후보 모델 $M_i$ 중에서 **사후확률** $P(M_i\mid \text{Data}) \propto P(\text{Data}\mid M_i)\,P(M_i)$ 가 최대인 모델을 고르는 근사.  
  라플라스 근사 등으로 주변우도의 로그가 $-2\log L+k\log n$ 형태로 떨어지며, 표본수 $n$이 커질수록 패널티 $k\log n$이 커져 **보수적**이 된다.
- 해석: 값이 작을수록 좋다. 단순성 선호가 AIC보다 강함.

## 5-4. Mallow’s Cp: 정의와 해석
- 기본식: $C_p=\dfrac{SSE_p}{\sigma^2}-(n-2p)$
  - $SSE_p$: 변수 $p$개(상수항 포함)를 쓴 부분모델의 잔차제곱합
  - $\sigma^2$: 오차분산(실무에서는 **전체모델**로 추정)  
    $\sigma^2 \approx \dfrac{SSE_{\text{full}}}{\,n-p_{\text{full}}\,}$
  - $n$: 표본수, $p$: 모수 개수(상수항 포함)
- 판단 규칙: 이상적이면 $C_p \approx p$  
  - $C_p>p$: 과소적합 경향(설명변수 부족, 오차 큼)  
  - $C_p<p$: 과적합 경향(불필요 변수 포함)

## 5-5. 왜 $p$와 $C_p$를 비교할 수 있는가
- $SSE_p/\sigma^2$는 “총 오차 에너지”를 “자유도당 평균 오차”로 **정규화**한 값이라 **자유도 스케일**을 갖는다.
- 결과적으로 $C_p$의 기대값이 $p$가 되도록 설계되어, $C_p$와 $p$는 **같은 축(자유도)**에서 직접 비교 가능하다.

## 5-6. 분산 추정과 자유도
- 회귀에서 오차분산의 불편추정: $\sigma^2=\dfrac{SSE}{\,n-p\,}$  
  단순히 $SSE/n$이 아니라 **추정한 모수 $p$만큼 자유도가 줄어든다**는 점이 핵심.

## 5-7. $C_p$의 직관적 구조
- $\sigma^2 \approx \dfrac{SSE_{\text{full}}}{\,n-p_{\text{full}}\,}$ 를 대입하면  
  $\dfrac{SSE_p}{\sigma^2} \approx (n-p_{\text{full}})\cdot \dfrac{SSE_p}{SSE_{\text{full}}}$  
  즉, 부분모델과 전체모델의 **오차 비율**을 자유도 스케일로 환산해 비교하는 구조다.
