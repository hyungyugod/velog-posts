# ğŸ“Œ 1. selenium ë™ì  í¬ë¡¤ë§ 
### 1-1. ì„¸íŒ…ê³¼ í…ŒìŠ¤íŠ¸
- ì•„ë‚˜ì½˜ë‹¤ ë‚´ì˜ ì£¼í”¼í„° í™˜ê²½ì—ì„œ ì§„í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.
- from selenium.webdriver.common.by import ByëŠ” Seleniumì—ì„œ ì—˜ë¦¬ë¨¼íŠ¸ë¥¼ ì°¾ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤ì´ë‹¤.
- ID, í´ë˜ìŠ¤ ì´ë¦„, íƒœê·¸ ì´ë¦„, CSS ì…€ë ‰í„° ë“±ì„ BY.ìœ¼ë¡œ ì§€ì •í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
```py
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By

import time

url = "http://www.naver.com"
driver = webdriver.Chrome()
driver.get(url)
```

### 1-2. ê¸°ë³¸ ëª…ë ¹
- current_urlëŠ” ê°€ì¥ ì•ì—ìˆëŠ” ë¸Œë¼ìš°ì €ì˜ ì£¼ì†Œ ì¦‰ ë¶€ëª¨ì˜ ì£¼ì†Œë¥¼ ë°˜í™˜í•œë‹¤.
- driver.back()ëŠ” í˜„ì¬ ë¸Œë¼ìš°ì €ì—ì„œ ë’¤ë¡œ ì´ë™í•œë‹¤.
- driver.forward()ëŠ” ì•ìœ¼ë¡œ ì´ë™í•œë‹¤.
- driver.refresh()ëŠ” ë¸Œë¼ìš°ì €ë¥¼ ìƒˆë¡œê³ ì¹¨í•œë‹¤.
- driver.close()ëŠ” í˜„ì¬ ë¸Œë¼ìš°ì €ë¥¼ ë‹«ëŠ”ë‹¤.
- driver.quit()ëŠ” ëª¨ë“  ë¸Œë¼ìš°ì €ë¥¼ ë‹«ëŠ”ë‹¤.
```py
driver.current_url
driver.back()
driver.forward()
driver.refresh()
driver.close()
driver.quit()
```

### 1-3. ë¸Œë¼ìš°ì €ì—ì„œ ê²€ìƒ‰í•´ë³´ê¸°
- ì•„ë˜ì™€ ê°™ì´ í• ë‹¹í•˜ë©´ searchëŠ” ì´ì œ WebElement ê°ì²´ê°€ ëœë‹¤.
- send_keys()ëŠ” í•´ë‹¹ ìš”ì†Œì— í‚¤ë³´ë“œ ì…ë ¥ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ë©”ì„œë“œì´ë‹¤.
- submit() ë©”ì„œë“œëŠ” í˜„ì¬ ìš”ì†Œë¥¼ í¼ ì œì¶œì„ íŠ¸ë¦¬ê±°í•˜ëŠ” ì—­í• ì„ í•œë‹¤.
```py
search = driver.find_element(By.ID, 'query')
search.send_keys('ë‚´ì¼ ë‚ ì”¨')
search.submit()
```

### 1-4. ë²„ê±°í‚¹ ë™ì  í¬ë¡¤ë§
```py
url = "https://www.burgerking.co.kr/#/home"
# driver = webdriver.Chrome('chromedriver.exe')
driver = webdriver.Chrome()
driver.get(url)
```
```py
# html ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')
bgk_addrs = soup.find_all("p", class_="txt_addr")

lst_bgk = [i.text.split(" ")[1] for i in bgk_addrs]
len(lst_bgk)

df_bgk = pd.DataFrame({"ë²„ê±°í‚¹ìœ„ì¹˜(êµ¬ì •ë³´)":lst_bgk}).reset_index() # ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ group by í–ˆì„ ë•Œ ê°œìˆ˜ë¥¼ ì…€ ì»¬ëŸ¼ì´ í•„ìš”í•˜ì—¬ì„œ indexë¥¼ ë‚´ë¦°ë‹¤.

pd.pivot_table(
    df_bgk,
    index = 'ë²„ê±°í‚¹ìœ„ì¹˜(êµ¬ì •ë³´)',
    values = 'index',
    aggfunc = 'count'
).sort_values(by='index')
```

### 1-5. ë§¥ë„ë‚ ë“œ ë™ì  í¬ë¡¤ë§
- ë¸Œë¼ìš°ì €ë¥¼ ì—´ê³  í˜ì´ì§€ ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ì„œ ì •ë³´ë¥¼ ê¸ì–´ì˜¨ë‹¤. ì´ë•Œ ê´‘í´ì´ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ì¤‘ê°„ì¤‘ê°„ì— ê¸°ë‹¤ë ¤ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
- element_to_be_clickable: element_to_be_clickableëŠ” "ìš”ì†Œê°€ í´ë¦­ ê°€ëŠ¥í•œ ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ”" ì¡°ê±´ì´ë‹¤. ìš”ì†Œê°€ DOM(ë¬¸ì„œ ê°ì²´ ëª¨ë¸)ì— ì¡´ì¬í•˜ëŠ”ì§€ì™€ ìš”ì†Œê°€ í™”ë©´ì—ì„œ í´ë¦­í•  ìˆ˜ ìˆëŠ” ìƒíƒœì¸ì§€ (ì¦‰, ìˆ¨ê²¨ì§€ì§€ ì•Šê³ , ë¹„í™œì„±í™”ë˜ì§€ ì•ŠìŒ) í™•ì¸í•œë‹¤.
- until() ë©”ì„œë“œëŠ” ì§€ì •í•œ ì¡°ê±´ì´ ì¶©ì¡±ë  ë•Œê¹Œì§€ ëŒ€ê¸°í•˜ê²Œ í•œë‹¤.
- WebDriverWait(driver, 10): driverê°ì²´ê°€ ìµœëŒ€ 10ì´ˆê¹Œì§€ ê¸°ë‹¤ë¦´ ìˆ˜ ìˆë„ë¡ í•œë‹¤.
```py
# fake ë¸Œë¼ìš°ì € ì‹¤í–‰
url = "https://www.mcdonalds.co.kr/kor/store/main"
driver = webdriver.Chrome()
driver.get(url)

page_no = 11
lst_mac = []

for page in range(1, page_no + 1):
    # í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚°
    if page % 5 == 0:
        p = 5
    else:
        p = page % 5  # í˜ì´ì§€ ë²ˆí˜¸ì— ë§ëŠ” ë²„íŠ¼ í´ë¦­ (1ë¶€í„° 5ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ)

    # í˜ì´ì§€ ë²ˆí˜¸ ë²„íŠ¼ í´ë¦­
    xpath = f"""//*[@id="container"]/section/div[4]/ul/li[{p}]/button"""
    WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.XPATH, xpath))
    ).click()
    
    # í˜ì´ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê¸°
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "mb-1"))
    )
    
    # í˜ì´ì§€ í™•ì¸ ë° HTML ì…ìˆ˜
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')
    
    # p íƒœê·¸ì—ì„œ ì£¼ì†Œ ì¶”ì¶œ
    mac_addrs = soup.find_all("p", class_="mb-1 text-15 text-gray-text")
    
    for i in mac_addrs:
        # ì£¼ì†Œ ì¶”ì¶œ ë°©ì‹ (split ì‚¬ìš© ì‹œ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ)
        text = i.text.strip()  # í…ìŠ¤íŠ¸ ì•ë’¤ ê³µë°± ì œê±°
        if len(text.split()) > 1:  # split()ìœ¼ë¡œ ì£¼ì†Œê°€ ì œëŒ€ë¡œ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸
            lst_mac.append(text.split()[1])  # ì£¼ì†Œ ì¶”ì¶œ
    
    # 5 í˜ì´ì§€ë§ˆë‹¤ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­
    if page % 5 == 0:
        next_xpath = """//*[@id="container"]/section/div[4]/ul/li[1]/button"""
        WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, next_xpath))
        ).click()
        time.sleep(1)

# ê²°ê³¼ ì¶œë ¥
print(lst_mac)

# ë“œë¼ì´ë²„ ì¢…ë£Œ
driver.quit()
```

# ğŸ“Œ 2. API ê¸°ë°˜ ìë£Œìˆ˜ì§‘
### 2-1. ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ api
- ê²€ìƒ‰ê³¼ ê²°ê³¼ì— ëŒ€í•œ ì •ë³´ë¥¼ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ apië¥¼ ì œê³µí•œë‹¤ëŠ”ê²ƒì´ í¥ë¯¸ë¡­ë‹¤.
- í•´ë‹¹ open api ì£¼ì†Œì— ì¿¼ë¦¬ìŠ¤íŠ¸ë§ìœ¼ë¡œ ê²€ìƒ‰í•  ê°’ì„ ì£¼ì†Œì— ë¶™ì´ê³  ë°œê¸‰ë°›ì€ í‚¤ê°’ì„ í—¤ë”ì— ì‹¤ì–´ì„œ í•´ë‹¹ ì£¼ì†Œë¡œ getìš”ì²­ì„ ë³´ë‚´ë©´ í•´ë‹¹ ê²€ìƒ‰ê²°ê³¼ì— ë§ëŠ” ì •ë³´ë“¤ì„ jsonìœ¼ë¡œ ë°˜í™˜í•´ì¤€ë‹¤.
```py
import requests    # URL ì •ë³´ë¥¼ í†µí•´ ì›¹ ì‚¬ì´íŠ¸ HTML ì…ìˆ˜
from bs4 import BeautifulSoup # HTML ì„ parsing êµ¬ì¡°í™”

import pandas as pd
import time

client_id = "ë°œê¸‰ë°›ì€ ì•„ì´ë””"
client_secret = "ë°œê¸‰ë°›ì€ ì‹œí¬ë¦¿í‚¤"

headers = {
    "X-Naver-Client-Id" : client_id,
    "X-Naver-Client-Secret" : client_secret
}

url = "https://openapi.naver.com/v1/search/blog?query="
keyword = "ê°•ë‚¨ì—­ ë§›ì§‘"
url_blog = url + keyword

req = requests.get(url_blog, headers=headers)
result = req.json()

result['items']
```
- ìœ„ì˜ ë¡œì§ì„ í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ ì‚¬ìš©í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.
```py
def nblog_search(keyword):
    client_id = "VHjMRxxw_mn9rIFMNKtb"
    client_secret = "BXIG_1Eev7"

    headers = {
        "X-Naver-Client-Id" : client_id,
        "X-Naver-Client-Secret" : client_secret
    }   

  url = "https://openapi.naver.com/v1/search/blog?query="
  url_blog = url + keyword
  req = requests.get(url_blog, headers=headers)
  return req.json()['items']
```