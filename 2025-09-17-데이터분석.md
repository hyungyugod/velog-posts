# 📌 0. 데이터 조작 (추가, 수정, 삭제)
### 📌 0-1. 데이터 추가
- data["몸무게"] → Series 반환
- Series * 1000 → 새 Series 반환
- (Series).fillna(...) → 결측값 채운 Series 반환
- 최종적으로 data["몸무게(g)"]에 할당
- * 1000 결과가 Series라는 걸 명확히 하기 위해 ( … )로 묶은 것이라고 생각하면 된다.
```py
data["몸무게(g)"] = (data["몸무게"] * 1000).fillna(data["몸무게"].mean() * 1000)
```
- 새로운 행을 추가할때는 sql의 insert처럼 자리수를 맞춰서 넣어주면 된다. 만약 해당 자리에 넣을게 없으면 비우는 것이 아니라 "" 으로 채워야한다.
```py
data.loc[9] = [9, "성현규", "2022-03-03", 75.0, "kg", "김현경", "서초구", 75000.0]
data.loc[10] = [9, "성현규", "", "", "kg", "김현경", "서초구", 75000.0]
```

### 📌 0-2. 데이터 삭제
- drop() 메서드는 호출한 객체와 같은 타입을 반환하게 되어있다.
- inplace=True이면 바로 원래 data의 값을 바꾸어 버린다.
```py
drop_cols = ["몸무게", "단위"]
data.drop(columns=drop_cols).head(3) # 열 삭제
data.drop(index=[9, 10]) # 행 삭제
data.drop(index=[9, 10], columns=drop_cols)
data.drop(index=[9, 10], columns=drop_cols, inplace=True) -> 이때는 원래 값을 바꾸는 것이므로 반환값이 none이다.
```

### 📌 0-3. 데이터 수정
- 데이터 수정은 series를 불러와서 바꿔치거나 rename
- 열 이름이나 인덱스명의 수정은 rename을 통해 수정한다. 
- 열 이름 변경 -> 딕셔너리를 활용하여 변경한다.
- 행이름은 index를 변경한다. 
```py
# 해당 행, 열의 값을 변경
data.loc[3, "몸무게"] = 77.5

# 열 이름 변경 -> 딕셔너리를 활용하여 변경한다.
# 행이름은 index를 변경한다. 
dict_col = {"담당":"담당자"}
data.rename(dict_col, index = {0:"zero"})
```
- 특정 컬럼의 값을 수정하고 싶다면 ```replace```를 사용하면 된다.
```py
data["지점"] = data["지점"].replace("강남구", "강남")
```

# 📌 1. 데이터 전처리
### 📌 1-1. unique, value_counts
- unique(): distinct가 적용된 배열(numpy.ndarray)을 리턴한다.
- nunique(): distinct가 적용된 배열의 원소 개수를 리턴한다.
- value_counts(): 원래 series는 인덱스는 인덱스 번호고 values는 값으로 들어가는데 value_counts()를 적용하면 index가 값(범주형 변수)이고 values가 해당 범주가 나온 횟수인 series를 반환한다.
- normalize=False → 절대 빈도 (횟수)
- normalize=True → 상대 빈도 (비율)
- 각 값의 절대 빈도를 세고, 그것들을전체 개수로 나눠서 0~1 사이의 값(상대빈도) 으로 바꾸는 과정을 **정규화(normalization)** 라고 한다.
- 이때 기준은 기준(norm)이라고 한다.
```py
data["지점"].unique()
data["지점"].nunique()
data["지점"].value_counts()
data["지점"].value_counts(normalize=True)
```

### 📌 1-2. 결측치 처리와 중복값 제거 
- fillna 외의 처리법에 대해서 다룬다.
- shift는 보통 시계열 데이터를 처리할때 데이터를 한칸씩 밀거나 당기는데에 사용한다.
```py
# .shift(+1)으로 이전 날짜의 몸무게를 가져온다.
# 혹시나 이전 기록이 없다면(=첫 번째 데이터) NaN을 가져온다.
data['몸무게(fill)(prev)'] = data['몸무게(fill)'].shift(+1)

# .shift(-1)으로 다음 날짜의 몸무게를 가져온다.
# 혹시나 다음 기록이 없다면(=마지막 데이터) NaN을 가져온다.
data['몸무게(fill)(next)'] = data['몸무게(fill)'].shift(-1)
```
- dropna(): 결측치가 있는 행 전부 삭제
- axis=0은 열삭제, axis=1은 행삭제를 의미한다.
```py
data.dropna(axis=0)
data.dropna(axis=1)
```
- data.drop_duplicates(): 열 전체값이 아예 같은 행들을 제거한다.
- subset=["담당", "지점"]을 주면 해당 열의 값이 중복되면 해당 행들을 삭제한다.
- 합쳐서 **“아래에 놓인 부분 집합(part that is set under)” → “전체 중에서 골라낸 일부 집합”**이라는 뜻이 된다.
- 그래서 subset은 **“전체 집합에서 뽑은 일부”**라는 의미로 쓰이고, 판다스에서는 **"열들 중에서 특정한 일부만 골라서 비교 기준으로 삼겠다"**는 뜻이 된다.
```py
# 중복제거 
data.drop_duplicates()
data.drop_duplicates(subset="담당")
data.drop_duplicates(subset=["담당", "지점"])
```

### 📌 1-3. dataframe결과 저장하기
- to_csv("이름", 인코딩)으로 저장한다. read_와 마찬가지로 뒤에 확장자를 넣으면 해당 파일로 저장이 가능하다.
```py
# dataframe 결과 저장하기
data.to_csv("작업결과-250917")

# 참고: 저장파일 한글 깨질 경우, 조치방안
data.to_csv("작업결과-250917", encoding="cp949") # cp949, utf-9-sig, euc-kr
```

### 📌 1-4. to_dateime()
- 해당 열의 값들을 datetime 데이터 타입으로 변경한다.
- datatime 데이터 타입으로 변경하면서 해당 객체에 속한 여러 함수들을 사용할 수 있다.
- dt로 연계해야하는 점을 유의해야한다.
```py
pd.to_datetime(data["측정일"]).dt.year
pd.to_datetime(data["측정일"]).dt.month
pd.to_datetime(data["측정일"]).dt.day
pd.to_datetime(data["측정일"]).dt.hour
pd.to_datetime(data["측정일"]).dt.minute
pd.to_datetime(data["측정일"]).dt.second
pd.to_datetime(data["측정일"]).dt.weekday
pd.to_datetime(data["측정일"]).dt.day_name()
pd.to_datetime(data["측정일"]).dt.quarter
pd.to_datetime(data["측정일"]).dt.isocalendar().week # 연주차: 1~52W
pd.to_datetime(data["측정일"]).dt.isocalendar().month # 연월: 1~12월
pd.to_datetime(data["측정일"]).dt.isocalendar().year # 연도: 1~12월
pd.to_datetime(data["측정일"]).dt.to_period("M") # 년월 : 2020-01
```

### 📌 1-5. pivot_table과 groupby
- pivot은 막대라는 뜻에서 중심축, 회전축을 의미한다.
- 이름처럼 데이터를 회전축(pivot)을 기준으로 “회전(pivoting)”시켜서 요약한다는 발상이다.
- 데이터의 중심축을 변환시킨다. 즉 아래 예시에서 지점을 인덱스로 하고 값은 몸무게의 값들로 채우고 aggfunc은 몸무게를 각 집계기준으로 모아서 새로운 테이블로 만든다. 
-  aggfunc을 아무것도 적지 않으면 기본 값 mean으로 나타낸다. 
```py
pd.pivot_table(data, index=["지점"], values=["몸무게"], aggfunc=["mean", "median", "count", "sum"])
pd.pivot_table(data, index=["지점"], values=["몸무게"])
```
- 아래 방식이 속도가 빠르고 groupby("지점") 이면 지점 열을 인덱스로 삼는다 이때 중복이 제거되면서 합쳐지는데 sql의 groupby처럼 특정 열과 집계함수를 사용하여 해당 값들만 보여줄 수 있다.
```py
data.groupby("지점")["몸무게"].mean()
data.groupby("지점")["몸무게"].median()
data.groupby("지점")["몸무게"].count()
data.groupby("지점")["몸무게"].sum()
data.groupby("지점")["몸무게"].agg(["mean", "median", "count", "sum"])
```

### 📌 1-6. apply 함수
- 특정 열의 값들을 모두 어떤 함수에 넣어서 series로 반환하고 싶을 때 이를 사용한다.
```py
import numpy as np
# band_weight라는 이름의 파이썬 함수를 만든다.
# 이 함수에서는 weight라는 값을 인자로 받는다.
def band_weight(weight):
    # weight 변수에 NaN이 들어왔으면 별도의 처리를 하지 않고 NaN으로 반환한다.
    if pd.isnull(weight):
        return np.nan

    # weight 변수에 있는 값이 75.5 kg 미만이면 '초과'라는 값을 반환한다.
    if weight < 75.5:
        return '초과'
    # weight 변수에 있는 값이 75.5 kg 이상이고 76.0 kg 이하면 '달성'이라는 값을 반환한다.
    elif weight >= 75.5 and weight < 76.0:
        return '달성'
    # 모든 조건해 해당하지 않으면 '미달'이라는 값을 반환한다.
    else:
        return '미달'

# Name 컬럼에 apply를 실행합니다. 인자로는 band_weight라는 함수를 집어넣는다.
# 이렇게 하면 전체 목표 데이터 값에 대해 band_weight를 실행한 뒤 그 결과를 반환한다.
# 반환한 결과를 목표라는 이름의 새로운 컬럼에 집어넣는다.

data["목표"] = data["몸무게"].apply(band_weight)
```
