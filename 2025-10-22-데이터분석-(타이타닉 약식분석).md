# 📌 1. kaggle 타이타닉 데이터 분석 (약식)
## 1-1. 결측치 처리
- info나 isna를 사용하여 결측치를 조사한다.
- 조사한 결과 
```py
train.isna().sum()
```
- .str Pandas에서 문자열 관련 함수(인덱싱, 분리, 대체 등)를 일괄 적용하는 접근자
- 결측치가 많기 때문에 결측치 자체가 의미있는 정보라고 판단 -> 일단 U로 처리함
- 실제 지도가 있으면 결측치를 그렇게 조사하여 채워넣을 수 있다.
- 또한 결측치를 처리할때 새로운 컬럼을 만들어서 채워두는게 좋다.
- 학습할때는 이 clean된것만 사용하면 된다.
```py
train["Cabin(clean)"] = train["Cabin"].str[0] 
train["Cabin(clean)"] = train["Cabin"].fillna("U") 
```
- age는 성별과 티켓 등급별로 나이 중앙값을 구해 결측치를 채워넣는다.
- train.groupby(["Sex", "Pclass"])["Age"] 이건 아직 평균을 구한 게 아니며. 단지 **Sex와 Pclass를 기준으로 묶은 다음, Age 컬럼만 선택한 “그룹 객체”**를 만든 것이다.
- 이후 train.groupby(["Sex", "Pclass"])["Age"].mean()처럼 처리할 수 있는 집계함수를 뒤에 붙여주어야한다.
- transform은 그룹 요약값을 각 행에 “방송(broadcast)” 해서 붙여준다.
- 예를 들어 ("female", 1) 그룹의 중앙값이 38이라면, 그 그룹에 속한 모든 행에 38이 들어있는 시리즈가 생성된다.
```py
train["Age(clean)"] = train["Age"].fillna(train.groupby(["Sex", "Pclass"])["Age"].transform("median"))
```
- 다음은 탑승한 항구데이터의 결측치를 채워야한다.
- 명목형 변수는 평균·중앙값 개념이 없고 S가 전체의 70% 이상이기 때문에 해당 결측치는 최빈값으로 채우는 것이 맞다.
```py
train["Embarked(clean)"] = train["Embarked"].fillna(train["Embarked"].mode())
```

## 1-2. 문자열 변수 인코딩
- 원핫인코딩으로 새로 생긴 열을 원래 데이터에 있는 문자 데이터로 바꿔치기하고 싶다면 train_encoded = pd.concat([train.drop(columns=obj_cols), encoded_df], axis=1)를 통해 바꾸면 된다.
```py
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False).set_output(transform="pandas")

encoded_df = ohe.fit_transform(train.select_dtypes("object"))

encoded_df.index = train.index # 인덱스를 처음에 이렇게 해놔서 이렇게 맞춰주었다.

train_with_ohe = pd.concat([train, encoded_df], axis=1)
```

## 1-3. 학습 데이터 생성, 분할
- 아래처럼 하거나 train.columns로 보고 일일히 지워줘도된다.
```py
from sklearn.model_selection import train_test_split

# 1) 타깃 분리
y = train_with_ohe["Survived"]

# 2) clean 숫자 변수 자동 선택 (이름에 '(clean)'이 붙고, 숫자형인 것만)
clean_cols = [c for c in train_with_ohe.columns if c.endswith("(clean)")]
clean_num_cols = train_with_ohe[clean_cols].select_dtypes("number").columns

# 3) 원핫 인코딩으로 새로 생긴 열만 선택
ohe_cols = encoded_df.columns  # fit_transform에서 나온 DF의 칼럼들

# 4) (선택) 추가로 쓸 원본 숫자열(예: Pclass, Fare, SibSp, Parch) 지정 -> 원본중에서 위에서 넣은 것 외에 원래 있던 칼럼 직접 추가
base_num_cols = ["Pclass","Fare","SibSp","Parch"]
base_num_cols = [c for c in base_num_cols if c in train_with_ohe.columns]

# 5) 최종 X 구성: clean 숫자 + 원핫 + (선택) 기본 숫자
X = train_with_ohe[list(clean_num_cols) + list(ohe_cols) + base_num_cols]

train_x, val_x, train_y, val_y = train_test_split(X, y, train_size=0.8, random_state=42)
```

## 1-4. 모델 학습 및 결과 비교
- 랜덤 포레스트와 의사결정 트리로 각각 계산해서 높은 학습기를 선택한다.
```py
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score

rf_clf = RandomForestClassifier(random_state=42, n_jobs=-1)
dt_clf = DecisionTreeClassifier(random_state=42)

rf_clf.fit(train_x, train_y)
dt_clf.fit(train_x, train_y)

pred_y = rf_clf.predict(val_x)
preddt_y = dt_clf.predict(val_x)

print(roc_auc_score(val_y, pred_y), roc_auc_score(val_y, preddt_y)) # 0.818018018018018 0.8164736164736165 -> 랜덤포레스트가 더 유의하다

```

## 1-5. 파이프라인 활용 테스트 일원화
- passthrough = “이 컬럼들은 변환 안 하고 그대로 모델로 넘겨줘.” 라는 뜻이다.
- 파이프라인을 활용하면 전처리 과정을 정리할 수 있지만 groupby 같은것을 처리하기는 빡세다 -> 할수는 있긴한데 따로 함수만들어서 해야하고 그렇다
```py
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# 1) 원본에서 쓸 열을 명시
cat_cols = ["Sex", "Embarked(clean)", "Cabin(clean)"]
num_cols = ["Pclass", "Age(clean)", "Fare", "SibSp", "Parch"]

X = train[cat_cols + num_cols]
y = train["Survived"]

# 2) 전처리 정의 -> imputer로 결측치도 처리가능
cat_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True))
])

num_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())  # 선택사항
])

preprocess = ColumnTransformer([
    ("cat", cat_pipe, cat_cols),
    ("num", num_pipe, num_cols)
])

# 3) 파이프라인 구성 (전처리 → 모델)
pipe = Pipeline([
    ("prep", preprocess),
    ("clf", RandomForestClassifier(random_state=42, n_jobs=-1)),
])

# 4) 학습/검증 분리 + 학습
X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
pipe.fit(X_tr, y_tr)

# 5) 평가 (AUC는 확률 사용)
val_proba = pipe.predict_proba(X_val)[:, 1]
print("AUC:", roc_auc_score(y_val, val_proba))
```

## 1-6. 파생변수 생성
- 번외로 파생변수를 한번 만들어봤다.
```py
# derived variables(feature) : sibsp + parch + 1
train["Family"] = train["SibSp"] + train["Parch"] + 1
```
- train.columns을 통해 
```py
Index(['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket',
       'Fare', 'Cabin', 'Embarked', 'Cabin(clean)', 'Age(clean)',
       'Embarked(clean)', 'Family'],
      dtype='object')
```
- 라는 결과를 받아서 이걸 그대로 복사하여 학습할 데이터의 컬럼을 구성해주었다.
- 자기 상관의 문제 때문에 연관된 컬럼을 삭제해보았다.
```py
train.columns
f_names = ['Survived', 'Pclass', 'Name', 'Sex', 'Ticket',
       'Fare', 'Cabin(clean)', 'Age(clean)',
       'Embarked(clean)', 'Family']

X = train[f_names].drop(columns="Survived")
y = train["Survived"]
```
- 이제 아래서 약식 원핫 인코딩을 했다. 여기까지 하고 나머지는 위처럼 다시 모델을 돌려보면 된다.
- drop_first=True 옵션은 하나의 열을 버려도 나머지로 예측이 가능하기 때문에 다중공선성 문제를 해결하고자 원핫인코딩되어 늘어난 열 중 한 열을 버려준다.
```py
X = pd.get_dummies(X, columns=["Sex", "Embarked(clean)", "Cabin(clean)"], drop_first=True)
```