# 📌 1. 7회 기출
## 1-1. 1 유형
- DE1~DE77컬럼 중 주가지수의 종가 "close"와 가장 상관관계가 높은 변수를 찾아, 해당 변수의 평균값을 구하시오. (반올림하여 소수 넷째자리까지 계산)
- loc로 슬라이싱 할땐 이름으로 슬라이싱 하는 것이므로 마지막 값이 포함된다.
-  df.corr()에서 특정 열을 선택하면 해당 열과 다른 변수들의 상관관계만 볼 수 있다. (df.corr()['close'].loc['DE1':'DE77'])
-  상관관계가 높다는 것은 음의 상관관계가 더 높을 수도 있으므로 절댓값으로 비교해야한다.
-  value_counts의 결과나 상관관계 데이터 프레임에서 idxmax함수는 특정 열의 이름이 바로 인덱스에 있어 조건에 해당하는 열을 바로 찾아내기에 좋다.
```py
# 1) close와의 상관관계(절대값)
df_corr = df.corr()['close'].abs() # abs 함수를 통해 절댓값으로 파악한다.

# 2) 상관관계가 높은 변수명
col = df_corr.loc['DE1':'DE77'].idxmax()

# 3) '2'에서 구한 변수명의 평균값
print(round(df[col].mean(), 4))
```
- 내가 풀때 sdf = pd.DataFrame(df[cond]['score']) 이런식으로 해서 series를 DataFrame으로 풀었는데 안이래도 그냥 df[cond][['score']] 이렇게 하면 된다.
- 두겹으로 감싸야 데이터프레임이 되는 헷갈리는 상황이었다. 
- 문제를 잘못읽어서 가장 큰값이 아닌 세번째 값을 구했는데 문제를 좀 더 집중해서 읽어야할 것 같다.
```py
# 0) 데이터 확인
print("결측치 수:", df.isnull().sum().sum())

# 1) 결측치 제거
print(df.shape)
df = df.dropna()
print(df.shape)

# 2) 가장 많이 수강한 과목 필터링
id = df['id_assessment'].value_counts().idxmax()
cond = df['id_assessment'] == id
df = df[cond]

# 3) 과목 점수 스탠다드 스케일
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df['score'] = scaler.fit_transform(df[['score']])

# 4) 가장 큰 값
print(round(df['score'].max(), 3))
```
- 만약 암컷의 우도를 구한다면 수컷 확률의 반대이므로 log를 씌웠을때는 -를 곱해주면 된다. 즉 아래와 같다.
```py
female_odds_ratio = round(np.exp(-model.params['weight']), 4)
print(female_odds_ratio)
```

## 1-2. 3유형
- 2유형은 풀이 후 기존과 내용이 같아서 정리본엔 옮기지 않았다. 회귀 문제였다.
- weight를 독립변수로 gender를 종속변수로 사용하여 로지스틱 회귀 모형을 만들고, weight 변수가 한 단위 증가할 때 수컷일 오즈비 값은? (반올림하여 소수 넷째자리까지 계산)
- train과 test를 분리해야했기 때문에 우선 iloc를 활용해서 train과 test를 주어진 비율로 분리했다. 
- 오즈비는 해당 지수의 해당 계수값승이므로 numpy를 활용해서 구해주었다.
```py
train = df.iloc[:210]
test = df.iloc[210:]

from statsmodels.formula.api import logit
formula = 'gender ~ weight'
model = logit(formula, data=train).fit()
# print(model.summary())

import numpy as np
print(round(np.exp(model.params['weight']),4)) # 1.0047
```
- 잔차이탈도(residual deviance) 란 피팅한 모델이 데이터를 설명하지 못한 정도로 작을 수록 좋다. 이는 범주형 예측에서 주로 사용된다.
- 우선 일반 로지스틱 회귀의 summary의 카테고리는 아래와 같다.
- pseudo R²는 설명변수가 없는 null모델에 비해 모델 설명력의 상대적 향상 정도
- 로그우도는 클수록 계수추정이 잘되었다는 뜻이며 음수여도 그냥 그대로 더 큰값이 좋은 값이다. 사실 음수인게 더 자연스럽다.
- 이때 llf는 log-likelihood of the fitted model이다.
- llnull이 설명변수가 없는 모델의 로그우도이다.
- GLM의 표준 정의가 Deviance = -2 × log-likelihood + 상수 (이때 상수는 최적의 모델을 의미) -> 제외하고 계산해도됨
- 단순히 LL에 음수를 붙였기 때문에 deviance가 오차가 되는 게 아니라,
GLM의 수학적 구조가 “우도 기반 오차 지표”를 -2 × LL 형태로 정의했기 때문이고,
이 형태가 카이제곱 검정과 모델 비교에 최적화되어 있어서 표준이 된 것이다.


| 항목                   | 의미(카테고리 설명)                                      |
| -------------------- | ------------------------------------------------ |
| **Dep. Variable**    | 종속변수(모델이 예측하려는 타겟 변수)                            |
| **No. Observations** | 사용된 전체 관측치 수(샘플 개수)                              |
| **Model**            | 사용된 모델 종류 (여기서는 로지스틱 회귀 Logit)                   |
| **Method**           | 모수 추정 방식 (보통 MLE = 최대우도추정)                       |
| **Date**             | 분석 실행 날짜                                         |
| **Time**             | 분석 실행 시간                                         |
| **Pseudo R-squ.**    | 로지스틱 회귀에서 사용되는 R² 유사 지표 (설명력 평가용)                |
| **Df Residuals**     | 잔차 자유도(샘플 수 − 모델 파라미터 수)                         |
| **Df Model**         | 모델에 포함된 설명변수의 자유도 (독립변수 개수)                      |
| **Log-Likelihood**   | 현재 모델의 로그우도(모델 적합도 기준)                           |
| **LL-Null**          | 설명변수 없이 예측한 Null 모델의 로그우도                        |
| **LLR p-value**      | 전체 모델이 유의한지 검정하는 p-value (Likelihood Ratio Test) |
| **Covariance Type**  | 추정된 공분산 행렬의 종류(robust 여부 등)                      |
| **converged**        | 최적화가 정상 수렴했는지 표시(True/False)                     |

- 두번째 문제는 이 잔차이탈도를 구하는 문제였다.
```py
from statsmodels.formula.api import logit
formula = 'gender ~ age + length + diameter + height + weight'
model = logit(formula, data=train).fit()
print(round(-2 * model.llf,2))
```
- 일반화 선형모형에서
- Gaussian → RSS 기반 deviance
- Poisson → 포아송 deviances
- Binomial → 로지스틱 deviance
- 이와같이 분포를 지정하고 모델을 만들면 이 잔차이탈도를 포함해준다.
- 이때 deviance가 잔차이탈도이다.
```py
from statsmodels.formula.api import glm
import statsmodels.api as sm

# 1) glm 모델 적합 (로지스틱 회귀를 위해 이항 분포 사용)
formula = "gender ~ age + length + diameter + height + weight"
model = glm(formula, data=train, family=sm.families.Binomial()).fit()

# 2) 잔차이탈도 계산
print(model.summary())
print(round(model.deviance,2))
```
- 독립변수 weight 만 사용해 학습한 로지스틱 회귀모델에서 test데이터의 gender를 예측하고, error rate(오류율)를 구하시오. (반올림하여 소수 셋째자리까지 계산)
- 이 문제에서 방심했던건 데이터셋을 분할할때 그냥 행 기준으로 잘랐기 때문에 target이 빠져있지 않았는데 이를 분리하지 못했던 점이다.
- 그리고 오류율은 1-정확도이기 때문에 1-accuracy로 계산해주면 된다.
- 그리고 accuracy_score 계산에도 pred가 필요한데 target에 1아니면 0으로 이진분류되어있으므로 확률값인 target도 >0.5조건을 활용해서 이진분류해주어야한다.
```py
# 데이터셋 분할
train = df.iloc[:210]
test = df.iloc[210:]

# 1) 학습, test데이터를 사용해 예측 (0.5 미만: 0, 0.5 이상 1)
model = logit("gender ~ weight", data=train).fit()
target = test.pop("gender")
pred = model.predict(test) > 0.5

# 2) 실제 값과 예측 값을 사용하여 정확도 계산
acc = accuracy_score(target, pred)

# 3) 오류율 계산
print(round(1-acc,3))
```
- 상관계수구하기는 풀이가 1유형과 같다.
```py
col = df.corr()['ERP'].loc['Feature1':'CPU'].abs().idxmax()
print(round(df.corr()['ERP'].loc[col],3)) # 0.434
```
- 결정계수와 p-value도 summary데이터만 읽을 수 있다면 쉽게 구할 수 있다.
- 아래는 최소제곱회귀의 summary와 코드이다.

| 항목                        | 의미(역할 설명)                                    |
| ------------------------- | -------------------------------------------- |
| **Dep. Variable**         | 종속변수(회귀가 설명하려는 타깃 변수)                        |
| **Model: OLS**            | 사용한 회귀 모델 종류 (Ordinary Least Squares, 최소제곱법) |
| **Method: Least Squares** | 회귀 계수(β)를 추정하는 방식이 최소제곱법임을 의미                |
| **No. Observations**      | 사용된 전체 관측치(샘플) 개수                            |
| **Df Model**              | 설명변수(독립변수) 개수 또는 자유도                         |
| **Df Residuals**          | 잔차 자유도 = (관측치 수 − 추정된 파라미터 개수)               |
| **R-squared**             | 모델이 종속변수의 분산을 얼마나 설명하는지 나타내는 결정계수            |
| **Adj. R-squared**        | 설명변수 수를 보정한 결정계수 (불필요한 변수 추가에 패널티 존재)        |
| **F-statistic**           | 전체 모델이 유의한지 검정하는 F통계량                        |
| **Prob (F-statistic)**    | F검정의 p-value. 모델 전체가 유의한지 판단                 |
| **Log-Likelihood**        | 모델이 관측 데이터를 얼마나 잘 설명하는지 나타내는 로그우도(적합도)       |
| **AIC**                   | 모델 선택 기준. 적합도 + 복잡도(변수 수)에 대한 패널티 포함         |
| **BIC**                   | AIC와 비슷하지만 변수 수에 더 강한 패널티 부여 (단순한 모델 선호)     |
| **Covariance Type**       | 추정된 계수의 공분산 행렬 종류(기본값은 nonrobust)            |

```py
cond = df['CPU'] < 100
df = df[cond]

from statsmodels.formula.api import ols
formula = 'ERP ~ Feature1	+ Feature2 + Feature3 + CPU'
model = ols(formula, data=df).fit()
print(model.summary())
print(round(model.rsquared,3)) # 0.226

print(round(model.pvalues['Feature2'],3)) # 0.457
```