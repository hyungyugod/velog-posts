# ğŸ“Œ 1. ë¹…ë¶„ê¸° 2025 ì‹¤ê¸° ì˜ˆì‹œ ë¬¸ì œ
## 1-1. 1ìœ í˜•
- ë‹µì€ ë§ì¶”ê¸´í–ˆëŠ”ë° ë” ì•Œì•„ë‘ë©´ ì¢‹ì„ ê²ƒë“¤ì€ ì•„ë˜ì™€ ê°™ë‹¤.
- í‰ê· ì„ ë³€ìˆ˜ mì— m = df['ê³ ê°ë§Œì¡±ë„'].mean() ì´ëŸ° ì‹ìœ¼ë¡œ ë‹´ì•„ê°€ëŠ” ê²ƒì´ ê¹”ë”í•˜ë‹¤. ê·¸ë¦¬ê³  í™•ì¸ë„ í•˜ê¸°
- df.groupby('ë¶€ì„œ')['ì—°ë´‰'].mean().nlargest(2)ë¥¼ ì‚¬ìš©í•˜ë©´ ê°€ì¥ í° 2ê°œì˜ ê°’ë§Œ ë½‘ì„ ìˆ˜ ìˆë‹¤.
```py
# ìˆ˜í–‰ 1)
df['ê³ ê°ë§Œì¡±ë„'] = df['ê³ ê°ë§Œì¡±ë„'].fillna(df['ê³ ê°ë§Œì¡±ë„'].mean())

# ìˆ˜í–‰ 2)
df = df.dropna(subset=['ê·¼ì†ì—°ìˆ˜'])

# ìˆ˜í–‰ 3)
print(df['ê³ ê°ë§Œì¡±ë„'].quantile(.75)) # 8

# ìˆ˜í–‰ 4)
print(df.groupby('ë¶€ì„œ')['ì—°ë´‰'].mean().sort_values(ascending=False)) # 74690
```

## 1-2. 2ìœ í˜•
- n_estimatorsëŠ” ë³µìˆ˜í˜•ìœ¼ë¡œ së¥¼ ë’¤ì— ê¼­ ë¶™ì—¬ì•¼ í•œë‹¤.
- metricsì— root_mean_squared_error í•¨ìˆ˜ê°€ ìˆë‹¤. ê·¸ë¦¬ê³  metricsì— aê°€ ì•„ë‹ˆë¼ eì¸ì ì„ ìœ ì˜í•˜ì
- ë§ˆì§€ë§‰ì— ì •ë‹µ í–‰ì˜ ê°œìˆ˜ ê¼­ í™•ì¸í•˜ê¸°
- ì¸ì½”ë”©í• ë•Œ ì¢…ë¥˜ê°€ ê°™ì€ì§€ ê¼­ í™•ì¸í•˜ê³  concatìœ¼ë¡œ í•©ì¹˜ê³  ì¸ì½”ë”©í•˜ê³  ë‚˜ëˆŒì§€ ê·¸ëƒ¥ ë‚˜ëˆŒì§€ ê²°ì •í•˜ê¸°
- edaì—ì„œ ì•„ë˜ë¥¼ ê¼­ ë¨¼ì € í™•ì¸í•˜ê³  í•´ì•¼í•œë‹¤. ì‹¤ì œë¡œ ì´ë¬¸ì œì—ì„œ í…ŒìŠ¤íŠ¸ì˜ ë²”ì£¼ê°€ í•˜ë‚˜ ì ì—ˆë‹¤.
```py
a = set(train['ì£¼êµ¬ë§¤ìƒí’ˆ'])
b = set(test['ì£¼êµ¬ë§¤ìƒí’ˆ'])
print('trainì—ë§Œ ìˆëŠ” ì¹¼ëŸ¼:', a-b)
print('testì—ë§Œ ìˆëŠ” ì¹¼ëŸ¼:', b-a)
```
- labelì¸ì½”ë”©ì€ ë¨¼ì € ì‚¬ì „ì„ ë§Œë“œëŠ” fitê³¼ì •ì´ ìˆê¸°ë•Œë¬¸ì— trainì´ testë¥¼ ë‹¤ í¬í•¨í•œë‹¤ë©´ ê·¸ëƒ¥ ë°ì´í„°ë¥¼ í•©ì¹  í•„ìš”ì—†ì´ ì§„í–‰í•´ë„ ëœë‹¤.
- ì›í•«ì¸ì½”ë”©ì€ í•œë²ˆ ë°ì´í„°ë¥¼ í•©ì³¤ë‹¤ê°€ ë¶„ë¦¬í•˜ëŠ” ë¡œì§ì´ í•„ìš”í•œë° ì´ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.
- ê·¸ë¦¬ê³  ì›¬ë§Œí•˜ë©´ ì›í•«ì¸ì½”ë”©ë„ ì§„í–‰í•´ë³´ê¸°
```py
df = concat([train, test])
df = pd.get_dummies(df)
train = df.iloc[:len(train)]
test = df.iloc[len(train):]
```
- ì•„ë˜ëŠ” ì „ì²´ ì½”ë“œì´ë‹¤.
```py
# 1) ì „ì²˜ë¦¬: ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ì¸ì½”ë”©
train['í™˜ë¶ˆê¸ˆì•¡'] = train['í™˜ë¶ˆê¸ˆì•¡'].fillna(0)
test['í™˜ë¶ˆê¸ˆì•¡'] = test['í™˜ë¶ˆê¸ˆì•¡'].fillna(0)

obj_cols = ['ì£¼êµ¬ë§¤ìƒí’ˆ','ì£¼êµ¬ë§¤ì§€ì ']

# baseline
# print(train.shape, test.shape)
# train = train.drop(obj_cols, axis=1)
# test = test.drop(obj_cols, axis=1)
# print(train.shape, test.shape)

# labelencoder
from sklearn.preprocessing import LabelEncoder
for col in obj_cols:
	le = LabelEncoder()
	train[col] = le.fit_transform(train[col])
	test[col] = le.transform(test[col])

# 2) ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
from sklearn.model_selection import train_test_split
x_tr, x_val, y_tr, y_val = train_test_split(train.drop('ì´êµ¬ë§¤ì•¡', axis=1), train['ì´êµ¬ë§¤ì•¡'], train_size=0.8, random_state=42)
# print(x_tr.shape, x_val.shape, y_tr.shape, y_val.shape)

# 3) ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
# lgbm
from lightgbm import LGBMRegressor
model = LGBMRegressor(verbose=-1, random_state=42, max_depth=6, n_estimators=300, learning_rate=0.02)

# randomforest
# from sklearn.ensemble import RandomForestRegressor
# model = RandomForestRegressor(random_state=42, max_depth=6, n_estimators=300)


from sklearn.metrics import root_mean_squared_error 
model.fit(x_tr, y_tr)
pred1 = model.predict(x_val)
print(root_mean_squared_error(y_val, pred1)) 
# lgbm max_depth=6, n_estimator=300, learning_rate=0.02  788.4995207798759
# rf random_state=42, max_depth=6, n_estimators=300 800.4953542753697

# 4) ê²°ê³¼ ì˜ˆì¸¡ ë° ì œì¶œ
pred = model.predict(test)
result = pd.DataFrame({
	'pred':pred
})

result.to_csv('result.csv', index=False)

# rs = pd.read_csv('result.csv')
# print(rs)
```

## 1-3. 3ìœ í˜•
- ìš°ì„  ë¡œê·¸ ë¦¬ì§€ìŠ¤í‹´ì´ë¼ê³  ëª…ì‹œí–ˆìœ¼ë‹ˆ ì¹¼ëŸ¼ ì „ì²´ì— ë¡œê·¸ ì—°ì‚°ì„ í•´ì¤˜ì•¼í•œë‹¤. ì´ëŠ” np.logë¡œ ë²¡í„°ë¡œê·¸ì—°ì‚°ì„ í•˜ë©´ëœë‹¤.
- df['Resistin'] = np.log(df['Resistin'])
- ê·¸ë¦¬ê³  F í†µê³„ëŸ‰ì˜ ê²€ì • í†µê³„ëŸ‰ì€ ë¶„ì‚°ë¹„ì´ë‹¤.
- ì´ë•Œ ë¶„ì‚°ìì²´ê°€ ì´ë¯¸ ììœ ë„ë¡œ ë‚˜ëˆ´ìœ¼ë¯€ë¡œ (var2 / df2) / (var1 / df1) ì´ë ‡ê²Œ ê¹Œì§€ ì•Ší•´ë„ ëœë‹¤. var2 / var1 ìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤.
- ë§Œì•½ ì œê³±í•©ë§Œ ìˆìœ¼ë©´ ììœ ë„ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ íƒ€ë‹¹í•˜ë‹¤.
- íŠ¹ì • ì—´ë§Œ ì„ íƒí•˜ëŠ”ê±´ ê·¸ëƒ¥ ì¡°ê±´ìœ¼ë¡œ ê±°ë¥¸ í›„ì— ì›í•˜ëŠ” ì—´ì„ ì„ íƒí•˜ë©´ ëœë‹¤.
- cond1 = df['Classification'] == 1
- var1 = df[cond1]['Resistin'].var()
- n1 = sum(cond1)ìœ¼ë¡œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì „ì²´ í–‰ê°œìˆ˜ë¥¼ ë°”ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤.
- equal_varsê°€ ì•„ë‹ˆê³  equal_varì´ë‹¤. ê°™ì€ ë¶„ì‚°ì€ ë‹¨ìˆ˜ì´ë‹¤.
- ttestì˜ ë°˜í™˜ê°’ì€ Ttest_indResult (namedtuple)ì´ë‹¤. íŠœí”Œ í˜•íƒœì´ë‹¤.
```py
#1
import numpy as np
df['Resistin'] = np.log(df['Resistin'])

cond1 = df['Classification'] == 1
cond2 = df['Classification'] == 2

var1 = df[cond1]['Resistin'].var()
var2 = df[cond2]['Resistin'].var()

result = var2 / var1
print(round(result,3))


#2
# ((ììœ ë„1 * ë¶„ì‚°1) + (ììœ ë„2 * ë¶„ì‚°2)) / ììœ ë„1+ììœ ë„2
n1 = sum(cond1)
n2 = sum(cond2)

result = (((n1-1) * var1) + ((n2-1) * var2)) / ((n1-1) + (n2-1))
print(round(result,3))


#3
from scipy import stats
result = stats.ttest_ind(df[cond1]['Resistin'], df[cond2]['Resistin'], equal_var = True)
print(round(result.pvalue,3))
```

## 1-4. 2023 ì˜ˆì‹œë¬¸ì œì—ì„œ ì±™ê²¨ê°ˆê²ƒë“¤
- ì´ ttestì¸ìì—ì„œ alternative íŒë‹¨ê¸°ì¤€ì€ ëŒ€ë¦½ê°€ì„¤ ê¸°ì¤€, aê¸°ì¤€ìœ¼ë¡œ ì‘ì•„ì•¼í•˜ëŠ”ì§€ ì»¤ì•¼í•˜ëŠ”ì§€ íŒë‹¨í•˜ë©´ ëœë‹¤.
- ê·¸ë¦¬ê³  ì• ì´ˆì— ë¦¬í„´ê°’ì´ íŠœí”Œì´ì–´ì„œ ë‘ ë³€ìˆ˜ì— ë¶„ë°°í•´ì„œ ë°›ì„ ìˆ˜ ìˆë‹¤.
- ì•„ë˜ì—ì„œ stëŠ” ê²€ì •í†µê³„ëŸ‰, pvëŠ” pvalueì´ë‹¤.
```py
result = ttest_rel(a, b, alternative='less')
st, pv = ttest_rel(a, b, alternative='less') 
```
- sklearn.preprocessingì—ì„œ encoderëŠ” í•˜ë‚˜ì˜ ë³€í™˜ëœ ê°’ë§Œ ë°˜í™˜í•˜ë¯€ë¡œ seriesí˜•íƒœë¡œ ì œê³µí•´ë„ ë˜ì§€ë§Œ scalerëŠ” ì»¬ëŸ¼ë³„ë¡œ ìŠ¤ì¼€ì¼ë§ì„ í•˜ë„ë¡ ë””ìì¸ë˜ì–´ìˆì–´ì„œ ê¼­ ë°ì´í„° í”„ë ˆì„ì„ ì œê³µí•´ì•¼í•œë‹¤.
```py
# MinMaxScaler í™œìš©
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
data['qsec'] = scaler.fit_transform(data[['qsec']])
```

## 1-5. ìœ ìš©í•œ íŒë‹¤ìŠ¤ ì„¤ì •, dir, __all__
- ì§€ìˆ˜í‘œê¸°ë²• -> ì¼ë°˜í‘œê¸°ë²•
```py
pd.set_option('display.float_format', '{:.10f}'.format)
```
- ì»¬ëŸ¼ ì „ë¶€ ì¶œë ¥í•˜ê¸°
```py
pd.set_option('display.max_columns',None)
```
- dirì€ í•´ë‹¹ íŒ¨í‚¤ì§€ì˜ ë§¤ì„œë“œë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
```py
import pandas as pd
print(dir(pd))
```
- sklearnì—ì„œ ëª¨ë“ˆ ëª©ë¡ì„ í™•ì¸í•˜ë ¤ë©´ __all__ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤.
- ì´í›„ ëª¨ë“ˆì˜ ë©”ì„œë“œë¥¼ í™•ì¸í•˜ë ¤ë©´ ë”°ë¡œ ì„í¬íŠ¸ í•œë‹¤ìŒì— ìµœìƒìœ„ ìˆ˜ì¤€ìœ¼ë¡œ ë§Œë“  í›„ dirì„ ì‚¬ìš©ê°€ëŠ¥í•˜ë‹¤.
```py
print(sklearn.__all__)

from sklearn import preprocessing
print(dir(preprocessing))
```

# ğŸ“Œ 2. ë¹…ë¶„ê¸° 2íšŒ ê¸°ì¶œ
## 2-1. 1ìœ í˜•
- ì£¼ì–´ì§„ ë°ì´í„°ì…‹(members.csv)ì—ì„œ 'views' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ê³ , 'views'ì»¬ëŸ¼ ìƒìœ„ 10ê°œ ë°ì´í„°ë¥¼ ìƒìœ„ 10ë²ˆì§¸ ê°’ìœ¼ë¡œ ëŒ€ì²´í•œ í›„ 'age'ì»¬ëŸ¼ì—ì„œ 80 ì´ìƒì¸ ë°ì´í„°ì˜ 'views' ì»¬ëŸ¼ í‰ê· ê°’ êµ¬í•˜ì‹œì˜¤
- ìœ„ì—ì„œë¶€í„° 10ë²ˆì§¸ ê°’ êµ¬í•˜ëŠ” ë°©ë²•ì´ min_value = df['views'].head(10).min()ì²˜ëŸ¼ minì„ í™œìš©í•´ë„ ë˜ê³  
- min_value = df['views'][:10].min()ì²˜ëŸ¼ ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ 10ì „ê¹Œì§€ ì˜ë¼ì„œ min()ìœ¼ë¡œ í•´ë„ëœë‹¤.
- ìŠ¬ë¼ì´ì‹±í•˜ë©´ í–‰ ì„ íƒ ë‚˜ë¨¸ì§€ëŠ” ë°”ë¡œ []ë©´ ì—´ì„ íƒ
- df.iloc[:10,-1] = min_value í•´ì„œ í–‰ì—´ì„ ì •í™•íˆ ì°ì–´ì£¼ë©´ í•œë²ˆì— ì „ë¶€ ëŒ€ì…ê°€ëŠ¥í•˜ë‹¤.
```py
s_df = df.sort_values('views',ascending=False)
print(s_df.iloc[9]['views']) # 9690.0

for i in range(10):
  s_df.iloc[i,9] = 9690.0 # ì™œ s_df.iloc[i]['views'] = 9690.0ì€ ì•ˆëì§€

print(s_df.head(10))

cond = s_df['age'] >= 80

print(s_df[cond]['views'].mean()) # 5674.04347826087
```
- ì£¼ì–´ì§„ ë°ì´í„°ì…‹(members.csv)ì˜ ì•ì—ì„œë¶€í„° ìˆœì„œëŒ€ë¡œ 80% ë°ì´í„°ë§Œ í™œìš©í•´ 'f1'ì»¬ëŸ¼ ê²°ì¸¡ì¹˜ë¥¼ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ìš°ê¸° ì „ í›„ì˜ í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•˜ê³ , ë‘ í‘œì¤€í¸ì°¨ ì°¨ì´ ê³„ì‚°í•˜ê¸° (ë‹¨, í‘œë³¸í‘œì¤€í¸ì°¨ ê¸°ì¤€, ë‘ í‘œì¤€í¸ì°¨ ì°¨ì´ëŠ” ì ˆëŒ€ê°’ìœ¼ë¡œ ê³„ì‚°)
- df80 = df.iloc[0:int(len(df) * 0.8), :]ì—ì„œ 0ì€ ìƒëµê°€ëŠ¥í•˜ë‹¤. ë’¤ì— , :ë„ ìƒëµê°€ëŠ¥í•˜ê¸´í•˜ë‹¤
- ê·¸ë¦¬ê³  ê·¸ëƒ¥ abs(std1-std2) ì´ë ‡ê²Œ í•´ë„ëœë‹¤. numpy ì•ˆì¨ë„ë¨!
- .iloc[i]['views']ëŠ” â€œì²´ì¸ ì¸ë±ì‹±(chain indexing)â€ì´ê¸° ë•Œë¬¸ì— ê°’ì´ ë°”ë€Œì§€ ì•ŠëŠ”ë‹¤. iloc[]ê¹Œì§€ì—ì„œ ê°’ì„ ë”± ì°ì–´ì•¼ [] ì—°ì‚°ìì˜ ìƒì„±ìê°€ ë°œë™ë˜ì–´ì„œ ê°’ì„ ëŒ€ì²´í•  ìˆ˜ ìˆë‹¤.
- **ë˜í•œ std(ddof=0)ìœ¼ë¡œ ì¸ìë¥¼ ë„£ìœ¼ë©´ í‘œë³¸ í‘œì¤€í¸ì°¨ê°€ ì•„ë‹Œ ëª¨ í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.**
```py
import numpy as np

df80 = df.iloc[0:int(len(df) * 0.8), :]

bf_std = df80['f1'].std()

f1_median = df80['f1'].median()

df80['f1'] = df80['f1'].fillna(f1_median)

af_std = df80['f1'].std()

print(np.abs(af_std - bf_std)) # 3.56406443000866
```
- ì£¼ì–´ì§„ ë°ì´í„°ì…‹(members.csv)ì˜ 'age'ì»¬ëŸ¼ì˜ ì´ìƒì¹˜ë¥¼ ëª¨ë‘ ë”í•˜ì‹œì˜¤! ë‹¨, í‰ê· ìœ¼ë¡œë¶€í„° 'í‘œì¤€í¸ì°¨*1.5'ë¥¼ ë²—ì–´ë‚˜ëŠ” ì˜ì—­ì„ ì´ìƒì¹˜ë¼ê³  íŒë‹¨í•¨
```py
m = df['age'].mean()
s = df['age'].std() 

cond = (m + 1.5 * s < df['age']) | (m - 1.5 * s > df['age']) 

print(df[cond]['age'].sum()) # 473.5
```

## 2-2. 2ìœ í˜•
```py
import pandas as pd
X_test = pd.read_csv("X_test.csv")
X_train = pd.read_csv("X_train.csv")
y_train = pd.read_csv("y_train.csv")

X_train.shape, y_train.shape, X_test.shape

X_train.info()

y_id = y_train.pop('ID')

# ì „ì²˜ë¦¬: ì¸ì½”ë”©
# print(
#     set(X_train['Warehouse_block']) - set(X_test['Warehouse_block']),
# set(X_train['Mode_of_Shipment']) - set(X_test['Mode_of_Shipment']),
# set(X_train['Product_importance']) - set(X_test['Product_importance']),
# set(X_train['Gender']) - set(X_test['Gender']) # set() set() set() set()
# )

# print(
#     set(X_test['Warehouse_block']) - set(X_train['Warehouse_block']),
# set(X_test['Mode_of_Shipment']) - set(X_train['Mode_of_Shipment']),
# set(X_test['Product_importance']) - set(X_train['Product_importance']) ,
# set(X_test['Gender']) - set(X_train['Gender'])  # set() set() set() set()
# )

obj_cols = ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender']

# baseline
# print(X_train.shape)
# X_train = X_train.drop(obj_cols, axis=1)
# X_test = X_test.drop(obj_cols, axis=1)
# print(X_train.shape)

# one-hot
X_train = pd.get_dummies(columns=obj_cols, data=X_train)
X_test = pd.get_dummies(columns=obj_cols, data=X_test)

# labelencoder
# from sklearn.preprocessing import LabelEncoder
# for col in obj_cols:
#   le = LabelEncoder()
#   X_train[col] = le.fit_transform(X_train[col])
#   X_test[col] = le.transform(X_test[col])
  

# ë°ì´í„° ë¶„í• 
from sklearn.model_selection import train_test_split
x_tr, x_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
# print(x_tr.shape, x_val.shape, y_tr.shape, y_val.shape)

# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
# from lightgbm import LGBMClassifier
# model = LGBMClassifier(verbose=-1, random_state=42, max_depth=5, n_estimators=300, learning_rate=0.05)


from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(random_state=42, max_depth=4, n_estimators=100)

model.fit(x_tr, y_tr)
pred1 = model.predict_proba(x_val)

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_val, pred1[:,1]))

# lgbm ì›í•« random_state=42, max_depth=5, n_estimators=300, learning_rate=0.1 0.7546772100699817
# ëœë¤ ì›í•« random_state=42, max_depth=4, n_estimators=300, learning_rate=0.1 0.754989386017775

# ì œì¶œ
pred = model.predict_proba(X_test)
submit = pd.DataFrame({
    'ID':X_test['ID'],
    'pred':pred[:,1]
})

submit.to_csv('submit.csv', index=False)

print(pd.read_csv('submit.csv'))
```