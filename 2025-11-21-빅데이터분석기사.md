# 📌 1. 카이제곱 검정
## 1-1. 적합도 검정
- 이는 실제 나타난 관찰된 빈도의 분포와 확률로서 기대되고 있는 빈도의 분포가 같은지 즉 적합한지 보는 검정이다. 
- 중요한건 기대도수는 확률에다가 전체 수를 곱해서 실제 값으로 만들어주어야한다.
- stats.chisquare의 스펠링을 유의해야한다.
- 이때 귀무가설(H0)이 동일함이다.
```py
#관찰
ob = [150, 120, 30]
#기대
ex = [0.5*300, 0.35*300, 0.15*300]

from scipy import stats 
print(stats.chisquare(ob, ex))
```

## 1-2. 독립성 검정
- python과 r이 빅데이터 분석기사 합격에 미치는 영향이 서로 독립적인가?
- 교차표를 chi2_contingency 내부에 넣어서 판단할 수 있다.
- con-: 함께(together) tingere(라틴어 어원): 닿다, 접촉하다
- contingency table(교차표)은 “두 변수가 상황에 따라 어떻게 함께 나타나는지 보여주는 표”이다.
- 교차표를 chi2_contingency에 넣어서 독립성을 판단한다.
- 이때 귀무가설이 독립이다.
```py
import pandas as pd
df = pd.DataFrame({
    '합격':[80, 90],
    '불합격':[20, 10]
    },index=['R', 'P']
)
df

from scipy import stats
stats.chi2_contingency(df)
```
- 만약 로우데이터로 주어진다면 각각 행벡터를 분리하여 교차표인 cross table 즉 cross tab으로 만들 것을 염두해두어야한다.
- 이후 stats.chi2_contingency(df)를 적용한다.
```py
df=pd.crosstab(df['언어'], df['합격여부'])
```

# 📌 2. 로지스틱 회귀분석 활용 타이타닉 문제
## 2-1. gender와 survived가 독립인지 종속인지 구하기 
- Gender와 Survived 열벡터를 교차표로 만들고 이를 기준으로 카이제곱 독립성 검사를 한다.
- 이때 검정통계량은 260.717이고 p값이 1.1973570627755645e-58로 매우 작으므로 이건 귀무가설을 기각 즉 독립이 아니다. 
- 당연히 성별은 생존여부에 영향을 준다.
```py
crosstab = pd.crosstab(df['Gender'], df['Survived'])
print(crosstab)

from scipy.stats import chi2_contingency
print(chi2_contingency(crosstab))
# 260.717
```

## 2-2. 로지스틱 회귀분석 시행
- Gender, SibSp, Parch, Fare 변수가 각각 survived에 어떤 영향을 주는지 로지스틱 회귀 분석을 시행하기
- from statsmodels.formula.api의 import logit을 활용한다.
- r에서 봤던 형태와 거의 식이 비슷하다.
- C()는 categorical의 C로 해당 변수가 범주형 변수임을 알려주고 모델이 이를 반영해서 계산을 해준다.
- fit 한 후 model의 summary함수를 통해 전체 결과를 한번에 확인할 수 있다.

```py
#2.
print(df[['Gender', 'SibSp', 'Parch', 'Fare']].head(3))

from statsmodels.formula.api import logit

model = logit("Survived ~ C(Gender) + SibSp + Parch + Fare", data=df).fit()
print(model.summary())
print(round(-0.2007,3))
# -0.201
```

## 2-3. 한 변수가 한단위 증가할때 오즈비
- 로그오즈와 선형결합식이 비례하는 관계가 있어 이를 토대로 로지스틱회귀를 분석한다.

$$
\log\frac{p}{1-p} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_kx_k
$$

- 즉 한 변수가 1 증가하면 그 계수만큼 로그 오즈가 증가하고 오즈는 e의 해당 계수승으로 증가한다.
- 이는 넘파이를 활용해 e의 지수승을 계산할 수 있다.
- 또한 model에서 params를 활용해여 특정 독립변수의 계수를 가져올 수 있다.
```py
import numpy as np
print(model.params['SibSp'])
print(np.exp(model.params['SibSp']))

print(np.exp(-0.3539))
```

# 📌 3. 선형 회귀분석
## 3-1. 상관관계
- 상관계수는 df.corr() 을 사용하여 구할 수 있으며 기본값은 method= 피어슨이다.
- 또한 열벡터끼리의 corr도 역시 구할 수 있다.
```py
print(df['키'].corr(df['몸무게']))
print(df['몸무게'].corr(df['키']))

# 피어슨
print(df.corr())
# 스피어맨
print(df.corr(method='spearman'))
# 켄달타우
print(df.corr(method='kendall'))
```
- scipy에 있는 값으로 t검정 역시 수행할 수 있는데 이때의 귀무가설은 상관이 없다 이다.
- pearson 뒤에 r이 하나 더 붙어 있는지 확인해봐야한다.
- 이때 t검정은 두 변수만 정확히 비교한다.
```py
# t검정
from scipy import stats
# 피어슨
print(stats.pearsonr(df['몸무게'], df['키']))

# 스피어맨
print(stats.spearmanr(df['몸무게'], df['키']))

# 켄달타우
print(stats.kendalltau(df['몸무게'], df['키']))
```

## 3-2. 단순, 다중 선형 회귀분석
- 보통 model에서 summary를 통해 결정계수, 회귀계수, 절편 등을 다 알 수 있다.
```py
from statsmodels.formula.api import ols
model = ols('키 ~ 몸무게', data=df).fit()
print(model.summary())

model.rsquared # 결정계수
model.params['몸무게'] # 회귀계수 
model.pvalues['몸무게'] # 몸무게의 회귀계수가 통계적으로 유의한지 pvalue
```
- 이때 학습한 모형으로 특정 데이터 값을 제공하면 종속변수를 예측하여 벡터를 반환받을 수 있는데 그 방법은 아래와 같다.
- 이때 formula 기반 모델(logit, ols, glm)을 썼다면 → DataFrame만 안정적으로 동작한다. 즉 series도 안정적이지 않아 dataframe을 넣어주어야 한다.
```py
newdata = pd.DataFrame({'몸무게':[50]})
model.predict(newdata)
```
- 잔차제곱합을 구하는 것은 아래와 같이 직접 df 열벡터를 빼주면 된다.
- 또 mse는 잔차 제곱에 mean함수를 적용해주기만 하면 된다.
- 물론 sklearn에서도 실제, 예측값을 넣어서 mse를 구할 수는 있다.
```py
# 잔차 제곱합
# 잔차 = 관측(실제)값 - 예측값
df['잔차'] = df['키'] - model.predict(df['몸무게'])
sum(df['잔차'] ** 2)

# MSE
(df['잔차'] ** 2).mean()

# 사이킷런 MSE
from sklearn.metrics import mean_squared_error
pred = model.predict(df)
mean_squared_error(df['키'], pred)
```
- 먄약 플랫폼이라는 범주형 변수가 섞여있어도 r처럼 수학 공식에 기반한다면 그냥 로지스틱 회귀처럼 c() 같은 표기도 필요없고 그냥 적어주면 원핫인코딩 후 drop_fist를 true로 설정하여 예측한후 결과를 제공한다.
```py
from statsmodels.formula.api import ols
model = ols('매출액 ~ 광고비 + 플랫폼', data=df).fit()
print(model.summary())
```
- 신뢰구간을 구하는 것은 get_prediction()을 사용하고 이때 get_prediction은 그 예측값에 대한 불확실성까지 계산하여 제공한다.
- 그리고 이 get_prediction의 결과를 표로 변환하려면 summary_frame이 필요하다.
- 이때 신뢰구간과 예측구간을 같이 제공하는데 신뢰구간(CI)은 회귀선 자체의 흔들림을 의미하고 예측구간은 새로운 점 하나의 흔들림을 의미한다. 하여 예측 구간이 신뢰구간보다 흔들림이 크다.
- 알파값이 제공하여 신뢰구간 추정에 대한 값을 받는다.
- 이때 OLS에서 get_prediction이 만드는 신뢰구간(CI)·예측구간(PI)은 t-분포로 추정한다.
```py
# 몸무게가 50일 때 예측키에 대한 신뢰구간, 예측구간
newdata = pd.DataFrame({'몸무게':[50]})
pred = model.get_prediction(newdata)
pred.summary_frame(alpha=0.05)
```

# 📌 4. 분산분석
## 4-1. 일원분산분석
- ttest_ind와 달리 3개 이상의 집단이 1개의 원인 아래 상관이 있는지 없는지 검사한다.
- 일원분산분석의 경우는 stats에도 있고 statsmodels에도 있다.
- 아래처럼 여러 열벡터를 넘겨주면 된다.
```py
from scipy import stats
stats.f_oneway(df['A'], df['B'], df['C'], df['D'])
```
- 이에도 정규성 검정은 shapiro로 등분산 검정은 levene으로 한다.
```py
# 정규성, 등분산, 일원 분산 분석

# Shapiro-Wilk(샤피로-윌크) 정규성 검정
print(stats.shapiro(df['A']))
print(stats.shapiro(df['B']))
print(stats.shapiro(df['C']))
print(stats.shapiro(df['D']))

# Levene(레빈) 등분산 검정
print(stats.levene(df['A'], df['B'], df['C'], df['D']))
```
- 만약 로우데이터가 바로 주어진다면 혹은 위처럼 a,b,c,d가 칼럼으로 있는 테이블을 melt하여 로우데이터로 준다면 statsmodels.fomula.api의 ols를 만들고 이 선형회귀 모형을 기초삼아 anova_lm 즉 아노바 테이블을 만들어 