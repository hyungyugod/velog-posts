# ğŸ“Œ 1. ëª¨ì˜ê³ ì‚¬ ì‘ì—…í˜• 1
## 1-1. 1ë²ˆ ë¬¸ì œ
- ì•ì—ì„œ ë¶€í„° 50%ê¹Œì§€ì˜ ë°ì´í„°ì—ì„œ 'target'ì»¬ëŸ¼ì´ 0 ê°’ì„ ê°€ì§„ ë°ì´í„°ë§Œ í™œìš©í•´ 'proline'ì»¬ëŸ¼ì˜ í‰ê· ì„ êµ¬í•˜ì‹œì˜¤ (ì†Œìˆ˜ì  ì ˆì‚¬(ë²„ë¦¼), ì •ìˆ˜í˜• ì¶œë ¥)
- ì •í™•íˆ ëª‡ í¼ì„¼íŠ¸ë¡œ ì˜ë¼ë‚¼ë•ŒëŠ” len(a)*50//100 ì´ëŸ°ì‹ìœ¼ë¡œ í•˜ë©´ ì •í™•í•˜ë‹¤.
- ilocê³¼ locì€ â€œí•¨ìˆ˜â€ê°€ ì•„ë‹ˆë¼ â€œì¸ë±ì„œ(indexer) ê°ì²´"ë‹¤ ì¦‰ í•¨ìˆ˜ì²˜ëŸ¼ ëŠë¼ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë©”ì†Œë“œ í˜¸ì¶œì´ ì•„ë‹ˆë¼ ì†ì„± ì ‘ê·¼ì´ë‹¤.
- ì¦‰ ì•„ë˜ì™€ ê°™ì´ ì†ì„±ìœ¼ë¡œ ì ‘ê·¼í•˜ë©´ í•´ë‹¹ìœ„ì¹˜ë¡œ ì—°ê²°í•´ì£¼ëŠ” ìƒì„±ìë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.
```py
a.iloc[0]        # __getitem__(0) í˜¸ì¶œ
a.iloc[0, 1]     # __getitem__((0,1)) í˜¸ì¶œ
a.loc['row']     # __getitem__('row') í˜¸ì¶œ
```
```py
a = a.iloc[0:int(len(a)*50//100),:] # 50% ì˜ë¼ë‚´ê¸°
cond = a['target'] == 0 # íƒ€ê²Ÿì´ 0ì¸ ì¡°ê±´
m = a[cond]['proline'].mean() # í•´ë‹¹ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” dataframeì—ì„œ 'proline' ì¹¼ëŸ¼ì˜ í‰ê· 
answer = int(m) # ì •ìˆ˜ë³€í™˜
print(answer)
```
- a[:ìˆ«ì]ëŠ” ì—´(column) ì ‘ê·¼ì´ ì•„ë‹ˆë¼ í–‰(row) ìŠ¬ë¼ì´ìŠ¤ë‹¤
- a['col'] ë¬¸ìì—´ì´ë©´ â†’ ì—´ ì´ë¦„ ì ‘ê·¼ (ì´ë•ŒëŠ” ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ ë™ì‘í•˜ëŠ” ê²ƒ.)
- a[ìˆ«ì] ìˆ«ìë¥¼ ì§ì ‘ ì“°ë©´ â†’ ì—´ ë²ˆí˜¸ ì ‘ê·¼ë„ í–‰ ë²ˆí˜¸ ì ‘ê·¼ë„ ì•„ë‹˜ â†’ ì—ëŸ¬
- í•˜ì§€ë§Œ ìŠ¬ë¼ì´ìŠ¤ a[:ìˆ«ì]ëŠ” ì˜ˆì™¸ì ìœ¼ë¡œ â€˜í–‰ ìŠ¬ë¼ì´ìŠ¤â€™ë¡œ ë™ì‘ 
- ì¦‰ ì²«ì¤„ì„ ì•„ë˜ì²˜ëŸ¼ ì¨ë„ ë¨
- ì´ë•Œ ì†Œìˆ«ì ì´ ë‚˜ì™”ì„ë•ŒëŠ” ê·¸ëƒ¥ ì ˆì‚­í•˜ë©´ ë”± ê·¸ë§Œí¼ì˜ í¼ì„¼íŠ¸ê°€ ëœë‹¤. ì¦‰ ê·¸ëƒ¥ intë¡œ ì ˆì‚­í•´ë„ ëœë‹¤.
```py
a = a[:int(len(a)/2)]
```

## 1-2. 2ë²ˆ ë¬¸ì œ
- s1, s2, s3, s4, s5, s6, ì»¬ëŸ¼ì˜ í–‰(row)ë³„ í•©ì„ êµ¬í•˜ê³  ê·¸ í•©ì´ 0.1ë³´ë‹¤ í° ê°’ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤
- print(sum(a.sum(axis=1)>0.1)) # ì—´ë°©í–¥í•©ì´ 0.1ë³´ë‹¤ í° ë°ì´í„°ë“¤ì˜ í•©
```py
# ì‚¬ìš©ì ì½”ë”©
cols = ['s1', 's2', 's3', 's4', 's5', 's6'] # í•´ë‹¹í•˜ëŠ” columnsë§Œ ì„ íƒ
a = a[cols] # í•´ë‹¹ ì»¬ëŸ¼ë“¤ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë®ì–´ì”€
print(sum(a.sum(axis=1)>0.1)) # ì—´ë°©í–¥í•©ì´ 0.1ë³´ë‹¤ í° ë°ì´í„°ë“¤ì˜ í•©
```

## 1-3. 3ë²ˆ ë¬¸ì œ
- ê³ ìœ í•œ ê°’ì´ ê°€ì¥ ë§ì€ ì»¬ëŸ¼ì˜ ì´ë¦„ì€?
- a.nunique() # a.nunique() ëŠ” DataFrameì˜ ê° ì—´(column)ë§ˆë‹¤ â€œê³ ìœ í•œ(unique) ê°’ì´ ëª‡ ê°œ ìˆëŠ”ì§€â€ ì„¸ì–´ì„œ Seriesë¡œ ë°˜í™˜
```py
a = a.nunique() # a.nunique() ëŠ” DataFrameì˜ ê° ì—´(column)ë§ˆë‹¤ â€œê³ ìœ í•œ(unique) ê°’ì´ ëª‡ ê°œ ìˆëŠ”ì§€â€ ì„¸ì–´ì„œ Seriesë¡œ ë°˜í™˜
print(a.sort_values(ascending=False).index[0]) # í˜„ì¬ ì¸ë±ìŠ¤ê°€ ì¹¼ëŸ¼ëª…ì´ë¯€ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ê°€ì¥ ìœ„ì— ìˆëŠ” ì¸ë±ìŠ¤ê°€ í•´ë‹¹í•˜ëŠ” ì¹¼ëŸ¼ì˜ ì´ë¦„ì´ ëœë‹¤.
```

# ğŸ“Œ 2. ì‘ì—…í˜• 2 ëª¨ì˜ë¬¸ì œ 1 (ì´ì§„ë¶„ë¥˜)
## 2-1. ë°ì´í„° ì „ì²˜ë¦¬ : ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ì¸ì½”ë”©, ìŠ¤ì¼€ì¼ë§
- ê²°ì¸¡ì¹˜ëŠ” isna().sum()ìœ¼ë¡œ ì´ìƒì¹˜ëŠ” describe()ë¡œ í™•ì¸
- í•´ë‹¹ ë¬¸ì œëŠ” ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ì„œ ìš°ì„  ì¸ì½”ë”©í•¨
- ì¸ì½”ë”©ì€ baseline(ì•„ì˜ˆ ë¬¸ìí˜•ì„ ì§€ì›Œë²„ë¦¼), onehotì¸ì½”ë”©, labelì¸ì½”ë”© ì‹œë„í•˜ê¸°
- one-hot encoding -> ê·¼ê±°ëŠ” obj_colsì˜ colums ì ìš©í•˜ê¸° ì „ì˜ nuniqueê°’ì´ ì‘ì€ì§€ í•œë²ˆ í™•ì¸ì€ í•´ë³´ê¸°
- ìš°ì„  ë¬¸ìí˜• ì»¬ëŸ¼ë“¤ë§Œ ë¶„ë¦¬ -> ë¬¸ìí˜•ë§Œ í•˜ë ¤ë©´ include ëº€ ë‚˜ë¨¸ì§€ í•˜ë ¤ë©´ excludeí•˜ë©´ëœë‹¤.
- columsë§Œ ë„£ì–´ì•¼ í•˜ëŠ”ê²ƒ ì£¼ì˜í•˜ì
- ë˜í•œ ì„¤ê³„ë³€ìˆ˜ëŠ” ê³¼ì í•©ì˜ ìš°ë ¤ê°€ ìˆì–´ì„œ ì¼ë‹¨ í•™ìŠµì—ì„  ì œì™¸í•˜ê³  ë‹µì§€ì— í¬í•¨í•´ì•¼í•˜ë©´ popì„ í†µí•´ test_id ë³€ìˆ˜ì— ë”°ë¡œ ì €ì¥í•´ë‘”ë‹¤.
```py
cols = train.select_dtypes(include='object').columns
cols

train = train.drop('CLIENTNUM', axis=1)
test_id = test.pop('CLIENTNUM')
```
- êµ¬ì¡°ëŠ” ì•„ë˜ì²˜ëŸ¼ í•˜ë‚˜ì”© í•˜ë©´ì„œ ì£¼ì„ìœ¼ë¡œ ë§‰ê³  í•˜ë©´ëœë‹¤. -> ë³€ìˆ˜ëª…ì€ ë™ì¼í•œê±° ì‚¬ìš©í•˜ê¸°
- ì‚­ì œ ì „í›„ì—ëŠ” printë¬¸ ì°ì–´ë³´ê¸°
-  LabelEncoderì˜ fit_transform() ì€ Seriesë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì•„ì„œ, numpy ë°°ì—´(ndarray)ì„ ë°˜í™˜í•œë‹¤. ê·¸ëŸ¬ë‚˜ pandasëŠ” numpy arrayë¥¼ íŠ¹ì • ì—´ì— ëŒ€ì…í•˜ë©´ ìë™ìœ¼ë¡œ Seriesë¡œ ë³€í™˜í•´ì¤€ë‹¤.
```py
# baseline
# print(train.shape, test.shape)
# train = train.drop(cols, axis=1)
# test = test.drop(cols, axis=1)
# print(train.shape, test.shape)


# label
from sklearn.preprocessing import LabelEncoder

for col in cols:
    le = LabelEncoder()
    train[col] = le.fit_transform(train[col])
    test[col] = le.transform(test[col])

train[cols].head()


# # one-hot
# train = pd.get_dummies(train, columns=cols)
# test = pd.get_dummies(test, columns=cols)
print(cols)
```

## 2-2. ê²€ì¦ë°ì´í„° ë¶„ë¦¬
- ì—¬ê¸°ì„œ ê¸°ì¡´ì˜ trainì€ ë³€í˜•í•˜ì§€ ë§ê³  dropì„ í†µí•´ íƒ€ê²Ÿë°ì´í„°ë§Œ ë¶„ë¦¬í•˜ëŠ”ê²Œ ë” ë‚«ë‹¤.
```py
from sklearn.model_selection import train_test_split

X_tr, X_val, y_tr, y_val = train_test_split(
    train.drop('Attrition_Flag', axis=1), train['Attrition_Flag'], test_size=0.2, random_state=2022
)
```

## 2-3. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
- ëª¨ë¸ì€ lightgbmê³¼ randomforestë¥¼ ì‚¬ìš©í•´ì„œ ê²€ì¦í•´ë³¸ë‹¤.
- ë‘˜ë‹¤ importì— ë’¤ì— Classifier í˜¹ì€ Regressorë¡œ 
- ì•„ë˜ëŠ” lightgbmì´ë‹¤. 
```py
from lightgbm import LGBMClassifier # ì„í¬íŠ¸ë¬¸ ì™¸ìš°ê¸°
lgbm = LGBMClassifier(random_state=42)
lgbm.fit(x_train, y_train)
lgbm_val_pred = lgbm.predict(x_val)
lgbm_val_pred_proba = lgbm.predict_proba(x_val)
```
- ì•„ë˜ëŠ” RandomForestì´ë‹¤.
```py
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=42)

rf.fit(x_train, y_train)

rf_val_pred = rf.predict(x_val)
rf_val_pred_proba = rf.predict_proba(x_val)
```
- ì•„ë˜ëŠ” metricsì— í¬í•¨ëœ ë¶„ë¥˜ëª¨ë¸ì˜ í‰ê°€ì§€í‘œë“¤ì´ë‹¤.
- sklearn ë¬¸ì„œì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì´ë ‡ê²Œ ì„¤ëª…ë¼ ìˆë‹¤. ëª¨ë“  metric signature(í•¨ìˆ˜ í˜•íƒœ)ëŠ” ê°€ëŠ¥í•˜ë©´ ë‹¤ìŒ í˜•íƒœë¥¼ ë”°ë¥¸ë‹¤: metric(y_true, y_pred, **kwargs)
- ì¦‰:
- ì²« ë²ˆì§¸ ì¸ì = ì •ë‹µ
- ë‘ ë²ˆì§¸ ì¸ì = ì˜ˆì¸¡ê°’
```py
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# ì •í™•ë„
print(accuracy_score(y_val, pred))

# ì •ë°€ë„
print(precision_score(y_val, pred))

# ì¬í˜„ìœ¨ (ë¯¼ê°ë„)
print(recall_score(y_val, pred))

# F1
print(f1_score(y_val , pred))

# baseline
# 0.9666872301048736
# 0.9444444444444444
# 0.8435114503816794
# 0.8911290322580645

# label
# 0.9685379395434917
# 0.948936170212766
# 0.851145038167939
# 0.89738430583501
```
- roc_auc_scoreì˜ ê²½ìš°ì—ëŠ” ë¬¸ì œì—ì„œ ìš”êµ¬í•˜ëŠ” ê°’ ì¤‘ 0 or 1ì˜ ê°’ì„ ì„ íƒí•˜ì—¬ ì œì‹œí•´ì•¼í•œë‹¤.
```py
# roc-auc
pred = model.predict_proba(X_val)
print(roc_auc_score(y_val, pred[:,1]))
# baseline
# 0.9894048160692921

# label
# 0.9899707351049547
```
- ì´ë ‡ê²Œ ë¨¸ì‹ ì— classes_ë¥¼ ë¶™ì—¬ì„œ ì–´ë–¤ ìˆœì„œë¡œ ë¼ë²¨ë§ ë˜ì–´ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
```py
lgbm.classes_
```

## 2-4. ì˜ˆì¸¡ ë° ì œì¶œ
- ìœ„ì—ì„œ ê°€ì¥ ë§ëŠ” ì¸ì½”ë”© + ë¨¸ì‹ ì„ ì„ íƒí•˜ì—¬ í•´ë‹¹ ë¨¸ì‹ ìœ¼ë¡œ ì˜ˆì¸¡ í›„ ì œì¶œí•œë‹¤.
```py
pred = model.predict_proba(test)
```
- ì œì¶œì‹œ seriesë‚˜ ë°°ì—´ì„ í•©ì³ì„œ dataframeìœ¼ë¡œ ë§Œë“ ë‹¤. ì´ë•Œ ì•„ê¹Œ ë¹¼ë‘ì—ˆë˜ ì„¤ê³„ë³€ìˆ˜ë„ ì•ì— ë¶™ì—¬ì¤€ë‹¤.(í•„ìš”í•  ê²½ìš°)
```py
submit = pd.DataFrame({
    'CLIENTNUM':test_id,
    'Attrition_Flag':pred[:,1]
})
```
- ìš”êµ¬í•˜ëŠ” ì œì¶œ ì–‘ì‹ì— ë§ì¶”ì–´ csvíŒŒì¼ì„ ì œì¶œí•œë‹¤. ì´ë•Œ index=Falseë¥¼ ê¼­ ë¶™ì´ëŠ” ê²ƒì„ ìŠìœ¼ë©´ ì•ˆëœë‹¤.
```py
submit.to_csv('00000.csv', index=False)
```

# ğŸ“Œ 3. ì‘ì—…í˜• 2 ëª¨ì˜ë¬¸ì œ 2 (íšŒê·€)
## 3-1. ë°ì´í„° ì „ì²˜ë¦¬
- ì•„ë˜ë¥¼ í†µí•´ ë°ì´í„° ìˆ˜ì™€ ë¹„ìŠ·í•˜ê²Œ ë‹¤ì–‘í•œ ê°’ì„ ê°–ëŠ” ê°’ë“¤ì´ë‚˜ ë‹µì§€ì— ì œì¶œí•˜ì§€ ì•Šì„ ì„¤ê³„ë³€ìˆ˜ëŠ” ì œì™¸í•œë‹¤.
- ë˜í•œ ë‹µì•ˆì— ì œì¶œí•  ì„¤ê³„ë³€ìˆ˜ idëŠ” ë”°ë¡œ ë³´ê´€í•´ë‘”ë‹¤.
- ì´í›„ ë‚¨ì€ í•˜ë‚˜ì˜ ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ëŒ€ì²´í•œë‹¤. ë¬¼ë¡  ì •í™•í•˜ê²Œ ì¹¼ëŸ¼ì´ë¦„ì„ ì°ì–´ì„œ ëŒ€ì¹˜í•˜ëŠ”ê²ƒì´ ë” ì¢‹ë‹¤.
```py
# ê²°ì¸¡ì¹˜ ì¹¼ëŸ¼ê³¼ ë‹¤ë¥¸ ì¹¼ëŸ¼ ì¤‘ í•™ìŠµì— í•„ìš”ì—†ëŠ” ì¹¼ëŸ¼ ì‚­ì œ
drop_cols = ['name', 'host_name', 'host_id', 'last_review']
train = train.drop(drop_cols, axis=1)
test = test.drop(drop_cols, axis=1)

# ì„¤ê³„ë³€ìˆ˜ id ì‚­ì œ, testëŠ” ì‚­ì œ í›„ ë”°ë¡œ ì¹¼ëŸ¼ì— ë³´ê´€
train = train.drop('id', axis=1)
id_test = test.pop('id')

# ê²°ì¸¡ì¹˜ ëŒ€ì¹˜ -> ì•„ë§ˆ ë¦¬ë·°ëŠ” ì—†ì–´ì„œ ê²°ì¸¡ì¹˜ì¸ê±° ì•„ë‹ê¹Œë¼ëŠ” ìƒê°ìœ¼ë¡œ 0ìœ¼ë¡œ ëŒ€ì¹˜
train = train.fillna(0)
test = test.fillna(0)
```
- ë‹¤ í•´ë³´ë©´ ì¢‹ì§€ë§Œ ì›í•«ì¸ì½”ë”©ì´ ë„ˆë¬´ ì¹¼ëŸ¼ìˆ˜ê°€ ë§ì•„ì§ˆ ê²ƒ ê°™ì•„ ì œì™¸í•œë‹¤.
- obj_cols = train.select_dtypes(include='object').columnsì—ì„œ ë’¤ì— columns ë¶™ëŠ”ê±° í•­ìƒ ì£¼ì˜í•´ì•¼ í•œë‹¤.
```py
# ì¸ì½”ë”©
obj_cols = train.select_dtypes(include='object').columns

# baseline
# print(train.shape)
# train = train.drop(obj_cols,axis=1)
# test = test.drop(obj_cols,axis=1)
# print(train.shape)

# onehot
# train = pd.get_dummies(train, columns = obj_cols)
# test = pd.get_dummies(test, columns = obj_cols)

# labelencoding
from sklearn.preprocessing import LabelEncoder
for col in obj_cols:
  le = LabelEncoder()
  train[col] = le.fit_transform(train[col])
  test[col] = le.transform(test[col])
```

## 3-2. ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
- í™•ì‹¤íˆ ë°ì´í„°ë¥¼ ë¶„ë¦¬í• ë•Œ íƒ€ê²Ÿì„ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ê¹”ë”í•œ ê²ƒ ê°™ë‹¤.
```py
from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(train.drop('price',axis=1), train['price'], train_size=0.8, random_state=42)
print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)
```

## 3-3. ëª¨ë¸í•™ìŠµ ë° í‰ê°€
- ë³€ìˆ˜ë¥¼ ëª¨ë¸ë¡œ í†µì¼í•˜ê³  ê°™ì€ ë³€ìˆ˜ë¥¼ ì“°ë©´ì„œ ì£¼ì„ìœ¼ë¡œ ë§‰ìœ¼ë©´ì„œ í•˜ëŠ”ê²Œ ì¢‹ì€ ê²ƒ ê°™ë‹¤.
- lightgbmê³¼ randomforestë¥¼ ì‚¬ìš©í•˜ë˜ classifierì™€ regressorë§Œ êµ¬ë¶„í•˜ë©´ ëœë‹¤.
```py
# lightgbm
# from lightgbm import LGBMRegressor
# model = LGBMRegressor(random_state=42)
# model.fit(x_train, y_train)
# pred = model.predict(x_val)

# randomforest
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(random_state=42)
model.fit(x_train,y_train)
pred = model.predict(x_val)
```
- ì•„ë˜ëŠ” íšŒê·€ë¶„ì„ì˜ ì£¼ìš” í‰ê°€ì§€í‘œë“¤ë¡œ r2_score, mean_absolute_error, mean_squared_errorê°€ ê¸°ë³¸ì´ê³  ë‚˜ë¨¸ì§€ëŠ” npë¡œ ê³„ì‚°í•´ì•¼ í•œë‹¤. í•˜ì§€ë§Œ êµ³ì´ ê³„ì‚°í•  ì¤„ ëª°ë¼ë„ ë¬¸ì œë¥¼ í‘¸ëŠ”ë° í° ì§€ì¥ì€ ì—†ë‹¤.
```py
import numpy as np
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
def rmse(y_val, pred): #RMSE
    return np.sqrt(mean_squared_error(y_val, pred))

def rmsle(y_val, pred): #RMSLE
    return np.sqrt(np.mean(np.power(np.log1p(y_val) - np.log1p(pred), 2)))

def mape(y_val, pred): #MAPE
    return np.mean(np.abs((y_val - pred) / y_val)) * 100

    print(r2_score(y_val, pred))
print(mean_absolute_error(y_val, pred))
print(mean_squared_error(y_val, pred))
print(rmse(y_val, pred))
print(rmsle(y_val, pred))
print(mape(y_val, pred))
```

## 3-4. ì˜ˆì¸¡ ë° ì œì¶œ
- result.to_csv('0000.csv', index=False)ë¥¼ ë˜ ê¹Œë¨¹ì„ ë»” í–ˆë‹¤.
```py
pred1 = model.predict(test)
result = pd.DataFrame({
    'id': id_test,
    'pred':pred1
})
result.to_csv('0000.csv', index=False)
```

# ğŸ“Œ 4. ì‘ì—…í˜• 2 ëª¨ì˜ë¬¸ì œ 3 (íšŒê·€-í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)
## 4-1. ë°ì´í„° ì „ì²˜ë¦¬
- ì•„ë˜ ì „ì²˜ë¦¬ ê²°ê³¼ ê²°ì¸¡ì¹˜, ì¸ì½”ë”©, ì´ìƒì¹˜ ì—†ìŒ -> ì„¤ê³„ë³€ìˆ˜ë§Œ ì œê±°
```py
# ê²°ì¸¡ì¹˜, ì¸ì½”ë”©, ì´ìƒì¹˜ ì—†ìŒ
train.info()
train.head()
train.isna().sum()
train.describe()

# id ë¶„ë¦¬
train = train.drop('id', axis=1)
test_id = test.pop('id')
```

## 4-2. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
- ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ëŠ” ìœ„ì—ì„œ í–ˆë˜ ê²ƒë“¤ê³¼ ê°™ì•„ì„œ ìƒëµí•¨'
- í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •ì€ ì•„ë˜ì™€ ê°™ìŒ
- lightgbm -> max_depth = 3~12 , n_estimator = 100~1000, learning_rate = 0.1 ~0.01 -> ì´ë•Œ n_estimatorê°€ ì˜¤ë¥´ë©´ learning_rateëŠ” ë‚´ë ¤ê°€ì•¼í•¨.
- ë¶€ìŠ¤íŒ… ëª¨ë¸ì€ ê° íŠ¸ë¦¬ê°€ â€œì´ì „ ì˜¤ì°¨ë¥¼ ì¡°ê¸ˆì”© ìˆ˜ì •â€í•˜ëŠ” ë°©ì‹ì¸ë°, íŠ¸ë¦¬ë¥¼ ë§ì´ ë§Œë“¤ìˆ˜ë¡(= n_estimators ì¦ê°€) ê° íŠ¸ë¦¬ì˜ ì˜í–¥ë ¥(= learning_rate)ì€ ì‘ì•„ì ¸ì•¼ ì „ì²´ ëª¨ë¸ì´ í­ì£¼í•˜ì§€ ì•Šê³  ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë  ìˆ˜ ìˆë‹¤.
- randomforest -> max_depth = 3~12 , n_estimator = 100~1000 ë§Œ ë°”ê¿”ë³´ë©´ ëœë‹¤.
- ì´ë•Œ ë°”ê¿”ì„œ ê°’ì´ ë†’ì•„ì§€ë©´ ì´ë¥¼ ê¼­ ì£¼ì„ìœ¼ë¡œ ë©”ëª¨í•´ë‘ì–´ì•¼í•œë‹¤.
```py
# lightgbm
from lightgbm import LGBMClassifier
model = LGBMClassifier(random_state=42, verbose=-1, max_depth = 6, n_estimator = 100, learning_rate = 0.1)
model.fit(x_train, y_train)
pred_pb = model.predict_proba(x_val)
pred = model.predict(x_val)

from sklearn.metrics import roc_auc_score, accuracy_score, f1_score
print(roc_auc_score(y_val, pred_pb[:,1]), accuracy_score(y_val, pred), f1_score(y_val, pred))

# max_depth = 6, n_estimator = 100, learning_rate = 0.1
# 0.8193979933110368 0.7755102040816326 0.8
```
```py
# randomforest
# from sklearn.ensemble import RandomForestClassifier
# model = RandomForestClassifier(random_state=42)
# model.fit(x_train, y_train)
# pred_pb = model.predict_proba(x_val)
# pred = model.predict(x_val)

# print(roc_auc_score(y_val, pred_pb[:,1]), accuracy_score(y_val, pred), f1_score(y_val, pred))

# 0.810200668896321 0.7959183673469388 0.8214285714285714
```

## 4-3. ì˜ˆì¸¡ ë° ì œì¶œ
- ë§ˆì§€ë§‰ì— read_csvë¡œ ì˜ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ëŠ”ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
```py
pred1 = model.predict_proba(test)
result = pd.DataFrame({
    'id':test_id,
    'output':pred1[:,1]
})
result.to_csv('ìˆ˜í—˜ë²ˆí˜¸.csv', index=False)
pd.read_csv('ìˆ˜í—˜ë²ˆí˜¸.csv') # í™•ì¸ê¹Œì§€ ê¼­ í•˜ê¸°
```


